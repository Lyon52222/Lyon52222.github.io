<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>GILE A Generalized Input-Label Embedding for Text Classification</title>
    <url>/2020/11/24/GILE_A_Generalized_Input_Label_Embedding_for_Text_Classification/</url>
    <content><![CDATA[<h1 id="GILE-A-Generalized-Input-Label-Embedding-for-Text-Classification"><a href="#GILE-A-Generalized-Input-Label-Embedding-for-Text-Classification" class="headerlink" title="GILE: A Generalized Input-Label Embedding for Text Classification"></a>GILE: A Generalized Input-Label Embedding for Text Classification</h1><h5 id="论文来源：TACL-2019"><a href="#论文来源：TACL-2019" class="headerlink" title="论文来源：TACL 2019"></a>论文来源：TACL 2019</h5><h5 id="论文链接：https-arxiv-org-abs-1806-06219"><a href="#论文链接：https-arxiv-org-abs-1806-06219" class="headerlink" title="论文链接：https://arxiv.org/abs/1806.06219"></a>论文链接：<a href="https://arxiv.org/abs/1806.06219">https://arxiv.org/abs/1806.06219</a></h5><h5 id="代码链接：https-github-com-idiap-gile"><a href="#代码链接：https-github-com-idiap-gile" class="headerlink" title="代码链接：https://github.com/idiap/gile"></a>代码链接：<a href="https://github.com/idiap/gile">https://github.com/idiap/gile</a></h5><hr>
<h2 id="启发："><a href="#启发：" class="headerlink" title="启发："></a>启发：</h2><p>现有的很多研究都是使用标签集合的k-hot向量进行训练。标签都只是被当成一个没有任何语义和标签空间结构的符号，而没有使用潜在的语义知识来描述输出的标签。<br>单词的语义描述已经被证实属于表示输入很有帮助，那么我们可以推断，对标签的语义描述应该也具有相同的效果。</p>
<h2 id="做了什么："><a href="#做了什么：" class="headerlink" title="做了什么："></a>做了什么：</h2><ul>
<li>作者鉴定了现有的联合输入标签模型的关键理论和实践限制。</li>
<li>作者在先前联合标签输入模型的基础上提出了一种新颖的带有灵活参数化的联合输入标签嵌入，并解决了现有模型的一些问题。</li>
<li>作者提供经验证据表明，作者的模型优于忽略标签语义的单语言和多语言模型，并且在可见标签和不可见标签上的标签都要优于之前的联合输入标签模型。<h2 id="动机："><a href="#动机：" class="headerlink" title="动机："></a>动机：</h2>先前的工作通过联合输入标签空间利用了标签文本中的知识，起初用于图像分类中。 这样的模型在训练过程中同时概况可见和不可见的标签，并且在大量标签集上的扩展很好。 不过它还是具有下面三种限制：</li>
<li>因为它的双向线性关系，所以它们的嵌入无法捕获复杂的标签关系。 </li>
<li>因为输出层依赖于文本和标签的编码的维度所以输出层的参数比较死板。</li>
<li>在可见标签上使用交叉熵损失函数的分类器表现的更好。</li>
</ul>
<h2 id="文本分类的背景知识"><a href="#文本分类的背景知识" class="headerlink" title="文本分类的背景知识:"></a>文本分类的背景知识:</h2><p>$D = \{(x_i,y_i),i=1,…,N\}$ </p>
<p>$x_i$表示第i个文本，$x_i=\{w_{11},w_{12},…,w_{K_i,T_{K_i}}\}$ </p>
<p>$K_i$表示是第i个文本中的句子数量,$T_{K_i}$表示该句子的单词数量。</p>
<p>$y_i=\{y_{ij}∈\{0,1\},j=1,…,k\}$ $k$表示标签的数量。</p>
<p>$c_i=\{c_{j1},c_{j2},…,c_{j,L_j}|j=1,…,k\}$ 表示标签$j$的描述。$L_j$表示描述的单词数量。</p>
<p>在训练过程中只给文本和他们的可见标签D。我们的目标是训练一个分类器在可见标签$Y_s$的样本集合和不可见标签$Y_u$的样本集合上都能预测。  $Y_s∩Y_u=Ø,Y_s∪Y_u=Y$</p>
<h2 id="输入的文本表示："><a href="#输入的文本表示：" class="headerlink" title="输入的文本表示："></a>输入的文本表示：</h2><p>作者使用<font color="red">多层注意力网络 (HANs,hierarchical attention networks)</font>来给输入文本编码。HANs在单语言和多语言分类上都有不错的竞争力。</p>
<p>输入：文本$x$   —&gt;  输出：文本向量$h$ 。 输入的文本单词和标签单词都是用$d$维的向量$E$来表示。</p>
<p>$E∈IR^{|V|×d}$  。$V$是词汇表， $d$是词嵌入的维度。</p>
<h2 id="编码："><a href="#编码：" class="headerlink" title="编码："></a>编码：</h2><p>$g_w$对句子$i$,$\{w_{it}|t=1,2…,T_i\}$ 进行编码。<br>$h_{w}^{(it)}=g_w(w_{it}),t∈[1,T_i]$</p>
<p>将单词矩阵$\{h_{w}^{(it)}|t=1,…,T_i\}$<font color="red">联合(combining)</font>成一个句子向量$s_i∈IR^{d_w}$,$d_w$是单词编码的维度。</p>
<p>$g_s$对句子序列$h_s^{(i)}=\{s_i|i=1,…,K\}$进行编码。</p>
<p>$g_w,g_s$可以是任何前向反馈网络，比如RNN或者GRU。</p>
<h2 id="注意力："><a href="#注意力：" class="headerlink" title="注意力："></a>注意力：</h2><p>$α_w,α_s$是注意力参数，用于衡量隐藏状态向量的重要程度。</p>
<p>$s_i=\sum\limits_{t=1}^{T_i} α_w^{(it)} h_w^{(it)}$</p>
<p>$=$ </p>
<p>$\sum\limits_{t=1}^{T_i} \frac{exp(v_{it}^Tu_w)}{\sum_jexp(v_{ij}^Tu_w)} h_{w}^{it}$</p>
<p>$v_{it}=f_w(h_w^{(it)})$是$W_w$作为参数的全连接网络</p>
<h2 id="HANs网络结构："><a href="#HANs网络结构：" class="headerlink" title="HANs网络结构："></a>HANs网络结构：</h2><p><img src="/2020/11/24/GILE_A_Generalized_Input_Label_Embedding_for_Text_Classification/HANs.jpg" alt="hans"></p>
<h2 id="标签表示："><a href="#标签表示：" class="headerlink" title="标签表示："></a>标签表示：</h2><p>标签编码使用标签描述$c_j$作为输入，并输出一个标签向量$e_j∈IR^{d_c} ∀j=1,…,k.$考虑到效率方面作者使用简单的无参数的方式来计算$e_j$。直接使用描述中单词向量的平均值来描述标签$j$，$e_j=\frac{1}{L_j}\sum\limits_{t=1}^{L_j}c_{jt}$。将所有标签向量连接成一个矩阵，得到标签嵌入$ε∈IR^{|y|×d}$。</p>
<h2 id="输出层参数化："><a href="#输出层参数化：" class="headerlink" title="输出层参数化："></a>输出层参数化：</h2><h3 id="传统的线性单元："><a href="#传统的线性单元：" class="headerlink" title="传统的线性单元："></a>传统的线性单元：</h3><p>最传统的输出层包含一个具有$W∈IR^{d_h×|y|}$和$b∈IR^{|y|}$的线性单元，并使用softmax或者sigmoid等激活函数进行激活。$d_h$是编码器的隐藏层表示$h$的维度。</p>
<p>$p(y|x)=∝exp(W^Th+b)$</p>
<p>$W$可以单独训练或者令$W=E^T$</p>
<p>无论是那种方式，模型的参数通常都是通过交叉熵损失函数来学习的。然而在两种情况下他们都无法适用于在训练过程中没有看见的标签。因为每个标签都学习了适用于特定那个标签的参数，所以未出现的标签的参数无法被学习到。</p>
<p>所以作者尝试使用可以处理不可见标签的模型。</p>
<h3 id="双向输入标签单元："><a href="#双向输入标签单元：" class="headerlink" title="双向输入标签单元："></a>双向输入标签单元：</h3><p>因为标签编码器的参数是共享的，所以联合输入输出嵌入模型可以归纳可见标签和不可见标签。</p>
<ul>
<li><p>双线性排序函数：<br>$f(x,y)=εωh$</p>
<p>$ε∈IR^{|y|×d}$ 表示标签嵌入。<br>$ω∈IR^{d×d_h}$是双向嵌入。</p>
<p>这个函数允许使用铰链损失函数对标签y根据和x的相关性进行排序，不过这样就失去了条件概率。</p>
<h3 id="限制："><a href="#限制：" class="headerlink" title="限制："></a>限制：</h3></li>
<li><p>$f(x,y)=εωh$只能通过$ω$来捕获文本编码$h$和标签嵌入$ε$之间的线性关系。但是不同标签之间的关系是非线性的。 所以一个更合理的形式是引入非线性函数$σ(·)$</p>
<p>  输入结构：$σ(εω)h$<br>  输出结构：$σε(ωh)$</p>
</li>
<li><p>因为参数$ω$的大小被$ε$和$h$的大小限制了，所以输出层的能力不好控制。</p>
</li>
<li>它的损失函数是优化一个排名而不是分类表现。</li>
</ul>
<h2 id="提出的用于文本分类的输出层参数化"><a href="#提出的用于文本分类的输出层参数化" class="headerlink" title="提出的用于文本分类的输出层参数化"></a>提出的用于文本分类的输出层参数化</h2><p>作者为神经文本分类提出了一种新的输出层参数化，包含了归纳输入标签嵌入从而获取标签结构，文本编码的结构和它们两种之间的关系。然后使用一个和标签集合大小无关的分类单元。</p>
<p><img src="/2020/11/24/GILE_A_Generalized_Input_Label_Embedding_for_Text_Classification/net.png" alt="net"></p>
<p>$e^j$是标签嵌入矩阵$ε$的第$j$行。</p>
<p>$e_j’=g_{out}(e_j)=σ(e_jU+b_u)$</p>
<p>$h’=g_{in}(h)=σ(Vh+b_v)$</p>
<p>传统的概率输出如下所示：</p>
<p>$p(y|x)∝exp(ε’h’)\\ \qquad \quad∝exp(g_{out}(ε)g_{in}(h)) \\ \qquad \quad∝exp(σ(εU+b_u) σ(Vh+b_v))$</p>
<p>作者提出了只依赖于联合输入标签空间的分类器</p>
<p>联合空间表示为：</p>
<p>$g_{joint}^{(ij)}=g_{in}(h_i)⊙g_{out}(e_j)$</p>
<p>$p_{val}^{ij}=g_{joint}^{(ij)}w+b$</p>
<p>$P_{v a l}^{(i)}=\left[\begin{array}{c}p_{v a l}^{(i 1)} \\ p_{v a l}^{(i 2)} \\ \cdots \\ p_{v a l}^{(i k)}\end{array}\right]=\left[\begin{array}{c}g_{j o i n t}^{(i 1)} w+b \\ g_{j o i n t}^{(i 2)} w+b \\ \cdots \\ g_{j o i n t}^{(i k)} w+b\end{array}\right]$</p>
<p>$\hat{y_i} = \hat{p} (y_{i} | x_{i})=\frac{1} {1+e^{-P_{val}^{(i) } } }$</p>
]]></content>
      <categories>
        <category>paper</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>label_embedding</tag>
      </tags>
  </entry>
  <entry>
    <title>Joint Embedding of Words and Labels for Text Classification</title>
    <url>/2020/11/26/Joint_Embedding_of_Words_and_Labels_for_Text_Classification/</url>
    <content><![CDATA[<h1 id="Joint-Embedding-of-Words-and-Labels-for-Text-Classification"><a href="#Joint-Embedding-of-Words-and-Labels-for-Text-Classification" class="headerlink" title="Joint Embedding of Words and Labels for Text Classification"></a>Joint Embedding of Words and Labels for Text Classification</h1><h5 id="论文来源：ACL-2018"><a href="#论文来源：ACL-2018" class="headerlink" title="论文来源：ACL 2018"></a>论文来源：ACL 2018</h5><h5 id="论文链接：https-arxiv-org-abs-1805-04174"><a href="#论文链接：https-arxiv-org-abs-1805-04174" class="headerlink" title="论文链接：https://arxiv.org/abs/1805.04174"></a>论文链接：<a href="https://arxiv.org/abs/1805.04174">https://arxiv.org/abs/1805.04174</a></h5><h5 id="代码链接：https-github-com-guoyinwang-LEAM"><a href="#代码链接：https-github-com-guoyinwang-LEAM" class="headerlink" title="代码链接：https://github.com/guoyinwang/LEAM"></a>代码链接：<a href="https://github.com/guoyinwang/LEAM">https://github.com/guoyinwang/LEAM</a></h5><hr>
<p>作者提出使用word和label的一个联合embedding来更好的学习文本表示。</p>
<p>目前大多数的文本分类方法都是基于CNN和RNN，并且还会加入attention机制来获取文本中的word的依赖和重要度从而更好的学习文本的representation。作者在本文中也沿着这条主线进行探索，但是与之前的方法不同的是作者引入了文本的label信息来更好的学习文本表示，这也是本文的最大贡献与创新，即提出了一个Label-Embedding Attentive Model(LEAM)。该模型学习word和label在同一空间内的embedding，利用text和label的相关性构建文本表示。</p>
<h1 id="符号表示"><a href="#符号表示" class="headerlink" title="符号表示"></a>符号表示</h1><p>$⊘$表示element-wise的除法。</p>
<p>训练集为$S=\{(X_n,y_n)\}_{n=1}^N$</p>
<p>$X_i$为文本序列，$y_i$为其对应标记。</p>
<p>针对单标记任务， $y_i$ 是一个one-hot vector，而针对多标记任务， $y_i$ 是一个二值向量。举个例子，假设有5个类别，分别是1，2，3，4，5。如果某个样本对应类别3，在单标记中 $y_i$ 为(0,0,1,0,0)。如果某个样本同时属于类别2和4，则 $y_i$ 为(0,1,0,1,0)。模型的目标是学习一个从 $X$ 到 $y$ 的映射，使得下式最小</p>
<script type="math/tex; mode=display">\underset{f \in F}{\min} \frac{1}{N} \sum_{n=1}^{N} \delta\left(y_{n}, f\left(X_{n}\right)\right)</script><p>其中 $\delta$ 是损失函数。 $X_i$ 中的每个word都是一个one-hot vector $△^D$ ,D是字典的大小。而word embedding就是做一个 $△^D$ 到 $R^P$ 的映射，P是embedding的维度，用 $v_i$ 表示第 $i$ 个word的embedding。</p>
<h1 id="模型详解"><a href="#模型详解" class="headerlink" title="模型详解"></a>模型详解</h1><p>作者将文本分类看作三个函数的组合， $f=f_0∘f_1∘f_2$,</p>
<ul>
<li>$f_0$ 是word embedding的函数， </li>
<li>$f_1$ 是将word embedding进行聚合得到文本表示的函数， </li>
<li>$f_2$ 是利用文本表示进行分类的函数。<br>设计 $f_1$ 的方法分为两大类，其中一类是将该过程看作一个黑盒，利用各种深度模型学习映射，而另一个是利用简单的max pooling或者mean pooling。但无论怎样，这两类都只利用了文本word的信息。可以看出，只有 $f_2$ 利用了类别信息，而类别的影响对于 $f_0$ 和 $f_1$ 都是间接的。因此，作者提出在每个过程中都使用标记信息，模型如下</li>
</ul>
<p><img src="/2020/11/26/Joint_Embedding_of_Words_and_Labels_for_Text_Classification/net.png" width="50%"></p>
<p>$f_0$: 学习label的embedding作为“anchor points”来影响word embedding，</p>
<p>$f_1$: 利用label和word之间的相关性进行word embedding的聚合。</p>
<p>$f_2$: 对于单标签问题和多标签问题有不同的处理方法，具体的看后面讲解。</p>
<p>首先将单词和标签嵌入到一个联合空间中：</p>
<p>$△^D \rightarrow R^P$ ,  $𝚢 \rightarrow R^P$</p>
<p>作者使用cosine相似度来计算每对label-word之间的相似度：</p>
<p>$G=(C^TV)⊘\hat{G}$</p>
<p>$C$是label embedding的矩阵，$C=[c_i,…,c_K]$,$K$为类的数量</p>
<p>$\hat{G}$是大小为$K×L$的归一化矩阵。$\hat{G}$中的每个元素为$\hat{g}_{kl}=||c_k|| \; ||v_l||$</p>
<p>为了把握连续单词之间的空间信息并引入非线性，作者对$G$进行了概括处理。</p>
<p>选取以$l$为中心,长度为$2r+1$的矩阵块$G_{l-r:l+r}$</p>
<p>$u_l = ReLU(G_{l−r:l+r}W_1 + b_1)$</p>
<p>$W_1∈R^{2e+1} \qquad b_1∈R^K \qquad u_l∈R^K$</p>
<p>$m_l=max_pooling(u_l)$</p>
<p>$\beta=SoftMax(m)$</p>
<p>$\beta_l = \frac{\exp \left(m_{l}\right)}{\sum_{l^{\prime}=1}^{L} \exp \left(m_{l^{\prime}}\right)}$</p>
<p>$z=\sum \limits_{l} \beta_lv_l$</p>
<h1 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h1><p><strong>单标记问题</strong>，训练目标为:</p>
<p>$\underset{f \in F}{\min} \frac{1}{N} \sum\limits_{n=1}^{N} C E\left(y_{n}, f_{2}\left(z_{n}\right)\right)$</p>
<p>$CE(x,y)$表示两个概率向量$x,y$的交叉熵</p>
<p>这时,$f_2(z_n)=SoftMax(z_n’)$</p>
<p>$z_n’=W_2z_n+b_2,W_2∈R^{K×P},b_2∈R^K$</p>
<p><strong>多标记问题</strong>，可以将其拆解为K个单标记问题，目标函数为下式</p>
<p>$\underset{f \in F}{\min} \frac{1}{N K} \sum\limits_{n=1}^{N} \sum\limits_{k=1}^{K} C E\left(y_{n k}, f_{2}\left(z_{n k}\right)\right)$</p>
<p>这时，$f_2(z_{nk})=\frac{1}{1+exp(z_{nk}’)}$,$z_{nk}’$是$z_n’$的第$k$列。</p>
<p>同时，作者希望label embedding能起到“anchor point”的作用，也就是相同类别的文本表示之间的距离小于不同类别的文本表示之间的距离。因此作者加入了一个正则化项</p>
<p>$\underset{f \in F}{\min} \frac{1}{K} \sum\limits_{n=1}^{K} C E\left(y_{k}, f_{2}\left(c_{k}\right)\right)$</p>
]]></content>
      <categories>
        <category>paper</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>label_embedding</tag>
      </tags>
  </entry>
  <entry>
    <title>LabelEmbedding-Research</title>
    <url>/2020/11/26/LabelEmbedding/</url>
    <content><![CDATA[<p>个人理解LabelEmbedding就是将标签的信息也利用起来结合到模型训练的过程中去。 这里的<strong>标签信息</strong>和<strong>结合训练的方法</strong>是值得入手的两个部分。</p>
<h2 id="标签信息："><a href="#标签信息：" class="headerlink" title="标签信息："></a>标签信息：</h2><p>目前最简单的方法就是直接使用标签向量，扩展的就是生成标签描述（Wikipedia的方式，抽取，抽象），有的还会结合使用标签的类层级关系和知识图谱。</p>
<h2 id="结合训练的方式："><a href="#结合训练的方式：" class="headerlink" title="结合训练的方式："></a>结合训练的方式：</h2><p>这里面的方法就比较多变了，目前我也看的不是很明白。 后面继续看论文补充吧。</p>
]]></content>
      <categories>
        <category>Research</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>label_embedding</tag>
      </tags>
  </entry>
  <entry>
    <title>RNN-LSTM-GRU</title>
    <url>/2020/11/26/RNN-LSTM-GRU/</url>
    <content><![CDATA[<h1 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h1><p><img src="/2020/11/26/RNN-LSTM-GRU/RNN.jpg" width="70%"></p>
<p>$x$ 为当前状态下数据的输入， $h$ 表示接收到的上一个节点的输入。</p>
<p>$y$ 为当前节点状态下的输出，而 $h’$ 为传递到下一个节点的输出。</p>
<p>通过上图的公式可以看到，输出 $h’$ 与 $x$ 和 $h$ 的值都相关。</p>
<p>而 $y$ 则常常使用 $h’$ 投入到一个线性层（主要是进行维度映射）然后使用softmax进行分类得到需要的数据。</p>
<p>对这里的$y$如何通过 $h’$ 计算得到往往看具体模型的使用方式。</p>
<p>通过序列形式的输入，我们能够得到如下形式的RNN。</p>
<p><img src="/2020/11/26/RNN-LSTM-GRU/RNN(1).jpg" width="70%"></p>
<h1 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h1><h2 id="什么是LSTM"><a href="#什么是LSTM" class="headerlink" title="什么是LSTM"></a>什么是LSTM</h2><p>长短期记忆（Long short-term memory, LSTM）是一种特殊的RNN，主要是为了解决长序列训练过程中的梯度消失和梯度爆炸问题。简单来说，就是相比普通的RNN，LSTM能够在更长的序列中有更好的表现。</p>
<p>LSTM结构（图右）和普通RNN的主要输入输出区别如下所示。<br><img src="/2020/11/26/RNN-LSTM-GRU/LSTM_input_output.jpg" width="70%"></p>
<p>相比RNN只有一个传递状态 $h^t$ ，LSTM有两个传输状态，一个 $c^t$ （cell state），和一个 $h^t$ （hidden state）。（Tips：RNN中的 $h^t$ 对于LSTM中的 $c^t$ ）</p>
<p>其中对于传递下去的 $c^t$ 改变得很慢，通常输出的 $c^t$ 是上一个状态传过来的 $c^{t-1}$ 加上一些数值。</p>
<p>而 $h^t$ 则在不同节点下往往会有很大的区别。</p>
<h2 id="深入LSTM结构"><a href="#深入LSTM结构" class="headerlink" title="深入LSTM结构"></a>深入LSTM结构</h2><p>下面具体对LSTM的内部结构来进行剖析。</p>
<p><strong>首先使用LSTM的当前输入 $x^t$ 和上一个状态传递下来的 $h^{t-1}$ 拼接训练得到四个状态。</strong></p>
<p><img src="/2020/11/26/RNN-LSTM-GRU/LSTM(1).jpg" width="30%"></p>
<p><img src="/2020/11/26/RNN-LSTM-GRU/LSTM(2).jpg" width="30%"></p>
<p>其中， $z^i$ ， $z^f$ ，$z^o$ 是由拼接向量乘以权重矩阵之后，再通过一个 $sigmoid$ 激活函数转换成0到1之间的数值，来作为一种门控状态。而 $z$ 则是将结果通过一个 $tanh$ 激活函数将转换成-1到1之间的值（这里使用 $tanh$ 是因为这里是将其做为输入数据，而不是门控信号）。</p>
<p><img src="/2020/11/26/RNN-LSTM-GRU/LSTM.jpg" width="70%"></p>
<p> <em>$⊙$是Hadamard Product，也就是操作矩阵中对应的元素相乘，因此要求两个相乘矩阵是同型的。</em></p>
<p> <em>$⊕$ 则代表进行矩阵加法。</em></p>
<h2 id="LSTM内部主要有三个阶段："><a href="#LSTM内部主要有三个阶段：" class="headerlink" title="LSTM内部主要有三个阶段："></a>LSTM内部主要有三个阶段：</h2><ul>
<li><p>忘记阶段。这个阶段主要是对上一个节点传进来的输入进行选择性忘记。简单来说就是会 “忘记不重要的，记住重要的”。<br>具体来说是通过计算得到的 $z^f$ （f表示forget）来作为忘记门控，来控制上一个状态的 $c^{t-1}$ 哪些需要留哪些需要忘。</p>
</li>
<li><p>选择记忆阶段。这个阶段将这个阶段的输入有选择性地进行“记忆”。主要是会对输入 $x^t$ 进行选择记忆。哪些重要则着重记录下来，哪些不重要，则少记一些。当前的输入内容由前面计算得到的 $z$ 表示。而选择的门控信号则是由 $z^i$ （i代表information）来进行控制。<br>将上面两步得到的结果相加，即可得到传输给下一个状态的 $c^t$ 。也就是上图中的第一个公式。</p>
</li>
<li><p>输出阶段。这个阶段将决定哪些将会被当成当前状态的输出。主要是通过 $z^o$ 来进行控制的。并且还对上一阶段得到的 $c^t$ 进行了放缩（通过一个tanh激活函数进行变化）。</p>
</li>
</ul>
<p>与普通RNN类似，输出 $y^t$ 往往最终也是通过 $h^t$ 变化得到。</p>
<h1 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h1><p>GRU（Gate Recurrent Unit）是循环神经网络（Recurrent Neural Network, RNN）的一种。和LSTM（Long-Short Term Memory）一样，也是为了解决长期记忆和反向传播中的梯度等问题而提出来的。</p>
<p>相比LSTM，使用GRU能够达到相当的效果，并且相比之下更容易进行训练，能够很大程度上提高训练效率，因此很多时候会更倾向于使用GRU。</p>
<h2 id="GRU的输入输出结构"><a href="#GRU的输入输出结构" class="headerlink" title="GRU的输入输出结构"></a>GRU的输入输出结构</h2><p>GRU的输入输出结构与普通的RNN是一样的。</p>
<p>有一个当前的输入 $x^t$ ，和上一个节点传递下来的隐状态（hidden state） $h^{t-1}$ ，这个隐状态包含了之前节点的相关信息。</p>
<p>结合 $x^t$ 和 $h^{t-1}$，GRU会得到当前隐藏节点的输出 $y^t$ 和传递给下一个节点的隐状态 $h^t$ 。</p>
<p><img src="/2020/11/26/RNN-LSTM-GRU/GRU_input_output.png" width="30%"></p>
<h2 id="GRU的内部结构"><a href="#GRU的内部结构" class="headerlink" title="GRU的内部结构"></a>GRU的内部结构</h2><p>我们先通过上一个传输下来的状态 $h^{t-1}$ 和当前节点的输入 $x^t$ 来获取两个门控状态。如下图2-2所示，其中 $r$ 控制重置的门控（reset gate）， $z$ 为控制更新的门控（update gate）。</p>
<p>得到门控信号之后，首先使用重置门控来得到“重置”之后的数据 $h^{t-1’}=h^{t-1}⊙r$ ，再将 $h^{t-1’}$ 与输入 $x^t$ 进行拼接，再通过一个$tanh$激活函数来将数据放缩到-1~1的范围内。即得到如下图2-3所示的 $h’$ 。</p>
<p><img src="/2020/11/26/RNN-LSTM-GRU/GRU_h.png" width="30%"></p>
<p>这里的 $h’$ 主要是包含了当前输入的 $x^t$ 数据。有针对性地对 $h’$ 添加到当前的隐藏状态，相当于”记忆了当前时刻的状态“。类似于LSTM的选择记忆阶段。<br><img src="/2020/11/26/RNN-LSTM-GRU/GRU.jpg" width="70%"></p>
<p><strong>更新表达式：</strong> $h^t=(1-z)⊙h^{t-1}+z⊙h’$</p>
<p>GRU很聪明的一点就在于，我们使用了同一个门控 $z$ 就同时可以进行遗忘和选择记忆（LSTM则要使用多个门控）。</p>
<ul>
<li>$(1-z)⊙h^{t-1}$  表示对原本隐藏状态的选择性“遗忘”。这里的 $(1-z)$ 可以想象成遗忘门（forget gate），忘记 $h^{t-1}$ 中一些不重要的信息。</li>
<li>$z⊙h’$ 表示对包含当前节点信息的 $h’$ 进行选择性”记忆“。与上面类似，这里的 $z$ 同理会忘记 $h’$ 中的一些不重要的信息。或者，这里我们更应当看做是对 $h’$ 中的某些信息进行选择。</li>
</ul>
]]></content>
      <categories>
        <category>Networks</category>
      </categories>
      <tags>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title>Multi_Task_learning</title>
    <url>/2020/11/27/Multi-Task-learning/</url>
    <content><![CDATA[<h1 id="多任务学习概述"><a href="#多任务学习概述" class="headerlink" title="多任务学习概述"></a>多任务学习概述</h1><h5 id="搬运自：CSDN"><a href="#搬运自：CSDN" class="headerlink" title="搬运自：CSDN"></a>搬运自：<a href="https://blog.csdn.net/u010417185/article/details/83065506">CSDN</a></h5><hr>
<h2 id="单任务学习VS多任务学习"><a href="#单任务学习VS多任务学习" class="headerlink" title="单任务学习VS多任务学习"></a>单任务学习VS多任务学习</h2><ul>
<li><p>单任务学习：一次只学习一个任务（task），大部分的机器学习任务都属于单任务学习。</p>
</li>
<li><p>多任务学习：把多个相关（related）的任务放在一起学习，同时学习多个任务。</p>
</li>
</ul>
<h4 id="多任务学习（multitask-learning）产生的原因？"><a href="#多任务学习（multitask-learning）产生的原因？" class="headerlink" title="多任务学习（multitask learning）产生的原因？"></a>多任务学习（multitask learning）产生的原因？</h4><p>现在大多数机器学习任务都是单任务学习。对于复杂的问题，也可以分解为简单且相互独立的子问题来单独解决，然后再合并结果，得到最初复杂问题的结果。这样做看似合理，其实是不正确的，因为现实世界中很多问题不能分解为一个一个独立的子问题，即使可以分解，各个子问题之间也是相互关联的，通过一些共享因素或共享表示（share representation）联系在一起。把现实问题当做一个个独立的单任务处理，忽略了问题之间所富含的丰富的关联信息。多任务学习就是为了解决这个问题而诞生的。把多个相关（related）的任务（task）放在一起学习。这样做真的有效吗？答案是肯定的。多个任务之间共享一些因素，它们可以在学习过程中，共享它们所学到的信息，这是单任务学习所具备的。相关联的多任务学习比单任务学习能去的更好的泛化（generalization）效果。</p>
<p>单任务与多任务对比如下图所示：</p>
<p><img src="/2020/11/27/Multi-Task-learning/dif_of_singleandmulti.png" title="图一" width="70%"></p>
<p>从图中可以发现，单任务学习时，各个任务之间的模型空间（Trained Model）是相互独立的（图1上）。多任务学习时，多个任务之间的模型空间（Trained Model）是共享的（图1下）。</p>
<p>假设用含一个隐含层的神经网络来表示学习一个任务，单任务学习和多任务学习可以表示成如下图所示。</p>
<p><img src="/2020/11/27/Multi-Task-learning/diff_of_singleandmulti2.png" title="图一" width="80%"></p>
<p>从图中可以发现，单任务学习时，各个task任务的学习是相互独立的，多任务学习时，多个任务之间的浅层表示共享（shared representation）。</p>
<h2 id="多任务学习的定义"><a href="#多任务学习的定义" class="headerlink" title="多任务学习的定义"></a>多任务学习的定义</h2><p>多任务学习（Multitask learning）定义：基于共享表示（shared representation），把多个相关的任务放在一起学习的一种机器学习方法。</p>
<p>多任务学习（Multitask Learning）是一种推导迁移学习方法，主任务（main tasks）使用相关任务（related tasks）的训练信号（training signal）所拥有的领域相关信息（domain-specific information），做为一直推导偏差（inductive bias）来提升主任务（main tasks）泛化效果（generalization performance）的一种机器学习方法。多任务学习涉及多个相关的任务同时并行学习，梯度同时反向传播，多个任务通过底层的共享表示（shared representation）来互相帮助学习，提升泛化效果。简单来说：多任务学习把多个相关的任务放在一起学习（注意，一定要是相关的任务，后面会给出相关任务（related tasks）的定义，以及他们共享了那些信息），学习过程（training）中通过一个在浅层的共享（shared representation）表示来互相分享、互相补充学习到的领域相关的信息（domain information），互相促进学习，提升泛化的效果。</p>
<h4 id="共享表示shared-representation："><a href="#共享表示shared-representation：" class="headerlink" title="共享表示shared representation："></a>共享表示shared representation：</h4><p>共享表示的目的是为了提高泛化（improving generalization），图2中给出了多任务学习最简单的共享方式，多个任务在浅层共享参数。MTL中共享表示有两种方式：</p>
<ul>
<li><p>基于参数的共享（Parameter based）：比如基于神经网络的MTL，高斯处理过程。</p>
</li>
<li><p>基于约束的共享（regularization based）：比如均值，联合特征（Joint feature）学习（创建一个常见的特征集合）。</p>
</li>
</ul>
<h2 id="多任务学习有效的原因"><a href="#多任务学习有效的原因" class="headerlink" title="多任务学习有效的原因"></a>多任务学习有效的原因</h2><p>为什么把多个相关的任务放在一起学习，可以提高学习的效果？关于这个问题，有很多解释。这里列出其中一部分，以图2中由单隐含层神经网络表示的单任务和多任务学习对比为例。</p>
<ul>
<li><p>多人相关任务放在一起学习，有相关的部分，但也有不相关的部分。当学习一个任务（Main task）时，与该任务不相关的部分，在学习过程中相当于是噪声，因此，引入噪声可以提高学习的泛化（generalization）效果。</p>
</li>
<li><p>单任务学习时，梯度的反向传播倾向于陷入局部极小值。多任务学习中不同任务的局部极小值处于不同的位置，通过相互作用，可以帮助隐含层逃离局部极小值。</p>
</li>
<li><p>添加的任务可以改变权值更新的动态特性，可能使网络更适合多任务学习。比如，多任务并行学习，提升了浅层共享层（shared representation）的学习速率，可能，较大的学习速率提升了学习效果。</p>
</li>
<li><p>多个任务在浅层共享表示，可能削弱了网络的能力，降低网络过拟合，提升了泛化效果。</p>
</li>
</ul>
<p>还有很多潜在的解释，为什么多任务并行学习可以提升学习效果（performance）。多任务学习有效，是因为它是建立在多个相关的，具有共享表示（shared representation）的任务基础之上的，因此，需要定义一下，什么样的任务之间是相关的。</p>
<h2 id="相关（related）定义"><a href="#相关（related）定义" class="headerlink" title="相关（related）定义"></a>相关（related）定义</h2><p>相关（related）的具体定义很难，但我们可以知道的是，在多任务学习中，related tasks可以提升main task的学习效果，基于这点得到相关的定义：</p>
<p>$Related（Main Task，Related tasks，LearningAlg）= 1$</p>
<p>$LearningAlg（Main Task||Related tasks）&gt; LearningAlg（Main Task)$                                      </p>
<ul>
<li>LearningAlg表示多任务学习采用的算法，公式（1）：第一个公式表示，把Related tasks与main tasks放在一起学习，效果更好；第二个公式表示，基于related tasks，采用LearningAlg算法的多任务学习Main task，要比单学习main task的条件概率概率更大。特别注意，相同的学习任务，基于不同学习算法，得到相关的结果不一样：</li>
</ul>
<p>$Related（Main Task，Related tasks，LearningAlg1）!= Related（Main Task，Related tasks，LearningAlg2）$</p>
<h2 id="多任务学习中的相关关系（task-relationship）"><a href="#多任务学习中的相关关系（task-relationship）" class="headerlink" title="多任务学习中的相关关系（task relationship）"></a>多任务学习中的相关关系（task relationship）</h2><p>多任务学习并行学习时，有5个相关因素可以帮助提升多任务学习的效果。</p>
<ul>
<li><p>数据放大（data amplification）。相关任务在学习过程中产生的额外有用的信息可以有效方法数据/样本（data）的大小/效果。主要有三种数据放大类型：</p>
<ul>
<li><p>统计数据放大（statistical data amplification）</p>
</li>
<li><p>采样数据放大（sampling data amplification）</p>
</li>
<li><p>块数据放大（blocking data amplification）。</p>
</li>
</ul>
</li>
<li><p>Eavesdropping（窃听）。假设</p>
</li>
<li><p>属性选择（attribute selection）</p>
</li>
<li><p>表示偏移（representation bias）</p>
</li>
<li><p>预防过拟合（overfitting prevention）<br>所有这些关系（relationships）都可以帮助提升学习效果（improve learning performance），关系具体定义可以参考<a href="https://link.springer.com/article/10.1023/A:1007379606734">Caruana, R. (1997). Multitask Learning. Machine Learning, 28(1), 41–75. doi: 10.1023/A:1007379606734</a></p>
</li>
</ul>
]]></content>
      <categories>
        <category>definition</category>
      </categories>
      <tags>
        <tag>multi_task_learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Multi-Task Label Embedding for Text Classification</title>
    <url>/2020/11/27/Multi_Task_Label_Embedding_for_Text_Classification/</url>
    <content><![CDATA[<h1 id="Multi-Task-Label-Embedding-for-Text-Classification"><a href="#Multi-Task-Label-Embedding-for-Text-Classification" class="headerlink" title="Multi-Task Label Embedding for Text Classification"></a>Multi-Task Label Embedding for Text Classification</h1><h5 id="论文来源：ACL-2018"><a href="#论文来源：ACL-2018" class="headerlink" title="论文来源：ACL 2018"></a>论文来源：ACL 2018</h5><h5 id="论文链接：https-arxiv-org-abs-1710-07210"><a href="#论文链接：https-arxiv-org-abs-1710-07210" class="headerlink" title="论文链接：https://arxiv.org/abs/1710.07210"></a>论文链接：<a href="https://arxiv.org/abs/1710.07210">https://arxiv.org/abs/1710.07210</a></h5><h5 id="代码链接：https-github-com-guoyinwang-LEAM"><a href="#代码链接：https-github-com-guoyinwang-LEAM" class="headerlink" title="代码链接：https://github.com/guoyinwang/LEAM"></a>代码链接：<a href="https://github.com/guoyinwang/LEAM">https://github.com/guoyinwang/LEAM</a></h5><hr>
<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>这篇文章将label embedding 应用到multi-task learning 当中，制造了一个统一的多任务学习框架。并且认为通过给赋予任务的label 语义信息，可以在多任务间进行迁移学习。</p>
<p>传统的多任务模型有三个缺陷：</p>
<ul>
<li><p>缺少 label 信息：每个任务的标签都用独立的、没有意义的单热点向量来表示，例如情绪分析中的正、负，编码为 [1,0] 和 [0,1]，可能会造成潜在标签信息的丢失。</p>
</li>
<li><p>鲁棒性不够好：网络结构被精心设计来建模多任务学习的各种关联，但大多数网络结构是固定的，只能处理两个任务之间的交互，即成对交互。当引入新的任务时，网络结构必须被修改，整个网络必须再次被训练。</p>
</li>
<li><p>不能迁移：对于人类来说，在学习了几个相关的任务之后，我们可以很容易的就可以处理一个全新的任务，这就是迁移学习的能力。以往大多数模型的网络结构都是固定的，不兼容的，以致于无法处理新的任务。</p>
</li>
</ul>
<p>因此，作者提出了多任务 label embedding (MTLE)，将每个任务的 label 也映射到语义向量中，类似于 word embedding 表示单词序列，从而将原始的文本分类任务转化为向量匹配任务。</p>
<p>利用label embedding 可以做到：</p>
<ul>
<li>增加标签语义信息，帮助模型更好的分裂</li>
<li>可以灵活的增加任务，并且分为直接添加 hot-update 和重新训练 cold-update 方式</li>
<li>利用标签嵌入，将所有分类标签嵌入到同一个语义空间，这样就可以在任务间迁移</li>
</ul>
<h2 id="单任务学习和多任务学习"><a href="#单任务学习和多任务学习" class="headerlink" title="单任务学习和多任务学习"></a>单任务学习和多任务学习</h2><h3 id="单任务学习"><a href="#单任务学习" class="headerlink" title="单任务学习"></a>单任务学习</h3><p>输入为文本序列：$x=\{x_1,x_2,…,x_T\}$</p>
<p>输出为类标签$y$或者one-hot表示$𝐲$</p>
<p>一个预训练好的lookup layer用于将每个单词$x_t$转换为其词嵌入向量$ⅹ_t∈R^d$,分类模型$f$用于对每个$ⅹ=\{ⅹ_1,ⅹ_2,…,ⅹ_T\}$产生一个预测$\hat{𝐲}$</p>
<p>训练目标是最小化其交叉熵损失函数</p>
<script type="math/tex; mode=display">l=-\sum\limits_{i=1}^{N}\sum\limits_{j=1}^{C}y_{ij}log\hat{y}_{ij}</script><p>$N$表示训练样本的数量,$C$表示类的数量。</p>
<h3 id="多任务学习"><a href="#多任务学习" class="headerlink" title="多任务学习"></a>多任务学习</h3><p>具有$K$个分类任务，$T_1,T_2,…,T_K$</p>
<p>模型$F$用于将$T_k$中的每一个$ⅹ^{(k)}$产生一个预测$\{\hat{y}^{(1)},\hat{y}^{(2)},…,\hat{y}^{(K)}\}$</p>
<script type="math/tex; mode=display">F(ⅹ_1^{(k)},ⅹ_2^{(k)},...,ⅹ_T^{(k)})=\{\hat{y}^{(1)},\hat{y}^{(2)},...,\hat{y}^{(K)}\}</script><p>这里只有$\hat{y}^{(k)}$会被用来计算损失。</p>
<script type="math/tex; mode=display">L=-\sum\limits_{k=1}^{K} \lambda_{k} \sum\limits_{i=1}^{N_{k}} \sum\limits_{j=1}^{C_{k}} y_{i j}^{(k)} \log \hat{y}_{i j}^{(k)}</script><p>$λ_k$表示权值，$N_k$表示样本数量，$C_k$表示任务$T_k$的类数量。</p>
<h2 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h2><p>作者提出了三种模型：<br><img src="/2020/11/27/Multi_Task_Label_Embedding_for_Text_Classification/general_idea_of_mtle.png" width="80%"></p>
<p><strong>第一种</strong>假设对于每个任务，我们只有 N 个输入序列和 C 个分类标签，但是缺少每个输入序列和对应标签的具体标注。在这种情况下，只能以无监督的方式实现 MTLE。包含三个部分：input encoder, label encoder, matcher。两个 encoder 将文本编码成定长的向量。</p>
<p><strong>第一种</strong>由于使用了非监督方法，performance 不如有监督的。</p>
<p><img src="/2020/11/27/Multi_Task_Label_Embedding_for_Text_Classification/supervised_model_for_mtle.png" width="80%"></p>
<p><strong>第二种</strong>就是有监督的了，两个 LSTM 分别对 label 和句子进行编码，之后分别 concat，过一层全连接（），得到 logits，个人感觉这个交互做的过于简单。</p>
<script type="math/tex; mode=display">X^{(k)}=LSTM_I(L_{uI}(x^{(k)})</script><script type="math/tex; mode=display">Y_j^{(k)}=LSTM_L(L_{uL}(y_j^{(k)}))</script><script type="math/tex; mode=display">s_j^{(k)}=σ(M_{2m×1}(X^{(k)}⊕Y_j^{(k)}))</script><script type="math/tex; mode=display">l^{(k)}=-\sum\limits_{j=1}^{C_k}\widetilde{y}_j^{(k)}log s_j^{(k)}</script><script type="math/tex; mode=display">L=\sum\limits_{k=1}^K \lambda_{k} \sum\limits_{i=1}^{N_{k}}l_i^{(k)}</script><p>$⊕$表示矩阵拼接。</p>
<p><strong>第三种</strong>则是基于 MTLE 的半监督学习模型。</p>
<p><strong>第二种</strong>和<strong>第三种</strong>之间唯一的不同是它们处理新任务的方式。如果新任务有标签，可以选择第二种的 Hot-Update 或 Cold-Update。如果新的任务完全没有标记，仍然可以使用第二种进行向量映射，无需进一步训练就可以为每个输入序列找到最佳的标记（但是还是映射到原来就有的 label 里），作者将其定义为 Zero-Update。</p>
<p>Hot-Update、Cold-Update 和 Zero-Update 之间的区别如下图所示，其中， Before Update 表示在引入新任务之前对旧任务进行训练的模型。<br><img src="/2020/11/27/Multi_Task_Label_Embedding_for_Text_Classification/hot_cold_zero_update.png" width="80%"></p>
<p><strong>Hot-Update:</strong> 在训练过多个 task 的模型基础上进行 finetune。</p>
<p><strong>Cold-Update:</strong> 在所有的 tasks 上重新训练。</p>
<p><strong>Zero update:</strong> 不更新模型。利用训练过的模型在新 task 上直接得出结果。</p>
]]></content>
      <categories>
        <category>paper</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>label_embedding</tag>
      </tags>
  </entry>
  <entry>
    <title>Sigmoid_and_SoftMax</title>
    <url>/2020/11/27/Sigmoid-and-SoftMax/</url>
    <content><![CDATA[<h1 id="Sigmiod-and-Softmax"><a href="#Sigmiod-and-Softmax" class="headerlink" title="Sigmiod and Softmax"></a>Sigmiod and Softmax</h1><p>设计模型执行分类任务（如对胸部X光检查到的疾病或手写数字进行分类）时，有时需要同时选择多个答案（如同时选择肺炎和脓肿），有时只能选择一个答案（如数字“8”）。本文将讨论如何应用Sigmoid函数或Softmax函数处理分类器的原始输出值。</p>
<h2 id="Sigmoid"><a href="#Sigmoid" class="headerlink" title="Sigmoid:"></a>Sigmoid:</h2><p>Sigmoid =多标签分类问题=多个正确答案=非独占输出（例如胸部X光检查、住院）</p>
<p>构建分类器，解决有多个正确答案的问题时，用Sigmoid函数分别处理各个原始输出值。</p>
<p>Sigmoid函数如下所示（注意e）：</p>
<script type="math/tex; mode=display">σ(z_j)=\frac{e^{z_j}}{1+e^{z_j}}</script><p>原始输出值为[-0.5,1.2，-0.1,2.4]，则$z_1$ = -0.5，$z_2$ = 1.2，$z_3$ = -0.1，$z_4$ = 2.4。</p>
<script type="math/tex; mode=display">σ(z_j)=σ(-0.5)=\frac{e^{-0.5}}{1+e^{-0.5}}=0.3775</script><p>$z_1,z_2,z_3$的计算同上。</p>
<p>由于Sigmoid函数分别应用于每个原始输出值，因此可能出现的输出情况包括：所有类别概率都很低（如“此胸部X光检查没有异常”），一种类别的概率很高但是其他类别的概率很低（如“胸部X光检查仅发现肺炎”），多个或所有类别的概率都很高（如“胸部X光检查发现肺炎和脓肿”）。</p>
<p>下图为Sigmoid函数曲线:</p>
<p><img src="/2020/11/27/Sigmoid-and-SoftMax/sigmoid.jpeg" width="60%"></p>
<h2 id="Softmax"><a href="#Softmax" class="headerlink" title="Softmax:"></a>Softmax:</h2><p>Softmax =多类别分类问题=只有一个正确答案=互斥输出（例如手写数字，鸢尾花）</p>
<p>构建分类器，解决只有唯一正确答案的问题时，用Softmax函数处理各个原始输出值。</p>
<p>Softmax函数的分母综合了原始输出值的所有因素，这意味着，Softmax函数得到的不同概率之间相互关联。</p>
<p>Softmax函数表述如下：</p>
<script type="math/tex; mode=display">softmax(z_j)=\frac{e^{z_j}}{\sum\limits_{k=1}^{K}e^{z_k}}for(j=1,...K)</script><h2 id="应用图示："><a href="#应用图示：" class="headerlink" title="应用图示："></a>应用图示：</h2><p><img src="/2020/11/27/Sigmoid-and-SoftMax/apply_sigmoid.jpeg" alt><br><img src="/2020/11/27/Sigmoid-and-SoftMax/apply_softmax.jpeg" alt></p>
]]></content>
      <categories>
        <category>basic_concept</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>output_function</tag>
        <tag>classification</tag>
      </tags>
  </entry>
  <entry>
    <title>loss_function</title>
    <url>/2020/11/30/loss-function/</url>
    <content><![CDATA[<h1 id="常见的损失函数-loss-function-总结"><a href="#常见的损失函数-loss-function-总结" class="headerlink" title="常见的损失函数(loss function)总结"></a>常见的损失函数(loss function)总结</h1><h5 id="搬运自知乎"><a href="#搬运自知乎" class="headerlink" title="搬运自知乎"></a>搬运自<a href="https://zhuanlan.zhihu.com/p/58883095">知乎</a></h5><hr>
<p><strong>损失函数</strong>用来评价模型的<strong>预测值</strong>和<strong>真实值</strong>不一样的程度，损失函数越好，通常模型的性能越好。不同的模型用的损失函数一般也不一样。</p>
<p><strong>损失函数</strong>分为<strong>经验风险损失函数</strong>和<strong>结构风险损失函数</strong>。经验风险损失函数指预测结果和实际结果的差别，结构风险损失函数是指经验风险损失函数加上正则项。</p>
<p>常见的损失函数以及其优缺点如下：</p>
<h2 id="0-1损失函数-zero-one-loss"><a href="#0-1损失函数-zero-one-loss" class="headerlink" title="0-1损失函数(zero-one loss)"></a>0-1损失函数(zero-one loss)</h2><p>0-1损失是指预测值和目标值不相等为1， 否则为0:</p>
<script type="math/tex; mode=display">f(x)=
\begin{cases}
1&,& Y \neq f(X)\\\\
0&,& Y = f(x)
\end{cases}</script><p>特点：</p>
<ul>
<li><p>0-1损失函数直接对应分类判断错误的个数，但是它是一个非凸函数，不太适用.</p>
</li>
<li><p>感知机就是用的这种损失函数。但是相等这个条件太过严格，因此可以放宽条件，即满足 $|Y-f(X)|&lt;T$ 时认为相等，</p>
</li>
</ul>
<script type="math/tex; mode=display">f(x)=
\begin{cases}
1&,& |Y-f(X)| \geq T\\\\
0&,& |Y-f(x)| <T
\end{cases}</script><h2 id="绝对值损失函数"><a href="#绝对值损失函数" class="headerlink" title="绝对值损失函数"></a>绝对值损失函数</h2><p>绝对值损失函数是计算预测值与目标值的差的绝对值：</p>
<script type="math/tex; mode=display">L(Y,f(x))=|Y-f(x)|</script><h2 id="log对数损失函数"><a href="#log对数损失函数" class="headerlink" title="log对数损失函数"></a>log对数损失函数</h2><p><strong>log对数损失函数</strong>的标准形式如下：</p>
<script type="math/tex; mode=display">L(Y,P(Y|X))=-logP(Y|X)</script><p>特点：</p>
<ul>
<li><p>log对数损失函数能非常好的表征概率分布，在很多场景尤其是多分类，如果需要知道结果属于每个类别的置信度，那它非常适合。</p>
</li>
<li><p>健壮性不强，相比于hinge loss对噪声更敏感。</p>
</li>
<li><p><strong>逻辑回归</strong>的损失函数就是log对数损失函数。</p>
</li>
</ul>
<h2 id="平方损失函数"><a href="#平方损失函数" class="headerlink" title="平方损失函数"></a>平方损失函数</h2><p>平方损失函数标准形式如下：</p>
<script type="math/tex; mode=display">L(Y|f(X))=\sum\limits_{N}(Y-f(X))^{2}</script><p>特点：</p>
<ul>
<li>经常应用与回归问题</li>
</ul>
<h2 id="指数损失函数（exponential-loss）"><a href="#指数损失函数（exponential-loss）" class="headerlink" title="指数损失函数（exponential loss）"></a>指数损失函数（exponential loss）</h2><p><strong>指数损失函数</strong>的标准形式如下：</p>
<script type="math/tex; mode=display">L(Y|f(X))=exp[-yf(x)]</script><p>特点：</p>
<ul>
<li>对离群点、噪声非常敏感。经常用在<strong>AdaBoost</strong>算法中。</li>
</ul>
<h2 id="Hinge-损失函数"><a href="#Hinge-损失函数" class="headerlink" title="Hinge 损失函数"></a>Hinge 损失函数</h2><p>Hinge损失函数标准形式如下：</p>
<script type="math/tex; mode=display">L(y,f(x))=max(0,1-yf(x))</script><p>特点：</p>
<ul>
<li><p>hinge损失函数表示如果被分类正确，损失为0，否则损失就为 $1-yf(x)$ 。<strong>SVM</strong>就是使用这个损失函数。</p>
</li>
<li><p>一般的 $f(x)$ 是预测值，在-1到1之间， $y$ 是目标值(-1或1)。其含义是， $f(x)$ 的值在-1和+1之间就可以了，并不鼓励 $|f(x)|&gt;1$ ，即并不鼓励分类器过度自信，让某个正确分类的样本距离分割线超过1并不会有任何奖励，从而使<strong>分类器可以更专注于整体的误差。</strong></p>
</li>
<li><p>健壮性相对较高，对异常点、噪声不敏感，但它没太好的概率解释。</p>
</li>
</ul>
<h2 id="感知损失-perceptron-loss-函数"><a href="#感知损失-perceptron-loss-函数" class="headerlink" title="感知损失(perceptron loss)函数"></a>感知损失(perceptron loss)函数</h2><p><strong>感知损失函数</strong>的标准形式如下：</p>
<script type="math/tex; mode=display">L(y,f(x))=max(0,-f(x))</script><p>特点：</p>
<ul>
<li>是Hinge损失函数的一个变种，Hinge loss对判定边界附近的点(正确端)惩罚力度很高。而perceptron loss<strong>只要样本的判定类别正确的话，它就满意，不管其判定边界的距离</strong>。它比Hinge loss简单，因为不是max-margin boundary，所以<strong>模型的泛化能力没 hinge loss强</strong>。</li>
</ul>
<h2 id="交叉熵损失函数-Cross-entropy-loss-function"><a href="#交叉熵损失函数-Cross-entropy-loss-function" class="headerlink" title="交叉熵损失函数 (Cross-entropy loss function)"></a>交叉熵损失函数 (Cross-entropy loss function)</h2><p>交叉熵损失函数的标准形式如下:</p>
<script type="math/tex; mode=display">C=- \frac{1}{n} \sum\limits_x [ylna+(1-y)ln(1-a)]</script><p>注意公式中 $x$ 表示样本， $y$ 表示实际的标签， $a$ 表示预测的输出， $n$ 表示样本总数量。</p>
<p>特点：</p>
<ul>
<li>本质上也是一种对数似然函数，可用于二分类和多分类任务中。</li>
</ul>
<p>二分类问题中的loss函数（输入数据是softmax或者sigmoid函数的输出）：</p>
<script type="math/tex; mode=display">loss=- \frac{1}{n} \sum\limits_x [ylna+(1-y)ln(1-a)]</script><p>多分类问题中的loss函数（输入数据是softmax或者sigmoid函数的输出）：</p>
<script type="math/tex; mode=display">loss = - \frac{1}{n} \sum\limits_i y_i lna_i</script><ul>
<li>当使用sigmoid作为激活函数的时候，常用<strong>交叉熵损失函数</strong>而不用<strong>均方误差损失函数</strong>，因为它可以<strong>完美解决平方损失函数权重更新过慢</strong>的问题，具有“误差大的时候，权重更新快；误差小的时候，权重更新慢”的良好性质。</li>
</ul>
]]></content>
      <categories>
        <category>basic_concept</category>
      </categories>
      <tags>
        <tag>loss_function</tag>
      </tags>
  </entry>
  <entry>
    <title>language_model</title>
    <url>/2020/12/01/language-model/</url>
    <content><![CDATA[<h1 id="NLP中的语言模型-language-model"><a href="#NLP中的语言模型-language-model" class="headerlink" title="NLP中的语言模型(language model)"></a>NLP中的语言模型(language model)</h1><h4 id="本文搬运自CSDN，并修改了其中一些错误。"><a href="#本文搬运自CSDN，并修改了其中一些错误。" class="headerlink" title="本文搬运自CSDN，并修改了其中一些错误。"></a>本文搬运自<a href="https://blog.csdn.net/huanghaocs/article/details/77935556">CSDN</a>，并修改了其中一些错误。</h4><hr>
<h2 id="什么是语言模型"><a href="#什么是语言模型" class="headerlink" title="什么是语言模型?"></a>什么是语言模型?</h2><p>统计语言模型是一个单词序列上的<strong>概率分布</strong>，对于一个给定长度为m的序列，它可以为整个序列产生一个概率 P(w_1,w_2,…,w_m) 。其实就是想办法找到一个概率分布，它可以<strong>表示任意一个句子或序列出现的概率</strong>。</p>
<p>目前在自然语言处理相关应用非常广泛，如语音识别(speech recognition) , 机器翻译(machine translation), 词性标注(part-of-speech tagging), 句法分析(parsing)等。传统方法主要是基于统计学模型，最近几年基于神经网络的语言模型也越来越成熟。</p>
<h2 id="Unigram-models"><a href="#Unigram-models" class="headerlink" title="Unigram models"></a>Unigram models</h2><p>Unigram models也即一元文法模型，它是一种上下文无关模型。该模型仅仅考虑当前词本身出现的概率，而不考虑当前词的上下文环境。概率形式为</p>
<script type="math/tex; mode=display">P ( w_1 , w_2 , . . . , w_m ) = P ( w_1 ) ∗ P ( w_2 ) ∗ . . . ∗ P ( w_m ) P(w_1,w_2,...,w_m)</script><p>即一个句子出现的概率等于句子中<strong>每个单词概率乘积</strong>。<br>以一篇文档为例，每个单词的概率只取决于该单词本身在文档中的概率，而文档中所有词出现的概率和为1，每个词的概率可以用该词在文档中出现的频率来表示，如下表中</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Terms</th>
<th>Probability</th>
</tr>
</thead>
<tbody>
<tr>
<td>a</td>
<td>0.1</td>
</tr>
<tr>
<td>world</td>
<td>0.2</td>
</tr>
<tr>
<td>likes</td>
<td>0.05</td>
</tr>
<tr>
<td>we</td>
<td>0.03</td>
</tr>
<tr>
<td>share</td>
<td>0.26</td>
</tr>
<tr>
<td>…</td>
<td>…</td>
</tr>
</tbody>
</table>
</div>
<p>对于这篇文档中，所有概率和相加为1，即</p>
<p>$\sum P(term) = 1$</p>
<h2 id="n-gram模型"><a href="#n-gram模型" class="headerlink" title="n-gram模型"></a>n-gram模型</h2><p>n-gram models也即n元语言模型，针对一个句子$w_1,w_2,…,w_m$的概率表示如下：</p>
<script type="math/tex; mode=display">P(w_1,w_2,...,w_m)= \prod_{i=1}^{m}P(w_i|w_1,...,w_{i-1})=\prod_{i=1}^{m}P(w_i|w_{i-(n-1),...,w_{i-1}})</script><p>这里可以理解为当前词的概率与前面的n个词有关系，可以理解为上下文有关模型。</p>
<p>n-gram模型中的条件概率可以用词频来计算：</p>
<script type="math/tex; mode=display">P(w_i|w_{i-(n-1)},...,w_{i-1})=\frac{count(w_{i-(n-1)},...,w_{i-1},w_i)}{count(w_{i-(n-1)},...,w_{i-1})}</script><p>​    </p>
<p>这里举个栗子：我们的原始文档是 doc = “我不想写代码了” ，经过中文分词处理后为 words = [“我”, “不想”, “写”, “代码”, “了”]。</p>
<p>那么产生原始文档doc的概率为：<br>P(我, 不想, 写, 代码, 了) = P(我) x P(不想|我) x P(写|我, 不想) x P(代码|我, 不想, 写) x P (了|我, 不想, 写, 代码)</p>
<p>相乘的每个概率可以通过统计词频来获得：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>n-gram</th>
<th>probability</th>
</tr>
</thead>
<tbody>
<tr>
<td>P(不想 &#124; 我) =</td>
<td>count(我, 不想) / count(我)</td>
</tr>
<tr>
<td>P(写 &#124; 我, 不想) =</td>
<td>count(我, 不想, 写) / count(我, 不想)</td>
</tr>
<tr>
<td>P(代码 &#124; 我, 不想, 写) =</td>
<td>count(我, 不想, 写, 代码) / count(我, 不想, 写)</td>
</tr>
<tr>
<td>P(了 &#124; 我, 不想, 写, 代码) =</td>
<td>count(我, 不想, 写, 代码, 了) / count(我, 不想, 写, 代码)</td>
</tr>
</tbody>
</table>
</div>
<p>如果像上面这样去计算，会疯掉的。。。这种计算太复杂了，所以我们是否可以简化只考虑少数的词呢，如二元bigram模型，当前词只与它前面的一个词相关，这样概率求解就简化很多：</p>
<script type="math/tex; mode=display">P(w_1,w_2,...,w_m)=\prod_{i=1}^{m}P(w_i|w_{i-(n-1),...,w_{i-1}}) = \prod_{i=1}^{m}P(w_i|w_{w_{i-1}})</script><p>上面那个栗子也可以简化了：</p>
<p>P(我, 不想, 写, 代码, 了) = P(我) x P(不想|我) x P(写|不想) x P(代码|写) x P (了|代码)</p>
<p>相乘的每个概率就更简单了：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>bigram</th>
<th>probability</th>
</tr>
</thead>
<tbody>
<tr>
<td>P(不想 ｜ 我) =</td>
<td>count(我, 不想) / count(我)</td>
</tr>
<tr>
<td>P(写 ｜ 不想) =</td>
<td>count(不想, 写) / count(不想)</td>
</tr>
<tr>
<td>P(代码 ｜ 写) =</td>
<td>count(写, 代码) / count(写)</td>
</tr>
<tr>
<td>P(了 ｜ 代码) =</td>
<td>count(代码, 了) / count(代码)</td>
</tr>
</tbody>
</table>
</div>
<p>这样我们只需要计算语料中每个词出现的次数，以及每两个词出现的次数，就可以求上面的概率了。</p>
<p>这里是以二元模型说明，你也可以使用三元(trigram)、四元模型，方法类似不在多说。前面提到的 Unigram模型其实就是一元模型。</p>
<h2 id="神经网络语言模型-NNLM"><a href="#神经网络语言模型-NNLM" class="headerlink" title="神经网络语言模型(NNLM)"></a>神经网络语言模型(NNLM)</h2><p>神经语言模型使用连续表示或词汇Embedding来进行预测。 以神经网络为基础来训练模型。</p>
<p><img src="/2020/12/01/language-model/nnlm.png" width="80%"></p>
<p>输入：n-1个之前的word（用词典库V中的index表示）</p>
<p>映射：通过|V|*D的矩阵C映射到D维</p>
<p>隐层：映射层连接大小为H的隐层</p>
<p>输出：输出层大小为|V|，表示|V|个词中每个词的概率</p>
<p>网络第一层：将 $C(w_{t-n+1}), … ,C(w_{t-2}),C(w_{t-1})$这n-1个向量首尾相接拼起来，形成一个(n-1)*m维的向量，记为输出x。</p>
<p>网络第二层：就是神经网络的隐藏层，直接使用 $d+Hx$ 计算得到，H是权重矩阵，d是一个偏置向量。然后使用tanh作为激活函数。</p>
<p>网络第三层：输出一共有|V|个节点，每个节点 $y_i$ 表示下一个词为i的未归一化log概率，最后使用softmax激活函数将输出值y归一化成概率。y的计算公式：</p>
<script type="math/tex; mode=display">y = b+Wx+Utanh(d+Hx)</script><p>式中U是一个|V| X h 的矩阵，表示隐藏层到输出层的参数；W是|V| x (n-1)m的矩阵，这个矩阵包含了从输入层到输出层的直连边，就是从输入层直接到输出层的一个线性变换。</p>
<script type="math/tex; mode=display">p = softmax(y)</script><h2 id="word2vec模型"><a href="#word2vec模型" class="headerlink" title="word2vec模型"></a>word2vec模型</h2><p><strong>word2vec</strong>分为两个基础模型<strong>CBOW</strong>和<strong>Skip-gram</strong>，其中CBOW模型是根据上下文词预测当前词，Skip-gram模型是根据当前词预测它的上下文，其实都是在发现语料中局部词汇之间的共现关系。</p>
<p><img src="/2020/12/01/language-model/cbow_skip-gram.png" width="80%"></p>
<h3 id="CBOW"><a href="#CBOW" class="headerlink" title="CBOW"></a>CBOW</h3><p>CBOW模型是在语料中设置一个窗口，每次滑动这个窗口，根据窗口除中心词之前的其他词，来预测中心词，就是根据上下文窗口词预测当前中心词，还是以前面的例子具体说明一下，<br>原始doc = “我真不想写代码了”，分词后 words = [“我”, “真”, “不想”, “写”, “代码”, “了”]。<br>如果窗口window_size = 3, 则window1 = [“真”, “不想”, “写”]、window2 = [“不想”, “写”, “代码”]等3个词的窗口，对应CBOW模型的输入和输出：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>input</th>
<th>output</th>
</tr>
</thead>
<tbody>
<tr>
<td>[“真”, “写”]</td>
<td>“不想”</td>
</tr>
<tr>
<td>[“不想”, “代码”]</td>
<td>“写”</td>
</tr>
</tbody>
</table>
</div>
<p>如果窗口window_size = 5, 则window1 = [“我”, “真”, “不想”, “写”, “代码”]、 window2 = [“真”, “不想”, “写”, “代码”, “了”] 等5个词的窗口，对应CBOW模型的输入和输出：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>input</th>
<th>output</th>
</tr>
</thead>
<tbody>
<tr>
<td>[“我”, “真”, “写”, “代码”]</td>
<td>“不想”</td>
</tr>
<tr>
<td>[“真”, “不想”, “代码”, “了”]</td>
<td>“写”</td>
</tr>
</tbody>
</table>
</div>
<p>而具体计算是要获得每个输入词的向量表示，然后求和取平均值，最后完成词汇表大小的分类任务。<br>对应公式就是假设当前词是w，w对应的上下文窗口词是context(w)，windows_size = 2c + 1，context(w)由w前后各c个词构成。</p>
<p><strong>输入层</strong>：包含context(w)中2c个词的词向量，$v(context(w)_1),v(context(w)_2)$，…。这里的词向量通过词编号映射得到，也就是我们模型要学习的词向量，词向量维度设为m。</p>
<p><strong>投影层</strong>：对输入的2c个向量做求和累加，公式标识如下， 其中 $x_w \in R^mx$</p>
<script type="math/tex; mode=display">x_w = \sum_{i=1}^{2c}v({Context(w)}_i)</script><p>对应上面window_size = 5的第一个例子就是 $x_{不想}$ = v(我) + v(真) + v(写) + v(代码)。</p>
<p><strong>输出层</strong>：对应输出层最直观的方法是，投影层得到的 $x_w$</p>
<p>直接乘以一个和词表大小一样的权重矩阵，然后过一层softmax函数得到词表大小的一个概率分布向量，取其中概率最大的为预测结果词。设U是一个$V × m$的矩阵，V对应词表大小，m对应词向量维度，b对应偏置项，我们可以得到上下文context(w)预测当前词w的概率分布。</p>
<script type="math/tex; mode=display">p(w|context(w)) = softmax(Ux_w + b)</script><p>然后用概率分布$p(w|context(w))$与真实label的交叉熵作为损失函数，来训练模型。这里理论上可行，但是训练速度很慢，一般词表大小v都在几十万，每次求softmax是非常耗时。所以word2vec就在这个输出还用层次softmax来优化，从而加速模型训练过程。</p>
<h3 id="Hierarchical-Softmax（层次Softmax）"><a href="#Hierarchical-Softmax（层次Softmax）" class="headerlink" title="Hierarchical Softmax（层次Softmax）"></a>Hierarchical Softmax（层次Softmax）</h3><p>具体就是根据词表中词的出现频率作为权重来构造Huffman树，这样树的叶子节点对应的就是每个词，按照叶子节点对应的路劲就能找到每个词，简单理解就是我们用这样一颗树来存储我们的词表，可以加速输出层的计算。<br>具体过程首先定义几个变量：</p>
<ul>
<li><p>$p^w$从根节点出发到词w对应叶子节点的路劲；</p>
</li>
<li><p>$l^w$ 路径 $p^w$ 中包含的结点个数；</p>
</li>
<li><p>$p_1^w,p_2^w,…,p_{l^w}^w$ 路径$p^w$中的$l^w$个节点，其中$p_1^w$表示根结点，$p_{l^w}^w$表示词$w$对应的结点；</p>
</li>
<li><p>$d_2^w,d_3^w,…,d_{l^w}^w \in \{0,1\}$ 词$w$的Huffman编码，它由$l^w-1$位编码构成，$d_j^w$表示路径$p^w$中第$j$个结点对应的编码（根结点不对应编码）</p>
</li>
<li><p>$\theta_1^w,\theta_2^w,…,\theta_{l^w-1}^w \in R^m$ 路径$p^w$中<strong>非叶子结点</strong>对应的向量，$\theta_j^w$表示路径$p^w$中第$j$个非叶子结点对应的向量。 </p>
</li>
</ul>
<p><em>这里为什么要给Haffuman树中的非叶子向量也定义一个同长的向量呢？ 其实它们只是辅助向量</em></p>
<p>计算概率的方法是对词w所在路径每个节点进行二分类，并把节点二分类的概率进行相乘，从而得到词w的概率$p(w|context(w))$。对应的二分类可以用逻辑回归，假设0代表正类，1代表负类，</p>
<h4 id="正类概率-："><a href="#正类概率-：" class="headerlink" title="正类概率 ："></a>正类概率 ：</h4><script type="math/tex; mode=display">p(0|x_w^{T}) = \sigma (x_w^{T}) = \frac{1}{1 + e^{-x_w^{T}\theta}}</script><h4 id="负类概率："><a href="#负类概率：" class="headerlink" title="负类概率："></a>负类概率：</h4><script type="math/tex; mode=display">p(1|x_w^{T}) = 1 - \sigma (x_w^{T})</script><p>求得词w对应路劲上每个节点的分类概率后，$p(w|context(w))$对应跟一般的概率表示：</p>
<script type="math/tex; mode=display">p(w|context(w)) = \prod_{j=2}^{l^w} p(d_j^w|x_w,{\theta}_{j-1}^w)</script><p>其中根据逻辑回归可得：</p>
<script type="math/tex; mode=display">p(d_j^w|x_w,{\theta}_{j-1}^w) = 
\begin{cases}
σ(x_w^T θ_{j−1}^w)&,&d_j^w=0 \\\\
1−σ(x_w^Tθ_{j−1}^w)&,&d_j^w=1 
\end{cases}</script><p>上式也可以表示为：</p>
<script type="math/tex; mode=display">p(d_{j}^{w}|x_{w},{\theta}_{j-1}^{w}) = {[\sigma(x_{w}^{T}{\theta}_{j-1}^{w})]^{1-d_{j}^{w}}}\cdot [1- \sigma(x_{w}^{T}{\theta}_{j-1}^{w})]^{d_{j}^{w}}</script><p>p(w|context(w))可以表示为：</p>
<script type="math/tex; mode=display">p(w|context(w)) = \prod_{j=2}^{l^w}p(d_j^w|x_w,{\theta}_{j-1}^w) = \prod_{j=2}^{l^w}[\sigma(x_w^T{\theta}_{j-1}^w)]^{1-d_j^w}\cdot [1- \sigma(x_w^T{\theta}_{j-1}^w)]^{d_j^w}</script><p>优化的目标，对数似然函数为：</p>
<script type="math/tex; mode=display">L = \sum_{w\in C}\log{p(w|Context(w))}</script><p>将p(w|context(w))带入到对数似然函数中，得到最终的优化函数：</p>
<script type="math/tex; mode=display">L = \sum_{w\in C}\log{\prod_{j=2}^{l^w}[\sigma(x_w^T{\theta}_{j-1}^w)]^{1-d_j^w}\cdot [1- \sigma(x_w^T{\theta}_{j-1}^w)]^{d_j^w}} \\ =\sum_{w\in C}\sum_{j=2}^{l^w}\{(1-d_j^w)\log[\sigma(x_w^T{\theta}_{j-1}^w)]+d_j^w\log[1- \sigma(x_w^T{\theta}_{j-1}^w)]\}</script><h3 id="Skip-gram"><a href="#Skip-gram" class="headerlink" title="Skip-gram"></a>Skip-gram</h3><p>Skip-gram模型也要设置一个窗口，每次滑动窗口，根据窗口中心词预测窗口内其他词，就是根据当前词预测上下文窗口词。还是以例子先理解一下。<br>原始doc = “我不想写代码了”，分词后 words = [“我”, “真”, “不想”, “写”, “代码”, “了”]。<br>如果窗口window_size = 3， 则window1 = [“真”, “不想”, “写”]、window2 = [“不想”, “写”, “代码”]，对应Skip-gram模型的输入和输出：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>input</th>
<th>context</th>
</tr>
</thead>
<tbody>
<tr>
<td>“不想”</td>
<td>[“真” ,“写” ]</td>
</tr>
<tr>
<td>“写”</td>
<td>[“不想” ,“代码”]</td>
</tr>
</tbody>
</table>
</div>
<p>如果窗口window_size = 5， 则window1 = [“我”, “真”, “不想”, “写”, “代码”]、window2 = [“真”, “不想”, “写”, “代码”, “了”]，对应Skip-gram模型的输入和输出：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>input</th>
<th>context</th>
</tr>
</thead>
<tbody>
<tr>
<td>“不想”</td>
<td>[“我” ,“真” ,“写”,“代码” ]</td>
</tr>
<tr>
<td>“写”</td>
<td>[“真”,“不想”, “代码”, “了” ]</td>
</tr>
</tbody>
</table>
</div>
<p>Skip-gram中给出当前词w，需要预测它的上下文词Context(w)，概率表示如下：</p>
<script type="math/tex; mode=display">p(Context(w)|w) = \prod_{u\in Context(w)}p(u|w)</script><p>这里的w就是上面表格中的input，u就是input对应的context中的词，这里取的时context中的词概率相乘。<br>比如对上面的例子input=“不想”, context = [“我” ,“真” ,“写”,“代码” ]，概率计算如下：<br>p(Context(不想)|不想) = p(我|不想) x p(真|不想) x p(写|不想) x p(代码|不想)</p>
<h4 id="Hierarchical-Softmax（层次Softmax）-1"><a href="#Hierarchical-Softmax（层次Softmax）-1" class="headerlink" title="Hierarchical Softmax（层次Softmax）"></a>Hierarchical Softmax（层次Softmax）</h4><p>类似前面CBOW模型，输出层也是用层次softmax来优化。上面p(u|w)就可以表示成词w的向量表示 $x_w$ 与词u的哈夫曼树路径表示相乘。</p>
<script type="math/tex; mode=display">p(u|w) = \prod_{j=2}^{l^u}p(d_j^u|x_w,\theta _{j-1}^u)</script><p>路径中每一步概率可表示为：</p>
<script type="math/tex; mode=display">p(d_j^u|x_w,{\theta}_{j-1}^u) = {[\sigma(x_w^T{\theta}_{j-1}^u)]^{1-d_j^u}}\cdot [1- \sigma(x_w^T{\theta}_{j-1}^u)]^{d_j^u}</script><p>同样对优化的目标，对数似然函数为：</p>
<script type="math/tex; mode=display">L = \sum_{w\in C}\log{p(Context(w)|w)}</script><p>带入p(u|w) 得到最终的优化函数：</p>
<script type="math/tex; mode=display">L = \sum_{w\in C}\log\prod_{u\in Context(w)}p(u|w) = \sum_{w\in C}\sum_{u\in Context(w)}\log p(u|w) \\ = \sum_{w\in C}\sum_{u\in Context(w)}\log{\prod_{j=2}^{l^u}[\sigma(x_w^T{\theta}_{j-1}^u)]^{1-d_j^u}\cdot [1- \sigma(x_w^T{\theta}_{j-1}^u)]^{d_j^u}} \\ =\sum_{w\in C}\sum_{u\in Context(w)}\sum_{j=2}^{l^u}\{(1-d_j^u)\log[\sigma(x_w^T{\theta}_{j-1}^u)]+d_j^u\log[1- \sigma(x_w^T{\theta}_{j-1}^u)]\}</script><p><strong>关于word2vec中的数学原理，这篇博客讲的很详细</strong><a href="https://www.cnblogs.com/peghoty/p/3857839.html">word2vec 中的数学原理详解</a></p>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>language_model</tag>
      </tags>
  </entry>
  <entry>
    <title>GCN</title>
    <url>/2020/12/02/GCN/</url>
    <content><![CDATA[<h1 id="Graph-Convolutional-Network"><a href="#Graph-Convolutional-Network" class="headerlink" title="Graph Convolutional Network"></a>Graph Convolutional Network</h1><hr>
<p>关于图卷积这里有两篇文章都讲的很好，</p>
<p>这一篇讲解了图卷机的相关知识和概念什么的，都很全面</p>
<p><a href="https://www.zhihu.com/question/54504471/answer/332657604">如何理解 Graph Convolutional Network</a></p>
<p>这一篇从物理和数学的角度生动形象的解释了图卷积的意义和机理，可以帮我我们更好的理解。</p>
<p><a href="https://www.zhihu.com/question/54504471/answer/630639025">如何理解 Graph Convolutional Network</a></p>
<hr>
<p>目前自己是先看这两篇博客了解下图卷积的相关知识，后面需要深入学习的时候在总结吧。</p>
<hr>
<p>下面是一些简单的理解</p>
<h2 id="图的定义"><a href="#图的定义" class="headerlink" title="图的定义"></a>图的定义</h2><p>对于图，我们有以下特征定义：</p>
<p>对于图 $G=(V,E)$ ， $V$ 为节点的集合， $E$ 为边的集合，对于每个节点 $i$ ， 均有其特征 $x_i$ ，可以用矩阵 $X_{N×D}$ 表示。其中 $N$ 表示节点数， $D$ 表示每个节点的特征数，也可以说是特征向量的维度。</p>
<h2 id="图卷积的形象化理解"><a href="#图卷积的形象化理解" class="headerlink" title="图卷积的形象化理解"></a>图卷积的形象化理解</h2><p>在一头扎进图卷积公式之前，我们先从其他的角度理解一下这个操作的物理含义，有一个形象化的理解，我们在试图得到节点表示的时候，容易想到的最方便有效的手段就是利用它周围的节点，也就是它的邻居节点或者邻居的邻居等等，这种思想可以归结为一句话：</p>
<p><strong>图中的每个结点无时无刻不因为邻居和更远的点的影响而在改变着自己的状态直到最终的平衡，关系越亲近的邻居影响越大。</strong></p>
<h2 id="图相关矩阵的定义"><a href="#图相关矩阵的定义" class="headerlink" title="图相关矩阵的定义"></a>图相关矩阵的定义</h2><p><img src="/2020/12/02/GCN/matrix.png" width="90%"></p>
<p>其Laplacian 矩阵的定义为 $L=D-A$，其中 $L$ 是Laplacian 矩阵， $D$ 是顶点的度矩阵（对角矩阵），对角线上元素依次为各个顶点的度， $A$ 是图的邻接矩阵。</p>
<h2 id="图卷积的通式"><a href="#图卷积的通式" class="headerlink" title="图卷积的通式"></a>图卷积的通式</h2><p>任何一个图卷积层都可以写成这样一个非线性函数：</p>
<script type="math/tex; mode=display">H^{l+1}=f(H^l,A)</script><p>$H^0=X$ 为第一层的输入， $X \in R^{N×D}$ ， $N$ 为图的节点个数， $D$ 为每个节点特征向量的维度， $A$ 为邻接矩阵，不同模型的差异点在于函数 $f$ 的实现不同。</p>
<p>下面介绍几种具体的实现，但是每一种实现的参数大家都统称拉普拉斯矩阵。</p>
<h3 id="实现一"><a href="#实现一" class="headerlink" title="实现一"></a>实现一</h3><script type="math/tex; mode=display">H^{l+1}= \sigma(AH^lW^l)</script><p>其中 $W^l$ 为第 $l$ 层的权重参数矩阵， $\sigma( ・ )$ 为非线性激活函数，例如ReLU。</p>
<p>这种思路是基于节点特征与其所有邻居节点有关的思想。邻接矩阵 $A$ 与特征 $H$ 相乘，等价于，某节点的邻居节点的特征相加。这样多层隐含层叠加，能利用多层邻居的信息。</p>
<p>但这样存在两个问题：</p>
<p><strong>没有考虑节点自身对自己的影响；</strong></p>
<p><strong>邻接矩阵$A$没有被规范化，这在提取图特征时可能存在问题，比如邻居节点多的节点倾向于有更大的影响力。</strong></p>
<p>因此实现二和实现三针对这两点进行了优化。</p>
<h3 id="实现二"><a href="#实现二" class="headerlink" title="实现二"></a>实现二</h3><script type="math/tex; mode=display">H^{l+1}=\sigma(LH^lW^l)</script><p>拉普拉斯矩阵 $L=D-A$ ，学名Combinatorial Laplacian​，是针对实现一的问题1的改进：</p>
<p><strong>引入了度矩阵，从而解决了没有考虑自身节点信息自传递的问题</strong></p>
<h3 id="实现三"><a href="#实现三" class="headerlink" title="实现三"></a>实现三</h3><script type="math/tex; mode=display">H^{l+1}=\sigma(D^{- \frac{1}{2}}\hat{A}D^{- \frac{1}{2}}H^lW^l)</script><p>对于这里的拉普拉斯矩阵</p>
<script type="math/tex; mode=display">
L^{s y m}=D^{-\frac{1}{2}} \hat{A} D^{-\frac{1}{2}}=D^{-\frac{1}{2}}(D-A) D^{-\frac{1}{2}}=I_{n}-D^{-\frac{1}{2}} A D^{-\frac{1}{2}}</script><p>本质上还是实现一的两个问题进行的改进：</p>
<p><strong>引入自身度矩阵，解决自传递问题；</strong><br><strong>对邻接矩阵的归一化操作，通过对邻接矩阵两边乘以节点的度开方然后取逆得到。</strong></p>
<p>具体到每一个节点对 $i,j$ ，矩阵中的元素由下面的式子给出（对于无向无权图）：</p>
<script type="math/tex; mode=display">
L_{i, j}^{\mathrm{sym}}:=\left\{\begin{array}{ll}
1 & \text { if } i=j \text { and } \operatorname{deg}\left(v_{i}\right) \neq 0 \\
\frac{1}{\sqrt{\operatorname{deg}\left(v_{i}\right) \operatorname{dog}\left(v_{j}\right)}} & \text { if } i \neq j \text { and } v_{i} \text { is adjacent to } v_{j} \\
0 & \text { otherwise. }
\end{array}\right.</script><p>其中 $deg(v_i),deg(v_j)$ 分别为节点 $i,j$ 的度，也就是度矩阵在节点 $i,j$ 处的值。</p>
]]></content>
      <categories>
        <category>basic_concept</category>
      </categories>
      <tags>
        <tag>graph convolutional network</tag>
      </tags>
  </entry>
  <entry>
    <title>attention_mechanism</title>
    <url>/2020/12/02/attention-mechanism/</url>
    <content><![CDATA[<h1 id="Attention-Mechanism"><a href="#Attention-Mechanism" class="headerlink" title="Attention Mechanism"></a>Attention Mechanism</h1><h4 id="搬运自简书"><a href="#搬运自简书" class="headerlink" title="搬运自简书"></a>搬运自<a href="https://www.jianshu.com/p/e14c6a722381">简书</a></h4><hr>
<h2 id="什么是Attention机制？"><a href="#什么是Attention机制？" class="headerlink" title="什么是Attention机制？"></a>什么是Attention机制？</h2><p>当我们人在看一样东西的时候，我们当前时刻关注的一定是我们当前正在看的这样东西的某一地方，换句话说，当我们目光移到别处时，注意力随着目光的移动也在转移，这意味着，当人们注意到某个目标或某个场景时，该目标内部以及该场景内每一处空间位置上的注意力分布是不一样的。————-（思考：对于图片，会有些特别显眼的场景会率先吸引住注意力，那是因为脑袋中对这类东西很敏感。对于文本，我们大都是带目的性的去读，顺序查找，顺序读，但是在理解的过程中，我们是根据我们自带的目的去理解，去关注的。  注意力模型应该与具体的目的(或者任务)相结合。）</p>
<p>从Attention的作用角度出发，我们就可以从两个角度来分类Attention种类：<strong>Spatial Attention空间注意力和Temporal Attention时间注意力</strong>。更具实际的应用，也可以将Attention分为<strong>Soft Attention和Hard Attention</strong>。Soft Attention是所有的数据都会注意，都会计算出相应的注意力权值，不会设置筛选条件。Hard Attention会在生成注意力权重后筛选掉一部分不符合条件的注意力，让它的注意力权值为0，即可以理解为不再注意这些不符合条件的部分。</p>
<h2 id="先了解编码-解码框架：Encoder-Decoder框架"><a href="#先了解编码-解码框架：Encoder-Decoder框架" class="headerlink" title="先了解编码-解码框架：Encoder-Decoder框架"></a>先了解编码-解码框架：Encoder-Decoder框架</h2><p>目前绝大多数文献中出现的AM模型是附着在Encoder-Decoder框架下的，当然，其实AM模型可以看作一种通用的思想，本身并不依赖于Encoder-Decoder模型，这点需要注意。<strong>Encoder-Decoder框架可以看作是一种文本处理领域的研究模式</strong>，应用场景异常广泛，本身就值得细谈。</p>
<p><img src="/2020/12/02/attention-mechanism/encoder_decoder.png" width="80%"></p>
<p>Encoder-Decoder框架可以这么直观地去理解：可以把它看作适合处理由一个句子（或篇章）生成另外一个句子（或篇章）的通用处理模型。对于句子对<X,Y>。  ————（思考：<X,Y>对很通用，X是一个问句，Y是答案；X是一个句子，Y是抽取的关系三元组；X是汉语句子，Y是汉语句子的英文翻译。等等），我们的目标是给定输入句子X，期待通过Encoder-Decoder框架来生成目标句子Y。X和Y可以是同一种语言，也可以是两种不同的语言。而X和Y分别由各自的单词序列构成：</X,Y></X,Y></p>
<script type="math/tex; mode=display">X=(x_1,x_2,...,x_m)</script><script type="math/tex; mode=display">Y=(y_1,y_2,...,y_n)</script><p>Encoder顾名思义就是对输入句子X进行编码，将输入句子通过非线性变换转化为中间语义表示C：</p>
<script type="math/tex; mode=display">C=ℱ(x_1,x_2,...,x_m)</script><p>对于解码器Decoder来说，其任务是根据句子X的中间语义表示C和之前已经生成的历史信息y1,y2….yi-1来生成i时刻要生成的单词yi ：</p>
<script type="math/tex; mode=display">y_i=𝒢(C,y_1,y_2,...,y_{i-1})</script><p>每个yi都依次这么产生，那么看起来就是整个系统根据输入句子X生成了目标句子Y。  ———（思考：其实这里的Encoder-Decoder是一个序列到序列的模型seq2seq，这个模型是对顺序有依赖的。）</p>
<p>Encoder-Decoder是个非常通用的计算框架，至于Encoder和Decoder具体使用什么模型都是由研究者自己定的，常见的比如CNN/RNN/BiRNN/GRU/LSTM/Deep LSTM等，这里的变化组合非常多。     ———（思考：人的学习过程包括输入、输出、外界评价。Encoder模型类似于人的输入学习过程，Decoder模型类似于人的输出学习过程，对输出的内容进行评价就类似于损失函数。英语老师给我上了几堂英语课，我在不断的输入Encoder；突然有一个随堂测试，我得做题输出Decoder；最后英语老师改卷子，给我一个分数，不对的地方我得反思调整我对输入数据的加工方式。）———-（再思考：关于英语翻译。课本上的单词和课文是原始数据输入，相当于X；我在大脑里加工这些数据，相当于Encoder模型，我的脑子里有很多加工后的数据，相当于C；现在要让我翻译一个英语句子，这个任务相当于Y，我不能翻课本，所以我只能借助我脑袋里加工的数据C去翻译这个句子，即我得动脑子，相当于Decoder。       学习的过程是什么都要学，要分类整理，要增加线索，并不知道未来的某天能用到什么，所以Encoder-Decoder是一个泛泛学习的框架）</p>
<h2 id="Attention-Model"><a href="#Attention-Model" class="headerlink" title="Attention Model"></a>Attention Model</h2><p>以上介绍的Encoder-Decoder模型是没有体现出“注意力模型”的，所以可以把它看作是注意力不集中的分心模型。为什么说它注意力不集中呢？请观察下目标句子Y中每个单词的生成过程如下：</p>
<script type="math/tex; mode=display">y_1=f(C)</script><script type="math/tex; mode=display">y_2=f(C,y_1)</script><script type="math/tex; mode=display">y_3=f(C,y_1,y_2)</script><p>其中<strong>f是decoder的非线性变换函数</strong>。从这里可以看出，在生成目标句子的单词时，不论生成哪个单词，是y1,y2也好，还是y3也好，他们使用的句子X的语义编码C都是一样的，没有任何区别。而语义编码C是由句子X的每个单词经过Encoder 编码产生的，这意味着不论是生成哪个单词，y1,y2还是y3，其实<strong>句子X中任意单词对生成某个目标单词yi来说影响力都是相同的，没有任何区别</strong>（其实如果Encoder是RNN的话，理论上越是后输入的单词影响越大，并非等权的，估计这也是为何Google提出Sequence to Sequence模型时发现把输入句子逆序输入做翻译效果会更好的小Trick的原因）。<strong>这就是为何说这个模型没有体现出注意力的缘由。</strong></p>
<p>引入AM模型，以翻译一个英语句子举例：输入X：Tom chase Jerry。   理想输出：汤姆追逐杰瑞。</p>
<p>应该在翻译“杰瑞”的时候，体现出英文单词对于翻译当前中文单词不同的影响程度，比如给出类似下面一个概率分布值：</p>
<p>（Tom,0.3）（Chase,0.2）（Jerry,0.5）</p>
<p>每个英文单词的概率代表了翻译当前单词“杰瑞”时，<strong>注意力分配模型分配给不同英文单词的注意力大小</strong>。这对于正确翻译目标语单词肯定是有帮助的，因为引入了新的信息。同理，目标句子中的每个单词都应该学会其对应的源语句子中单词的注意力分配概率信息。这意味着在生成每个单词Yi的时候，原先都是相同的中间语义表示C会替换成根据当前生成单词而不断变化的Ci。理解AM模型的关键就是这里，即由<strong>固定的中间语义表示C换成了根据当前输出单词来调整成加入注意力模型的变化的Ci。</strong></p>
<p><img src="/2020/12/02/attention-mechanism/encode_decoder_attention.png" width="80%"></p>
<p>即生成目标句子单词的过程成了下面的形式：</p>
<script type="math/tex; mode=display">y_1=f(C_1)</script><script type="math/tex; mode=display">y_2=f(C_2,y_1)</script><script type="math/tex; mode=display">y_3=f(C_3,y_1,y_2)</script><p>而每个$C_i$可能对应着不同的源语句子单词的注意力分配概率分布，比如对于上面的英汉翻译来说，其对应的信息可能如下：</p>
<script type="math/tex; mode=display">C_{汤姆}=g(0.6*f_2('Tom'),0.2*f_2('Chase'),0.2*f_2('Jerry')</script><script type="math/tex; mode=display">C_{追逐}=g(0.2*f_2('Tom'),0.7*f_2('Chase'),0.1*f_2('Jerry')</script><script type="math/tex; mode=display">C_{杰瑞}=g(0.3*f_2('Tom'),0.2*f_2('Chase'),0.5*f_2('Jerry')</script><p>其中，<strong>f2函数代表Encoder对输入英文单词的某种变换函数</strong>，比如如果Encoder是用的RNN模型的话，这个f2函数的结果往往是某个时刻输入xi后隐层节点的状态值；<strong>g代表Encoder根据单词的中间表示合成整个句子中间语义表示的变换函数</strong>，一般的做法中，<strong>g函数就是对构成元素加权求和</strong>，也就是常常在论文里看到的下列公式：</p>
<script type="math/tex; mode=display">c_i=\sum\limits_{j=1}^{T_x}a_{ij}h_j</script><p>假设Ci中那个i就是上面的“汤姆”，<strong>那么 $T_x$就是3，代表输入句子的长度</strong>，$h_1=f(“Tom”)，h_2=f(“Chase”),h_3=f(“Jerry”)$，对应的注意力模型权值分别是0.6,0.2,0.2，所以<strong>g函数就是个加权求和函数</strong>。如果形象表示的话，翻译中文单词“汤姆”的时候，数学公式对应的<strong>中间语义表示Ci的形成过程</strong>类似下图：</p>
<p><img src="/2020/12/02/attention-mechanism/generate_ci.png" width="80%"></p>
<p>这里还有一个问题：生成目标句子某个单词，比如“汤姆”的时候，<strong>你怎么知道AM模型所需要的输入句子单词注意力分配概率分布值呢？</strong>就是说“汤姆”对应的概率分布：</p>
<p><strong>划重点(注意力权重获取的过程)（Tom,0.3）（Chase,0.2）（Jerry,0.5）是如何得到的呢？</strong></p>
<p>为了便于说明，我们假设对图1的非AM模型的Encoder-Decoder框架进行细化，Encoder采用RNN模型，Decoder也采用RNN模型，这是比较常见的一种模型配置，则图1的图转换为下图：</p>
<p><img src="/2020/12/02/attention-mechanism/encoder_decoder_1.png" width="80%"></p>
<p>注意力分配概率分布值的通用计算过程：</p>
<p><img src="/2020/12/02/attention-mechanism/am_generate.png" width="80%"></p>
<p>对于采用RNN的Decoder来说，如果要生成$y_i$单词，在时刻i，我们是可以知道在生成Yi之前的隐层节点i时刻的输出值Hi的，而我们的目的是要计算生成Yi时的输入句子单词“Tom”、“Chase”、“Jerry”对Yi来说的注意力分配概率分布，那么可以用<strong>i时刻的隐层节点状态Hi</strong>去一一和输入句子中每个单词对应的<strong>RNN隐层节点状态$h_j$进行对比</strong>，即通<strong>过函数$F(h_j,H_i)$来获得目标单词Yi和每个输入单词对应的对齐可能性</strong>，这个<strong>F函数在不同论文里可能会采取不同的方法</strong>，然后<strong>函数F的输出经过Softmax进行归一化就得到了符合概率分布取值区间的注意力分配概率分布数值（这就得到了注意力权重）</strong>。图5显示的是当输出单词为“汤姆”时刻对应的输入句子单词的对齐概率。绝大多数AM模型都是采取上述的计算框架来计算注意力分配概率分布信息，区别只是在F的定义上可能有所不同。</p>
<p><strong>上述内容就是论文里面常常提到的Soft Attention Model</strong>（任何数据都会给一个权值，没有筛选条件）的基本思想，你能在文献里面看到的大多数AM模型基本就是这个模型，区别很可能只是把这个模型用来解决不同的应用问题。那么怎么理解<strong>AM模型的物理含义呢？</strong>一般文献里会把AM模型看作是单词对齐模型，这是非常有道理的。目标句子生成的每个单词对应输入句子单词的概率分布可以理解为输入句子单词和这个目标生成单词的对齐概率，这在机器翻译语境下是非常直观的：<strong>传统的统计机器翻译一般在做的过程中会专门有一个短语对齐的步骤，而注意力模型其实起的是相同的作用。</strong>在其他应用里面把AM模型理解成输入句子和目标句子单词之间的对齐概率也是很顺畅的想法。</p>
<p>当然，从概念上理解的话，把<strong>AM模型理解成影响力模型也是合理的</strong>，就是说生成目标单词的时候，输入句子每个单词对于生成这个单词有多大的影响程度。这种想法也是比较好理解AM模型物理意义的一种思维方式。</p>
]]></content>
      <categories>
        <category>basic_concept</category>
      </categories>
      <tags>
        <tag>attention mechanism</tag>
      </tags>
  </entry>
  <entry>
    <title>knowledge_graph</title>
    <url>/2020/12/04/knowledge-graph/</url>
    <content><![CDATA[<h1 id="知识图谱-Knowledge-Graph"><a href="#知识图谱-Knowledge-Graph" class="headerlink" title="知识图谱(Knowledge Graph)"></a>知识图谱(Knowledge Graph)</h1><h2 id="知识图谱的定义"><a href="#知识图谱的定义" class="headerlink" title="知识图谱的定义"></a>知识图谱的定义</h2><p><strong>知识图谱，本质上，是一种揭示实体之间关系的语义网络。</strong></p>
<p>如果你看过网络综艺《奇葩说》第五季第17期：你是否支持全人类一秒知识共享，你也许会被辩手陈铭的辩论印象深刻。他在节目中区分了信息和知识两个概念：</p>
<p><strong>信息是指外部的客观事实。</strong> 举例：这里有一瓶水，它现在是7°。</p>
<p><strong>知识是对外部客观规律的归纳和总结。</strong> 举例：水在零度的时候会结冰。</p>
<p>“客观规律的归纳和总结” 似乎有些难以实现。Quora 上有另一种经典的解读，区分 “信息” 和 “知识” 。</p>
<p><img src="/2020/12/04/knowledge-graph/img1.png" width="80%"></p>
<p>有了这样的参考，我们就很容易理解，在信息的基础上，建立实体之间的联系，就能行成 “知识”。当然，我认为叫事实（Fact）更为合适。换句话说，知识图谱是由一条条知识组成，<strong>每条知识表示为一个SPO三元组(Subject-Predicate-Object)。</strong></p>
<p><img src="/2020/12/04/knowledge-graph/img2.png" width="50%"></p>
<p>知识图谱实际上就是如此工作的。曾经知识图谱非常流行<strong>自顶向下(top-down)</strong>的构建方式。自顶向下指的是先为知识图谱定义好本体与数据模式，再将实体加入到知识库。该构建方式需要利用一些现有的结构化知识库作为其基础知识库，例如 Freebase 项目就是采用这种方式，它的绝大部分数据是从维基百科中得到的。</p>
<p>然而目前，大多数知识图谱都采用<strong>自底向上(bottom-up)</strong>的构建方式。自底向上指的是从一些开放链接数据（也就是 “信息”）中提取出实体，选择其中置信度较高的加入到知识库，再构建实体与实体之间的联系。</p>
<h2 id="知识图谱的体系架构"><a href="#知识图谱的体系架构" class="headerlink" title="知识图谱的体系架构"></a>知识图谱的体系架构</h2><p>知识图谱的架构主要包括自身的逻辑结构以及体系架构，</p>
<p>知识图谱在<strong>逻辑结构</strong>上可分为<strong>模式层</strong>与<strong>数据层</strong>两个层次，</p>
<p><strong>数据层</strong>主要是由一系列的事实组成，而知识将以事实为单位进行存储。如果用（实体1，关系，实体2）、（实体、属性，属性值）这样的三元组来表达事实，可选择图数据库作为存储介质，例如开源的 Neo4j、Twitter 的 FlockDB、JanusGraph 等。</p>
<p><strong>模式层</strong>构建在数据层之上，主要是通过本体库来规范数据层的一系列事实表达。本体是结构化知识库的概念模板，通过本体库而形成的知识库不仅层次结构较强，并且冗余程度较小。</p>
<p><strong>知识图谱的体系架构是指其构建模式的结构</strong>，如下图所示：</p>
<p><img src="/2020/12/04/knowledge-graph/img3.png" width="80%"></p>
<p>大规模知识库的构建与应用需要多种智能信息处理技术的支持。</p>
<p>通过<strong>知识抽取</strong>技术，可以从一些公开的半结构化、非结构化的数据中提取出实体、关系、属性等知识要素。</p>
<p>通过<strong>知识融合</strong>，可消除实体、关系、属性等指称项与事实对象之间的歧义，形成高质量的知识库。</p>
<p><strong>知识推理</strong>则是在已有的知识库基础上进一步挖掘隐含的知识，从而丰富、扩展知识库。分布式的知识表示形成的综合向量对知识库的构建、推理、融合以及应用均具有重要的意义。</p>
<h3 id="知识抽取"><a href="#知识抽取" class="headerlink" title="知识抽取"></a>知识抽取</h3><p>知识抽取主要是面向开放的链接数据，通过自动化的技术抽取出可用的知识单元，知识单元主要包括实体(概念的外延)、关系以及属性3个知识要素，并以此为基础，形成一系列高质量的事实表达，为上层模式层的构建奠定基础。知识抽取有三个主要工作：</p>
<p><strong>实体抽取</strong>：在技术上我们更多称为 NER（named entity recognition，命名实体识别），指的是从原始语料中自动识别出命名实体。由于实体是知识图谱中的最基本元素，其抽取的完整性、准确、召回率等将直接影响到知识库的质量。因此，实体抽取是知识抽取中最为基础与关键的一步；</p>
<p><strong>关系抽取</strong>：目标是解决实体间语义链接的问题，早期的关系抽取主要是通过人工构造语义规则以及模板的方法识别实体关系。随后，实体间的关系模型逐渐替代了人工预定义的语法与规则。</p>
<p><strong>属性抽取</strong>：属性抽取主要是针对实体而言的，通过属性可形成对实体的完整勾画。由于实体的属性可以看成是实体与属性值之间的一种名称性关系，因此可以将实体属性的抽取问题转换为关系抽取问题。</p>
<h3 id="知识表示"><a href="#知识表示" class="headerlink" title="知识表示"></a>知识表示</h3><p>近年来，以深度学习为代表的表示学习技术取得了重要的进展，可以将实体的语义信息表示为稠密低维实值向量，进而在低维空间中高效计算实体、关系及其之间的复杂语义关联，对知识库的构建、推理、融合以及应用均具有重要的意义。graph embedding 就是一种表示学习。</p>
<h3 id="知识融合"><a href="#知识融合" class="headerlink" title="知识融合"></a>知识融合</h3><p>由于知识图谱中的知识来源广泛，存在知识质量良莠不齐、来自不同数据源的知识重复、知识间的关联不够明确等问题，所以必须要进行知识的融合。知识融合是高层次的知识组织，使来自不同知识源的知识在同一框架规范下进行异构数据整合、消歧、加工、推理验证、更新等步骤，达到数据、信息、方法、经验以及人的思想的融合，形成高质量的知识库。</p>
<p>其中，知识更新是一个重要的部分。人类的认知能力、知识储备以及业务需求都会随时间而不断递增。因此，知识图谱的内容也需要与时俱进，不论是通用知识图谱，还是行业知识图谱，它们都需要不断地迭代更新，扩展现有的知识，增加新的知识。</p>
]]></content>
      <categories>
        <category>basic_concept</category>
      </categories>
      <tags>
        <tag>knowledge graph</tag>
      </tags>
  </entry>
  <entry>
    <title>siamese-triplet</title>
    <url>/2020/12/07/siamese-triplet/</url>
    <content><![CDATA[<h1 id="Siamese-network"><a href="#Siamese-network" class="headerlink" title="Siamese network"></a>Siamese network</h1><h4 id="可参考代码地址-github"><a href="#可参考代码地址-github" class="headerlink" title="可参考代码地址:github"></a>可参考代码地址:<a href="https://github.com/adambielski/siamese-triplet">github</a></h4><hr>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>在人脸识别中，存在所谓的<strong>one-shot</strong>问题。举例来说，就是对公司员工进行人脸识别，每个员工只给你一张照片（<strong>训练集样本少</strong>），并且员工会离职、入职（每次变动都要重新训练模型）。有这样的问题存在，就没办法直接训练模型来解决这样的分类问题了。</p>
<p>为了解决<strong>one-shot</strong>问题，我们会训练一个模型来输出给定<strong>两张图像的相似度</strong>，所以模型学习得到的是<strong>similarity</strong>函数。</p>
<p>哪些模型能<strong>通过学习得到similarity函数呢？Siamese网络就是这样的一种模型。</strong></p>
<h2 id="Siamese网络原理"><a href="#Siamese网络原理" class="headerlink" title="Siamese网络原理"></a>Siamese网络原理</h2><p>简单来说，Siamese network就是“连体的神经网络”，神经网络的“连体”是通过<strong>共享权值</strong>来实现的，如下图所示。</p>
<p><img src="/2020/12/07/siamese-triplet/img1.png" width="60%"></p>
<p>这里的Net1,Net2可以是lstm,cnn等等，都可以。 这里的关键是它们的权值是共享的。</p>
<h2 id="Triplet-network"><a href="#Triplet-network" class="headerlink" title="Triplet network"></a>Triplet network</h2><p>在<strong>Siamese network</strong>的基础上又出现了<strong>Triplet network</strong>。论文是<a href="https://arxiv.org/pdf/1412.6622.pdf">《Deep metric learning using Triplet network》</a></p>
<h3 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h3><ul>
<li><p>使具有相同标签的样本在embedding空间尽量接近。</p>
</li>
<li><p>使具有不同标签的样本在embedding空间尽量远离。</p>
</li>
</ul>
<p>如果只遵循以上两点，最后embedding空间中相同类别的样本间距离很小，不同类别的样本之间距离也会偏小。因此，需要加入margin。</p>
<font color="red"> 这里我们希望不同类的距离大，但也不是越大越好</font>

<p><img src="/2020/12/07/siamese-triplet/img2.png" width="80%"></p>
<h2 id="Siamese-network-loss-function一般用哪一种呢？"><a href="#Siamese-network-loss-function一般用哪一种呢？" class="headerlink" title="Siamese network loss function一般用哪一种呢？"></a>Siamese network loss function一般用哪一种呢？</h2><p>这里的损失函数主要是用于衡量两个样本之间的相似度。 </p>
<p>常用的就是欧式距离，交叉熵，余弦距离，很多论文也提出了比较复杂的损失函数。</p>
<p>下面给出一些参看论文：</p>
<p>这篇论文主要是提出了siamese network 和 其使用的损失函数</p>
<p><a href="http://www.cs.utoronto.ca/~hinton/csc2535_06/readings/chopra-05.pdf">Learning a Similarity Metric Discriminatively, with Application to Face Verification</a></p>
<p>这篇论文在siamese network的基础上提出了triplet network和相关的损失函数。</p>
<p><a href="https://arxiv.org/pdf/1503.03832.pdf">FaceNet: A Unified Embedding for Face Recognition and Clustering</a></p>
]]></content>
      <categories>
        <category>Networks</category>
      </categories>
      <tags>
        <tag>useful</tag>
      </tags>
  </entry>
  <entry>
    <title>ACM:LedDisplay</title>
    <url>/2020/12/11/ACM-LedDisplay/</url>
    <content><![CDATA[<h1 id="LED-Display"><a href="#LED-Display" class="headerlink" title="LED Display"></a>LED Display</h1><p>One day in the laboratory, Fred found some LED displays. This seven-segment LED can display digit 0 to 9 properly. But Fred soon find the LED have a serious problem, at the beginning, the seven bars were all on. But when one bar once was trun off, it can’t be turn on again! So ecah LED only can display digit in certain oder. For example, one LED can display 9,3,7 successively, but can’t display 2,4.<br>Now, Fred have a task to display a sequence of digit that is between 0 to 9. Because of the shortcoming of these LEDs, he need a number of them. But he also want to minimize the number, can you help him?<br>NOTE:If two digits in a sequece are the same,Fred want for the clearness, so he thought the latter digit can’t be displayed on the same LED. </p>
<p><strong>Input:</strong></p>
<p>The input consists of several test cases. The first line of each test case contains a positive integer N (&lt;=1000), then followed by a list of N digits. Each digit follows with a blank space.</p>
<p><strong>Output:</strong></p>
<p>For each test case, you must print a minimum number of LED that Fred need in one line. </p>
<p><strong>Sample Input:</strong></p>
<p>1</p>
<p>8</p>
<p>4</p>
<p>9 0 7 3</p>
<p>8</p>
<p>8 8 8 9 6 5 4 1</p>
<p><strong>Sample Output:</strong></p>
<p>1</p>
<p>2</p>
<p>3</p>
<h1 id="解题"><a href="#解题" class="headerlink" title="解题"></a>解题</h1><p><strong>(1) 思路：</strong></p>
<p>首先先把数字之间的转换变为一张图，比如8能转换为0-9，则8到0-7，9各有一条边。7能转换为1。则7到1有一条边。 我们先用一个邻接矩阵把图表示出来。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> convert[<span class="number">10</span>][<span class="number">10</span>]=&#123;</span><br><span class="line"><span class="comment">//0,1,2,3,4,5,6,7,8,9</span></span><br><span class="line">    &#123;<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>&#125;,</span><br><span class="line">    &#123;<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>&#125;,</span><br><span class="line">    &#123;<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>&#125;,</span><br><span class="line">    &#123;<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>&#125;,</span><br><span class="line">    &#123;<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>&#125;,</span><br><span class="line">    &#123;<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>&#125;,</span><br><span class="line">    &#123;<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>&#125;,</span><br><span class="line">    &#123;<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>&#125;,</span><br><span class="line">    &#123;<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>&#125;,</span><br><span class="line">    &#123;<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>&#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>然后这题要求的是把所有的数字覆盖，然后路径最小。 这就是一个最小路径覆盖问题。 </p>
<p>最小路径覆盖=n-最大匹配数。</p>
<p>这里考虑每条路径除了最后一个结点其它都有一个后续结点。 所有几个点没有尾结点，有有几条路径。 这里我们还需要让路径最小。所以需要让 pre-&gt;next的匹配数尽可能的多。</p>
<p><strong>(2) 代码：</strong><br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> MAXN 1010</span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> data[MAXN];</span><br><span class="line"><span class="type">int</span> pre[MAXN];</span><br><span class="line"><span class="type">int</span> vis[MAXN];</span><br><span class="line"><span class="type">int</span> map[MAXN][MAXN];</span><br><span class="line"><span class="type">int</span> convert[<span class="number">10</span>][<span class="number">10</span>]=&#123;</span><br><span class="line"><span class="comment">//0,1,2,3,4,5,6,7,8,9</span></span><br><span class="line">    &#123;<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>&#125;,</span><br><span class="line">    &#123;<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>&#125;,</span><br><span class="line">    &#123;<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>&#125;,</span><br><span class="line">    &#123;<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>&#125;,</span><br><span class="line">    &#123;<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>&#125;,</span><br><span class="line">    &#123;<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>&#125;,</span><br><span class="line">    &#123;<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>&#125;,</span><br><span class="line">    &#123;<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>&#125;,</span><br><span class="line">    &#123;<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>&#125;,</span><br><span class="line">    &#123;<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>&#125;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="type">int</span> n;</span><br><span class="line"></span><br><span class="line"><span class="comment">//匈牙利算法</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">find</span><span class="params">(<span class="type">int</span> cur)</span></span>&#123;<span class="comment">//cur为当前起点</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>; i&lt;n; i++)&#123; <span class="comment">//被匹配的终点</span></span><br><span class="line">        <span class="keyword">if</span>(map[cur][i] &amp;&amp; !vis[i])&#123;<span class="comment">//该终点未被匹配</span></span><br><span class="line">            vis[i]=<span class="literal">true</span>;<span class="comment">//这次匹配中，该终点已经被匹配了</span></span><br><span class="line">            <span class="keyword">if</span>(pre[i]==<span class="number">-1</span> || <span class="built_in">find</span>(pre[i]))&#123;<span class="comment">//该终点没有被匹配，或者被抢了的起点再去找一个终点</span></span><br><span class="line">                pre[i]=cur;</span><br><span class="line">                <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line"></span><br><span class="line">    cin&gt;&gt;n;</span><br><span class="line">    <span class="built_in">memset</span>(vis,<span class="number">0</span>,<span class="built_in">sizeof</span>(vis));</span><br><span class="line">    <span class="built_in">memset</span>(pre,<span class="number">-1</span>,<span class="built_in">sizeof</span>(pre));</span><br><span class="line">    <span class="built_in">memset</span>(map,<span class="number">0</span>,<span class="built_in">sizeof</span>(map));</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;n;i++)&#123;</span><br><span class="line">        cin&gt;&gt;data[i];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;n;i++)&#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> j=i+<span class="number">1</span>;j&lt;n;j++)&#123;</span><br><span class="line">            <span class="keyword">if</span>(convert[data[i]][data[j]]==<span class="number">1</span>)&#123;</span><br><span class="line">                map[i][j]=<span class="literal">true</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">int</span> ans=<span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;n;i++)&#123;</span><br><span class="line">        <span class="built_in">memset</span>(vis,<span class="number">0</span>,<span class="built_in">sizeof</span>(vis));</span><br><span class="line">        ans+=<span class="built_in">find</span>(i);</span><br><span class="line">    &#125;</span><br><span class="line">    cout&lt;&lt;n-ans&lt;&lt;endl;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>acm</category>
      </categories>
      <tags>
        <tag>最大二分匹配</tag>
        <tag>最小全覆盖</tag>
      </tags>
  </entry>
  <entry>
    <title>Explicit Interaction Model Towards Text Classification</title>
    <url>/2020/12/13/Explicit-interaction-model-towards-text-classification/</url>
    <content><![CDATA[<h1 id="Explicit-Interaction-Model-towards-Text-Classiﬁcation"><a href="#Explicit-Interaction-Model-towards-Text-Classiﬁcation" class="headerlink" title="Explicit Interaction Model towards Text Classiﬁcation"></a>Explicit Interaction Model towards Text Classiﬁcation</h1><h5 id="论文来源：AAAI-2019"><a href="#论文来源：AAAI-2019" class="headerlink" title="论文来源：AAAI 2019"></a>论文来源：AAAI 2019</h5><h5 id="论文链接：https-arxiv-org-abs-1811-09386"><a href="#论文链接：https-arxiv-org-abs-1811-09386" class="headerlink" title="论文链接：https://arxiv.org/abs/1811.09386"></a>论文链接：<a href="https://arxiv.org/abs/1811.09386">https://arxiv.org/abs/1811.09386</a></h5><h5 id="代码链接：https-github-com-NonvolatileMemory-AAAI-2019-EXAM"><a href="#代码链接：https-github-com-NonvolatileMemory-AAAI-2019-EXAM" class="headerlink" title="代码链接：https://github.com/NonvolatileMemory/AAAI_2019_EXAM"></a>代码链接：<a href="https://github.com/NonvolatileMemory/AAAI_2019_EXAM">https://github.com/NonvolatileMemory/AAAI_2019_EXAM</a></h5><hr>
<p>该文章的 idea 和之前的几篇类似，文本分类中没有充分利用 label 信息的问题，也都指出了对 label 做 encoding 的方法，作者提出了一个新的框架 EXplicit interAction Model (EXAM)，加入了 interaction mechanism。</p>
<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>如下图所示，传统分类的解决方案通过 dot-product 操作 <strong>将文本级表示与 label 表示匹配。</strong> 在数学上，FC 层的参数矩阵可以解释为一组类表示（每个列与一个类关联）。</p>
<p><img src="/2020/12/13/Explicit-interaction-model-towards-text-classification/img1.png" width="80%"></p>
<p>因此，文本属于某个类的概率在很大程度上取决于其整体匹配得分，而与单词级匹配信号无关，单词级匹配信号会为分类提供明确的信号（例如，missile 强烈暗示了军事的主题）。</p>
<p>针对上述情况，作者引入了交互机制，该机制能够将单词级匹配信号纳入文本分类中。交互机制背后的关键思想是显式计算单词和类之间的匹配分数。从单词级别的表示中，它会计算一个交互矩阵，其中每个条目是单词和类（dot-product）之间的匹配得分。</p>
<h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p><img src="/2020/12/13/Explicit-interaction-model-towards-text-classification/img2.png" width="80%"></p>
<h2 id="字级编码器（Encoder）"><a href="#字级编码器（Encoder）" class="headerlink" title="字级编码器（Encoder）"></a>字级编码器（Encoder）</h2><p>用于将输入文本 $d_i$ 投影到字级表示 $H$。</p>
<ul>
<li><p>Gated Recurrent Unit</p>
</li>
<li><p>Region Embedding 来学习和利用 Ngrams 的任务特定的分布式表示。</p>
</li>
</ul>
<h2 id="交互层（Interaction）"><a href="#交互层（Interaction）" class="headerlink" title="交互层（Interaction）"></a>交互层（Interaction）</h2><p>用于计算单词和类之间的交互信号的交互层。</p>
<p><img src="/2020/12/13/Explicit-interaction-model-towards-text-classification/img3.png" width="80%"></p>
<h2 id="聚合层（Aggregation）"><a href="#聚合层（Aggregation）" class="headerlink" title="聚合层（Aggregation）"></a>聚合层（Aggregation）</h2><p>用于聚合每个类的交互信号并进行最终预测。</p>
<p>该层的设计目的是将每个类的交互特性聚合到一个 logits 中，表示类与输入文本之间的匹配分数。聚合层可以通过不同的方式实现，如 CNN 和 LSTM。但是，为了保持考试的简单性和效率，这里作者只使用了一个具有两个 FC 层的 MLP，其中 ReLU 被用作第一层的激活函数。在形式上，MLP对类的交互特性进行聚合，并计算其关联 logits 如下:</p>
<p><img src="/2020/12/13/Explicit-interaction-model-towards-text-classification/img4.png" width="80%"></p>
<h2 id="Loss（Cross-Entropy）"><a href="#Loss（Cross-Entropy）" class="headerlink" title="Loss（Cross Entropy）"></a>Loss（Cross Entropy）</h2><p><img src="/2020/12/13/Explicit-interaction-model-towards-text-classification/img5.png" width="80%"></p>
]]></content>
      <categories>
        <category>paper</category>
      </categories>
      <tags>
        <tag>word label embedding</tag>
        <tag>text classification</tag>
      </tags>
  </entry>
  <entry>
    <title>最大人工岛</title>
    <url>/2020/12/13/%E6%9C%80%E5%A4%A7%E4%BA%BA%E5%B7%A5%E5%B2%9B/</url>
    <content><![CDATA[<h1 id="最大人工岛"><a href="#最大人工岛" class="headerlink" title="最大人工岛"></a>最大人工岛</h1><p>在二维地图上， 0代表海洋， 1代表陆地，我们最多只能将一格 0 海洋变成 1变成陆地。</p>
<p>进行填海之后，地图上最大的岛屿面积是多少？（上、下、左、右四个方向相连的 1 可形成岛屿）</p>
<h2 id="Input："><a href="#Input：" class="headerlink" title="Input："></a>Input：</h2><p>每个样例</p>
<p>第一行输入两个数字n m 表示地图的行数和列数。<br>后面n行每行输入m个数字，表示地图。</p>
<h2 id="Output"><a href="#Output" class="headerlink" title="Output:"></a>Output:</h2><p>输出一个数字，表示将一格海洋变成陆地后最大的岛屿面积。</p>
<h2 id="Sample-Input"><a href="#Sample-Input" class="headerlink" title="Sample Input:"></a>Sample Input:</h2><p>2 2</p>
<p>1 0</p>
<p>0 1</p>
<p>2 2</p>
<p>1 1</p>
<p>1 0</p>
<p>2 2</p>
<p>1 1</p>
<p>1 1</p>
<h2 id="Sample-Output"><a href="#Sample-Output" class="headerlink" title="Sample Output:"></a>Sample Output:</h2><p>3</p>
<p>4</p>
<p>4 </p>
<p><strong>(1)    思路：</strong></p>
<p>遍历每一块海洋，让其变为陆地，加上附近大陆的面积。  这里的难点就是如何快速通过每一个陆地，得到这块大陆的面积。这里就可以用并查集来做。</p>
<p><strong>(2)    代码：</strong><br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;set&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> MAXN 100</span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="type">int</span> n,m,ans;</span><br><span class="line"><span class="type">int</span> map[MAXN][MAXN];</span><br><span class="line"><span class="type">int</span> tree[MAXN][<span class="number">2</span>];</span><br><span class="line"><span class="type">int</span> direction[<span class="number">4</span>][<span class="number">2</span>] = &#123;&#123;<span class="number">1</span>,<span class="number">0</span>&#125;,&#123;<span class="number">0</span>,<span class="number">1</span>&#125;,&#123;<span class="number">-1</span>,<span class="number">0</span>&#125;,&#123;<span class="number">0</span>,<span class="number">-1</span>&#125;&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">findroot</span><span class="params">(<span class="type">int</span> x)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(tree[x][<span class="number">0</span>]==<span class="number">-1</span>)&#123;</span><br><span class="line">        <span class="keyword">return</span> x;</span><br><span class="line">    &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">        <span class="type">int</span> root = <span class="built_in">findroot</span>(tree[x][<span class="number">0</span>]);</span><br><span class="line">        tree[x][<span class="number">0</span>] = root;</span><br><span class="line">        <span class="keyword">return</span> root;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">uni</span><span class="params">(<span class="type">int</span> x,<span class="type">int</span> y)</span></span>&#123;</span><br><span class="line">    <span class="type">int</span> root1 = <span class="built_in">findroot</span>(x);</span><br><span class="line">    <span class="type">int</span> root2 = <span class="built_in">findroot</span>(y);</span><br><span class="line">    <span class="keyword">if</span>(root1 == root2)&#123;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    tree[root1][<span class="number">0</span>]=root2;</span><br><span class="line">    tree[root2][<span class="number">1</span>]+=tree[root1][<span class="number">1</span>];</span><br><span class="line">    ans = <span class="built_in">max</span>(ans,tree[root2][<span class="number">1</span>]);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">while</span>(cin&gt;&gt;n&gt;&gt;m&amp;&amp;n!=<span class="number">0</span>&amp;&amp;m!=<span class="number">0</span>)&#123;</span><br><span class="line">        ans=<span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;n*m;i++)&#123;</span><br><span class="line">            tree[i][<span class="number">0</span>]=<span class="number">-1</span>;</span><br><span class="line">            tree[i][<span class="number">1</span>]=<span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;n;i++)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="type">int</span> j=<span class="number">0</span>;j&lt;m;j++)&#123;</span><br><span class="line">                cin&gt;&gt;map[i][j];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;n;i++)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="type">int</span> j=<span class="number">0</span>;j&lt;n;j++)&#123;</span><br><span class="line">                <span class="keyword">if</span>(map[i][j]==<span class="number">1</span>)&#123;</span><br><span class="line">                    ans = <span class="built_in">max</span>(ans,<span class="number">1</span>);</span><br><span class="line">                    <span class="keyword">for</span>(<span class="type">int</span> k=<span class="number">0</span>;k&lt;<span class="number">4</span>;k++)&#123;</span><br><span class="line">                        <span class="type">int</span> I = i+direction[k][<span class="number">0</span>];</span><br><span class="line">                        <span class="type">int</span> J = j+direction[k][<span class="number">1</span>];</span><br><span class="line">                        <span class="keyword">if</span>(I&gt;=<span class="number">0</span>&amp;&amp;I&lt;n&amp;&amp;J&gt;=<span class="number">0</span>&amp;&amp;J&lt;m&amp;&amp;map[I][J]==<span class="number">1</span>)&#123;</span><br><span class="line">                            <span class="built_in">uni</span>(I*m+J,i*m+j);</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;n;i++)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="type">int</span> j=<span class="number">0</span>;j&lt;m;j++)&#123;</span><br><span class="line">                <span class="keyword">if</span>(map[i][j]==<span class="number">0</span>)&#123;</span><br><span class="line">                    <span class="type">int</span> size=<span class="number">1</span>;</span><br><span class="line">                    set&lt;<span class="type">int</span>&gt; neb;</span><br><span class="line">                    <span class="keyword">for</span>(<span class="type">int</span> k=<span class="number">0</span>;k&lt;<span class="number">4</span>;k++)&#123;</span><br><span class="line">                        <span class="type">int</span> I=i+direction[k][<span class="number">0</span>];</span><br><span class="line">                        <span class="type">int</span> J=j+direction[k][<span class="number">1</span>];</span><br><span class="line">                        <span class="keyword">if</span>(I&gt;=<span class="number">0</span>&amp;&amp;I&lt;n&amp;&amp;J&gt;=<span class="number">0</span>&amp;&amp;j&lt;m&amp;&amp;map[I][J]==<span class="number">1</span>)&#123;</span><br><span class="line">                            <span class="type">int</span> fa = <span class="built_in">findroot</span>(I*m+J);</span><br><span class="line">                            <span class="keyword">if</span>(neb.<span class="built_in">count</span>(fa)==<span class="number">0</span>)&#123;</span><br><span class="line">                                size+=tree[fa][<span class="number">1</span>];</span><br><span class="line">                                neb.<span class="built_in">insert</span>(fa);</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                    ans = <span class="built_in">max</span>(ans,size);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        cout&lt;&lt;ans&lt;&lt;endl;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>acm</category>
      </categories>
      <tags>
        <tag>并查集</tag>
        <tag>深度优先</tag>
      </tags>
  </entry>
  <entry>
    <title>A new method of region embedding for text classification</title>
    <url>/2020/12/14/A-new-method-of-region-embedding-for-text-classification/</url>
    <content><![CDATA[<h1 id="A-new-method-of-region-embedding-for-text-classification"><a href="#A-new-method-of-region-embedding-for-text-classification" class="headerlink" title="A new method of region embedding for text classification"></a>A new method of region embedding for text classification</h1><h5 id="论文来源：ICLR-2018"><a href="#论文来源：ICLR-2018" class="headerlink" title="论文来源：ICLR 2018"></a>论文来源：ICLR 2018</h5><h5 id="论文链接：https-openreview-net-references-pdf-id-ByD5LekDM"><a href="#论文链接：https-openreview-net-references-pdf-id-ByD5LekDM" class="headerlink" title="论文链接：https://openreview.net/references/pdf?id=ByD5LekDM"></a>论文链接：<a href="https://openreview.net/references/pdf?id=ByD5LekDM">https://openreview.net/references/pdf?id=ByD5LekDM</a></h5><h5 id="代码链接：https-github-com-text-representation-local-context-unit"><a href="#代码链接：https-github-com-text-representation-local-context-unit" class="headerlink" title="代码链接：https://github.com/text-representation/local-context-unit"></a>代码链接：<a href="https://github.com/text-representation/local-context-unit">https://github.com/text-representation/local-context-unit</a></h5><p>传统的bag-of-words方法虽然能够有效的训练出词向量，不过却损失了重要的单词顺序信息。</p>
<p>之后提出的n-grams，虽然有用，但是也有一定的局限性：</p>
<ol>
<li>n-grams的数量会随着n的增加爆炸式增长。</li>
<li>n-grams模型中的参数非常多，这会导致数据稀疏的问题。</li>
</ol>
<p>于是作者就提出了一种新的n-gram embedding方法叫做 region embedding。</p>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>$region(i,c)$ 表示以单词$w_i$为中心，向左右延伸$c$个单词，长度为$2*c+1$的区域。</p>
<p>在作者的模型中，一个单词的嵌入由两部分组成。$e_{w_i} \in ℝ^{h×1}$ 和 $K_{w_i}\in ℝ^{h×(2×c+1)}$。</p>
<p>$e_{w_i}$表示单词$w_i$的词嵌入向量。 $K_{w_i}$表示单词$w_i$的上下文单元。</p>
<p>模型中</p>
<p>所有的$e_{w_i}$用矩阵$E \in ℝ^{h×v}$, $v$表示字典大小。$h$表示词嵌入的大小。</p>
<p>所有的$K_{w_i}$用$U\inℝ^{h×(2×c+1)×v}$表示。</p>
<p>之后使用$E和U$计算出$P$。</p>
<p>$p_{w_{i+t}}^i = K_{w_i,t}⊙e_{w_{i+t}}$</p>
<p>这里的$⊙$表示按元素乘。</p>
<p>这里的计算方法有两种，具体如下图所示：</p>
<p><img src="/2020/12/14/A-new-method-of-region-embedding-for-text-classification/img1.png" width="80%"></p>
<p>然后对P进行最大池化得到区域嵌入。</p>
<p>Word-Context Region Embedding</p>
<script type="math/tex; mode=display">r_{(i,c)}=max([p_{w_{i-c}}^i p_{w_{i-c+1}}^i...p_{w_{i+c-1}}^ip_{w_{i+c}}^i])</script><p>Context-Word Region Embedding</p>
<script type="math/tex; mode=display">r_{(i,c)}=max([p_{w_i}^{i-c} p_{w_i}^{i-c+1}...p_{w_i}^{i+c-1} p_{w_i}^{i+c}])</script><p>之后在使用region embedding 进行分类预测。</p>
<script type="math/tex; mode=display">f(x;E,U,W,b)=g(W\sigma(\sum\limits_{i=0}^nr_{(i,c)})+b)</script>]]></content>
      <categories>
        <category>paper</category>
      </categories>
      <tags>
        <tag>text classification</tag>
        <tag>region embedding</tag>
      </tags>
  </entry>
  <entry>
    <title>Forward_Backward_Update耗时测试</title>
    <url>/2020/12/19/Forward-Backward-Update%E8%80%97%E6%97%B6%E6%B5%8B%E8%AF%95/</url>
    <content><![CDATA[<h1 id="Forward-Backward-Update耗时测试"><a href="#Forward-Backward-Update耗时测试" class="headerlink" title="Forward_Backward_Update耗时测试"></a>Forward_Backward_Update耗时测试</h1><p>本文主要是编写代码并测试在模型训练过程中，正向传播，反向传播，参数更新三个阶段的耗时比例。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">      <span class="built_in">super</span>(Net,self).__init__()</span><br><span class="line">      self.conv1 = nn.Conv2d(<span class="number">1</span>,<span class="number">6</span>,<span class="number">5</span>)</span><br><span class="line">      self.conv2 = nn.Conv2d(<span class="number">6</span>,<span class="number">16</span>,<span class="number">5</span>)</span><br><span class="line">      self.fc1 = nn.Linear(<span class="number">16</span>*<span class="number">5</span>*<span class="number">5</span>,<span class="number">120</span>)</span><br><span class="line">      self.fc2 = nn.Linear(<span class="number">120</span>,<span class="number">84</span>)</span><br><span class="line">      self.fc3 = nn.Linear(<span class="number">84</span>,<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">      x = F.max_pool2d(F.relu(self.conv1(x)),<span class="number">2</span>)</span><br><span class="line">      x = F.max_pool2d(F.relu(self.conv2(x)),<span class="number">2</span>)</span><br><span class="line">      x = x.view(-<span class="number">1</span>,self.num_flat_features(x))</span><br><span class="line">      x = F.relu(self.fc1(x))</span><br><span class="line">      x = F.relu(self.fc2(x))</span><br><span class="line">      x = self.fc3(x)</span><br><span class="line">      <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">num_flat_features</span>(<span class="params">self,x</span>):</span><br><span class="line">      size = x.size()[<span class="number">1</span>:]</span><br><span class="line">      num_features = <span class="number">1</span></span><br><span class="line">      <span class="keyword">for</span> s <span class="keyword">in</span> size:</span><br><span class="line">        num_features *= s</span><br><span class="line">      <span class="keyword">return</span> num_features</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">  model = Net()</span><br><span class="line">  criterion = nn.CrossEntropyLoss()</span><br><span class="line">  optimizer = torch.optim.SGD(model.parameters(),lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line">  inputs = torch.randn(<span class="number">256</span>,<span class="number">1</span>,<span class="number">32</span>,<span class="number">32</span>)</span><br><span class="line">  targets = torch.empty(<span class="number">256</span>,dtype=torch.long).random_(<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    start_time = time.time()</span><br><span class="line">    outputs = model(inputs)</span><br><span class="line">    loss = criterion(outputs,targets)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Forward: &#123;&#125;s&#x27;</span>.<span class="built_in">format</span>(time.time()-start_time))</span><br><span class="line"></span><br><span class="line">    model.zero_grad()</span><br><span class="line">    start_time = time.time()</span><br><span class="line">    loss.backward()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Backward: &#123;&#125;s&#x27;</span>.<span class="built_in">format</span>(time.time()-start_time))</span><br><span class="line"></span><br><span class="line">    start_time = time.time()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Update: &#123;&#125;s&#x27;</span>.<span class="built_in">format</span>(time.time()-start_time))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;========================&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Forward: <span class="number">0.035942792892456055</span>s</span><br><span class="line">Backward: <span class="number">0.035076141357421875</span>s</span><br><span class="line">Update: <span class="number">0.0007233619689941406</span>s</span><br><span class="line">========================</span><br><span class="line">Forward: <span class="number">0.025434494018554688</span>s</span><br><span class="line">Backward: <span class="number">0.03362894058227539</span>s</span><br><span class="line">Update: <span class="number">0.00023508071899414062</span>s</span><br><span class="line">========================</span><br><span class="line">Forward: <span class="number">0.032956838607788086</span>s</span><br><span class="line">Backward: <span class="number">0.0335536003112793</span>s</span><br><span class="line">Update: <span class="number">0.0002675056457519531</span>s</span><br><span class="line">========================</span><br><span class="line">Forward: <span class="number">0.02550196647644043</span>s</span><br><span class="line">Backward: <span class="number">0.04209303855895996</span>s</span><br><span class="line">Update: <span class="number">0.00024390220642089844</span>s</span><br><span class="line">========================</span><br><span class="line">Forward: <span class="number">0.02639007568359375</span>s</span><br><span class="line">Backward: <span class="number">0.033740997314453125</span>s</span><br><span class="line">Update: <span class="number">0.001081705093383789</span>s</span><br><span class="line">========================</span><br><span class="line">Forward: <span class="number">0.026480913162231445</span>s</span><br><span class="line">Backward: <span class="number">0.033477067947387695</span>s</span><br><span class="line">Update: <span class="number">0.0007789134979248047</span>s</span><br><span class="line">========================</span><br><span class="line">Forward: <span class="number">0.02574777603149414</span>s</span><br><span class="line">Backward: <span class="number">0.033417463302612305</span>s</span><br><span class="line">Update: <span class="number">0.000713348388671875</span>s</span><br><span class="line">========================</span><br><span class="line">Forward: <span class="number">0.027414798736572266</span>s</span><br><span class="line">Backward: <span class="number">0.03293299674987793</span>s</span><br><span class="line">Update: <span class="number">0.00022792816162109375</span>s</span><br><span class="line">========================</span><br><span class="line">Forward: <span class="number">0.02549004554748535</span>s</span><br><span class="line">Backward: <span class="number">0.03346991539001465</span>s</span><br><span class="line">Update: <span class="number">0.00023126602172851562</span>s</span><br><span class="line">========================</span><br><span class="line">Forward: <span class="number">0.025561094284057617</span>s</span><br><span class="line">Backward: <span class="number">0.033406972885131836</span>s</span><br><span class="line">Update: <span class="number">0.00024080276489257812</span>s</span><br><span class="line">========================</span><br></pre></td></tr></table></figure>
<p>根据上面实验我们可以得出  $Backward &gt; Forward &gt;&gt; Update$</p>
]]></content>
      <categories>
        <category>experiment</category>
      </categories>
      <tags>
        <tag>forward</tag>
        <tag>backward</tag>
        <tag>update</tag>
      </tags>
  </entry>
  <entry>
    <title>人工智能</title>
    <url>/2020/12/25/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/</url>
    <content><![CDATA[<h1 id="人工智能"><a href="#人工智能" class="headerlink" title="人工智能"></a>人工智能</h1><h2 id="人工智能学科概述"><a href="#人工智能学科概述" class="headerlink" title="人工智能学科概述"></a>人工智能学科概述</h2><h3 id="人工智能学科发展的历史，研究内容及其研究特点。"><a href="#人工智能学科发展的历史，研究内容及其研究特点。" class="headerlink" title="人工智能学科发展的历史，研究内容及其研究特点。"></a>人工智能学科发展的历史，研究内容及其研究特点。</h3><h2 id="知识表示方式"><a href="#知识表示方式" class="headerlink" title="知识表示方式"></a>知识表示方式</h2><h3 id="知识表示的概念"><a href="#知识表示的概念" class="headerlink" title="知识表示的概念"></a>知识表示的概念</h3><p>知识表示实际上就是对知识的一种描述，即用一些约定的符号把知识编码成一组计算机可以接受的数据结构。 </p>
<p>知识必须有适当的表示形式才便于在计算机中存储、检索、修改和使用。</p>
<p>总之，知识表示就是研究如何用最合适的形式来组织知识，使对所要解决的问题最为有利。</p>
<h3 id="一阶谓词逻辑"><a href="#一阶谓词逻辑" class="headerlink" title="一阶谓词逻辑"></a><font color="red">一阶谓词逻辑</font></h3><p>命题变量：任何命题均可用变量表示，通常用P、Q、R等表示。</p>
<p>命题连接词(Connectives)：通常有以下5个：</p>
<p> ¬ ：否定(Negation)，非(Not)：¬P</p>
<p>∧：合取(Conjunction)，与(and)：P∧Q</p>
<p>∨：析取(Disjunction)，或(or)： P∨Q</p>
<p>$\rightarrow$条件(Conditional)，蕴含(Impliction)：P$\rightarrow$Q</p>
<p>$\leftrightarrow$双条件(Biconditional)，等价(Equivalence)：P$\leftrightarrow$Q</p>
<p>量词(Quantifier)</p>
<p>在谓词逻辑中，使用以下两种量词：</p>
<p>$\forall$全称(Universal)量词，表示“全体的”。</p>
<p>$\exist$存在(Existential)量词，表示“存在”。</p>
<p>例： </p>
<p>$(\forall x)P(x)$，表示对于所有的x，谓词P(x)均为T。</p>
<p>$(\exist x)P(x)$，表示存在某些x，使谓词P(x)为T。</p>
<p>量词的作用称为量化，一元谓词量化后转化为命题。此外，将谓词中的变量全部换成确定的对象，也可使谓词变为命题。</p>
<p>例：</p>
<ol>
<li>所有偶数可被2整除</li>
</ol>
<p>E(x)：x是偶数，Q(x)：x可被2整除</p>
<p>$(\forall x)[E(x) \rightarrow Q(x)]$</p>
<ol>
<li>有些偶数可被3整除</li>
</ol>
<p>T(x)：x可被3整除</p>
<p>$(\exist x)[E(x)∧T(x)]$</p>
<ol>
<li>存在一个偶数        $(\exist x)E(x)$$</li>
</ol>
<p>每个整数是偶数或奇数　$(\forall x)[E(x)∨O(x)]$</p>
<p>不是所有整数是奇数    $¬\forall xO(x)或\exist x¬O(x)$</p>
<p>不是所有素数是奇数    $¬\forall x[P(x)\rightarrow O(x)]或\exist x[P(x)∧¬O(x)]$</p>
<p>如果一个整数不是奇数，那么是偶数  $\forall x[¬O(x)\rightarrow E(x)]$</p>
<ol>
<li>自然数的定义：</li>
</ol>
<p>(1) 对每一个数，存在一个且只有一个后继数。</p>
<p>(2) 不存在以0为后继数的数</p>
<p>(3) 除0以外，每一个数都有一个且只有一个前驱数。</p>
<p>设：</p>
<p>S(x)：x的后继数</p>
<p>P(x)：x的前驱数</p>
<p>E(x, y)：x等于y</p>
<p>则：</p>
<p>(1) $\forall x \exist y{E(y, S(x))∧ \forall z[E(z, S(x))\rightarrow E(y, z)]}$</p>
<p>(2) $¬\exist x[E(0, S(x))]$</p>
<p>(3) $\forall x{¬E(x, 0)\rightarrow y[E(y, P(x))∧\forall z(E(z, P(x))\rightarrow E(y, z))]}$</p>
<p>定义表示状态的谓词：</p>
<p>TABLE(x)：x是桌子。 </p>
<p>EMPTY(y)：y手中是空的。 </p>
<p>AT(y, z)：y在z的附近。 </p>
<p>HOLDS(y, w)：y手中拿着w。 </p>
<p>ON(w, x)：w在x桌子上。 </p>
<p>其中各变量的个体域是： </p>
<p>x：{a, b} </p>
<p>y：{robot} </p>
<p>z：{a, b, c} </p>
<p>w：{box} </p>
<p><strong>谓词逻辑表示法的特点</strong></p>
<p>自然：是一种接近与自然语言的形式语言系统，接近与人们对问题的直观理解，易于被人们接受。</p>
<p>明确：对如何由简单陈述句构造复杂陈述句的方法有明确的规定，表示的知识明确，易于理解。 </p>
<p>精确：是一种二值逻辑，用于表示精确知识，并可保证推理结论的精确性。 </p>
<p>灵活：逻辑表示法把知识和处理知识的程序有效地分开。</p>
<p>模块化：在逻辑表示法中，各条知识都是相对独立的，它们之间不直接发生关系。 </p>
<p><strong>谓词逻辑表示法的缺点</strong></p>
<ol>
<li>知识表示能力差：逻辑表示法只能表示确定性知识，而不能表示非确定性知识，如不精确、模糊性知识。  </li>
<li>知识库管理困难：缺乏知识的组织原则。</li>
<li>存在组合爆炸：难以表示启发性知识。 </li>
<li>系统效率低：根据形式逻辑进行推理，把推理演算与知识含义截然分开，抛弃了表达内容中所含有的语义信息，使推理过程冗长。 </li>
</ol>
<h3 id="产生式系统"><a href="#产生式系统" class="headerlink" title="产生式系统"></a><font color="red">产生式系统</font></h3><p>产生式系统由三个基本要素组成： </p>
<ol>
<li><p>综合数据库：描述问题状态和有关事实的数据结构。它含有所求解问题的信息，其中有些部分可以是不变的，有些部分则可能只与当前问题的解有关。它对应叙述性知识，相当于人脑的短期记忆功能。</p>
</li>
<li><p>产生式规则库：用于求解该领域问题的所有规则的集合。 </p>
</li>
</ol>
<p>产生式规则的一般形式为：  IF   P  THEN  Q</p>
<p>它对应于过程性知识，相当于人脑的长期记忆功能。</p>
<ol>
<li>控制策略：如何选择一条可应用的规则对综合数据库进行操作。即决定了问题求解过程的推理线路。它对应于控制性知识。 </li>
</ol>
<p><strong>产生式系统表示法的特点</strong></p>
<p>优点：</p>
<p>自然性 模块性 有效性</p>
<p>缺点：</p>
<p>效率较低 不能表示结构性知识</p>
<h3 id="语义网络"><a href="#语义网络" class="headerlink" title="语义网络"></a><font color="red">语义网络</font></h3><p>语义网络最初是在自然语言理解系统中，为表达单词的意义而设计的一种表示方式。它是一种用实体及其语义关系来表达知识的有向图，实际上是对知识的一种图表示法，它由节点和弧组成，节点代表实体，用来表示事物、概念、情况、属性、状态、事件、动作等，而弧表示节点之间的语义关系。</p>
<h3 id="框架"><a href="#框架" class="headerlink" title="框架"></a><font color="red">框架</font></h3><h3 id="脚本"><a href="#脚本" class="headerlink" title="脚本"></a>脚本</h3><h3 id="概念的从属表示方法"><a href="#概念的从属表示方法" class="headerlink" title="概念的从属表示方法"></a>概念的从属表示方法</h3><h2 id="确定性推理"><a href="#确定性推理" class="headerlink" title="确定性推理"></a>确定性推理</h2><h3 id="推理的基本概念"><a href="#推理的基本概念" class="headerlink" title="推理的基本概念"></a>推理的基本概念</h3><h3 id="推理的逻辑基础"><a href="#推理的逻辑基础" class="headerlink" title="推理的逻辑基础"></a><font color="red">推理的逻辑基础</font></h3><h3 id="自然演绎推理"><a href="#自然演绎推理" class="headerlink" title="自然演绎推理"></a>自然演绎推理</h3><h3 id="归结演绎推理"><a href="#归结演绎推理" class="headerlink" title="归结演绎推理"></a><font color="red">归结演绎推理</font></h3><h3 id="归结策略"><a href="#归结策略" class="headerlink" title="归结策略"></a><font color="red">归结策略</font></h3><h2 id="不确定性推理"><a href="#不确定性推理" class="headerlink" title="不确定性推理"></a>不确定性推理</h2><h3 id="不确定性推理的概念"><a href="#不确定性推理的概念" class="headerlink" title="不确定性推理的概念"></a>不确定性推理的概念</h3><h3 id="确定性理论"><a href="#确定性理论" class="headerlink" title="确定性理论"></a><font color="red">确定性理论</font></h3><h3 id="主观Bayes方法"><a href="#主观Bayes方法" class="headerlink" title="主观Bayes方法"></a><font color="red">主观Bayes方法</font></h3><h3 id="D-S证据理论"><a href="#D-S证据理论" class="headerlink" title="D-S证据理论"></a>D-S证据理论</h3><h3 id="可能性理论与模糊推理"><a href="#可能性理论与模糊推理" class="headerlink" title="可能性理论与模糊推理"></a><font color="red">可能性理论与模糊推理</font></h3><h2 id="搜索策略"><a href="#搜索策略" class="headerlink" title="搜索策略"></a>搜索策略</h2><h3 id="搜索的基本概念"><a href="#搜索的基本概念" class="headerlink" title="搜索的基本概念"></a>搜索的基本概念</h3><h3 id="状态空间的盲目搜索"><a href="#状态空间的盲目搜索" class="headerlink" title="状态空间的盲目搜索"></a>状态空间的盲目搜索</h3><h3 id="状态空间的启发式搜索"><a href="#状态空间的启发式搜索" class="headerlink" title="状态空间的启发式搜索"></a><font color="red">状态空间的启发式搜索</font></h3><h3 id="与-或树-图-的盲目搜索和启发式搜索"><a href="#与-或树-图-的盲目搜索和启发式搜索" class="headerlink" title="与/或树(图)的盲目搜索和启发式搜索"></a><font color="red">与/或树(图)的盲目搜索和启发式搜索</font></h3><h3 id="博弈搜索"><a href="#博弈搜索" class="headerlink" title="博弈搜索"></a><font color="red">博弈搜索</font></h3><h2 id="专家系统"><a href="#专家系统" class="headerlink" title="专家系统"></a>专家系统</h2><h3 id="专家系统的基本概念组成、工作原理及开发过程，专家系统示例。"><a href="#专家系统的基本概念组成、工作原理及开发过程，专家系统示例。" class="headerlink" title="专家系统的基本概念组成、工作原理及开发过程，专家系统示例。"></a>专家系统的基本概念组成、工作原理及开发过程，专家系统示例。</h3>]]></content>
      <categories>
        <category>课程</category>
      </categories>
  </entry>
  <entry>
    <title>大理石分割问题</title>
    <url>/2020/12/29/%E5%A4%A7%E7%90%86%E7%9F%B3%E5%88%86%E5%89%B2%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<h1 id="大理石分割问题"><a href="#大理石分割问题" class="headerlink" title="大理石分割问题"></a>大理石分割问题</h1><p>有若干块大理石，其大小及美观程度不一，为了比较客观的分割这些大理石，我们需要先给这些大理石一个评分，评分分为6个等级，分别用1~6的数字来表示。现希望将这些大理石分成两部分，使每部分的评分之和相同。</p>
<h2 id="输入："><a href="#输入：" class="headerlink" title="输入："></a>输入：</h2><p>输入一行，包括6个数，分别是每个等级的大理石的数量。每种等级的大理石数量不超过20000.</p>
<h2 id="输出："><a href="#输出：" class="headerlink" title="输出："></a>输出：</h2><p>如果这些大理石能否分割成评价等级之和相同的两部分，则输出true，否则输出false.</p>
<h2 id="样例输入："><a href="#样例输入：" class="headerlink" title="样例输入："></a>样例输入：</h2><p>1 0 1 2 0 0</p>
<h2 id="样例输出："><a href="#样例输出：" class="headerlink" title="样例输出："></a>样例输出：</h2><p>false  </p>
<h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>多重背包使用二进制分包转换为01背包。 看是否能将总评分一半的背包完全装满。</p>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p><em>里面有一些测试输出的代码没有删。</em><br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> N 6</span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="type">int</span> Sum;</span><br><span class="line"><span class="type">int</span> num[N];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">Cout</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp; v)</span></span>&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;v.<span class="built_in">size</span>();i++)</span><br><span class="line">        cout&lt;&lt;v[i]&lt;&lt;<span class="string">&#x27; &#x27;</span>;</span><br><span class="line">        cout&lt;&lt;endl;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    Sum=<span class="number">0</span>;</span><br><span class="line">    vector&lt;<span class="type">int</span>&gt;items;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">1</span>;i&lt;=N;i++)&#123;</span><br><span class="line">        cin&gt;&gt;num[i];</span><br><span class="line">        Sum+=i*num[i];</span><br><span class="line"></span><br><span class="line">        <span class="type">int</span> t = <span class="number">1</span>;</span><br><span class="line">        <span class="type">int</span> num_ = num[i];</span><br><span class="line">        <span class="keyword">while</span>(num_&gt;=t)&#123;</span><br><span class="line">            items.<span class="built_in">push_back</span>(i*t);</span><br><span class="line">            num_-=t;</span><br><span class="line">            t*=<span class="number">2</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span>(num_) items.<span class="built_in">push_back</span>(i*num_);</span><br><span class="line">    &#125; </span><br><span class="line"></span><br><span class="line">    <span class="comment">//Cout(items);</span></span><br><span class="line">    <span class="keyword">if</span>(Sum%<span class="number">2</span>==<span class="number">1</span>)&#123;</span><br><span class="line">        cout&lt;&lt;<span class="literal">false</span>&lt;&lt;endl;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="type">int</span> V = Sum/<span class="number">2</span>;</span><br><span class="line">    <span class="comment">//cout&lt;&lt;Sum&lt;&lt;endl;</span></span><br><span class="line">    <span class="type">int</span> f[V];</span><br><span class="line">    <span class="built_in">memset</span>(f,<span class="number">128</span>,<span class="built_in">sizeof</span>(f));</span><br><span class="line">    <span class="comment">//cout&lt;&lt;f[0]&lt;&lt;endl&lt;&lt;f[1]&lt;&lt;endl;</span></span><br><span class="line">    f[<span class="number">0</span>]=<span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;items.<span class="built_in">size</span>();i++)&#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> j=V;j&gt;=items[i];j--)&#123;</span><br><span class="line">            f[j] = <span class="built_in">max</span>(f[j],f[j-items[i]]+items[i]);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//cout&lt;&lt;f[V]&lt;&lt;endl;</span></span><br><span class="line">    <span class="keyword">if</span>(f[V]==V) cout&lt;&lt;<span class="literal">true</span>&lt;&lt;endl;</span><br><span class="line">    <span class="keyword">else</span> cout&lt;&lt;<span class="literal">false</span>&lt;&lt;endl;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//1 2 3 4 5 6</span></span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>acm</category>
      </categories>
      <tags>
        <tag>动态规划</tag>
        <tag>多重背包</tag>
      </tags>
  </entry>
  <entry>
    <title>木材切割问题</title>
    <url>/2020/12/29/%E6%9C%A8%E6%9D%90%E5%88%87%E5%89%B2%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<h1 id="木材切割问题"><a href="#木材切割问题" class="headerlink" title="木材切割问题"></a>木材切割问题</h1><p>一个木匠从木材公司买了一批木材，每块木材的长度均相同，但由于制作家具时所需的木块长度各不相同，因此需要把这些木材切割成长度不同的木块。同时每次切割时由于锯子本身有一定的宽度，因此每切割一次就会浪费掉一些木料。请设计一个程序使木匠能够用最少的木材切割出所需的木块。</p>
<h2 id="输入描述："><a href="#输入描述：" class="headerlink" title="输入描述："></a>输入描述：</h2><p>输入有若干个测试样例，每个测试样例占一行。每行由若干个整数构成，第一个整数为所购买的木块的长度L$(0&lt;L&lt;=30000)$,第二个整数为锯子的宽度W$(0&lt;W&lt;=1000)$,其后的若干个整数分别表示制作家具时需要的木块的长度。</p>
<h2 id="输出描述："><a href="#输出描述：" class="headerlink" title="输出描述："></a>输出描述：</h2><p>每个测试样例输出一行，为一个整数N，表示制作家具时需要购买的木块的数量。</p>
<h2 id="样例输入："><a href="#样例输入：" class="headerlink" title="样例输入："></a>样例输入：</h2><p>1000 100 250 250 500 650 1000</p>
<p>1000 50 200 250 250 500 650 970</p>
<h2 id="样例输出："><a href="#样例输出：" class="headerlink" title="样例输出："></a>样例输出：</h2><p>3</p>
<p>4</p>
<h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a><strong>思路</strong></h2><p>这里先解决掉每次切割都会损耗锯子宽度的木材的问题。 给每个需要的木材加上锯子的宽度，然后给购买的木材的长度也加上一个木块的长度。这样就可以不考虑切割过程中锯子的损耗了。</p>
<p>然后就是贪心了，从木块中选择长度小于当前木料剩余长度的最大木块切割出去，如果没有一块小于当前木块了，那就用一块新的。消耗木材量加一。</p>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a><strong>代码</strong></h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> bisect</span><br><span class="line">n = <span class="built_in">int</span>(<span class="built_in">input</span>())</span><br><span class="line"></span><br><span class="line">maxlen = <span class="built_in">int</span>(<span class="built_in">input</span>())</span><br><span class="line">len_a = <span class="built_in">int</span>(<span class="built_in">input</span>())</span><br><span class="line"></span><br><span class="line">lens = [<span class="number">0</span>]*n</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">	lens[i] = <span class="built_in">int</span>(<span class="built_in">input</span>())</span><br><span class="line">	lens[i] += len_a</span><br><span class="line"></span><br><span class="line">maxlen += len_a</span><br><span class="line"></span><br><span class="line">lens.sort()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(lens)</span><br><span class="line"></span><br><span class="line">left = maxlen</span><br><span class="line">ans = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> <span class="built_in">len</span>(lens):</span><br><span class="line">	</span><br><span class="line">	site = bisect.bisect(lens,left)</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">if</span> site == <span class="number">0</span>:</span><br><span class="line">		ans += <span class="number">1</span></span><br><span class="line">		left = maxlen</span><br><span class="line">		<span class="keyword">continue</span></span><br><span class="line">		</span><br><span class="line">	left -= lens[site-<span class="number">1</span>]</span><br><span class="line">	lens.pop(site-<span class="number">1</span>)</span><br><span class="line">	</span><br><span class="line"><span class="keyword">if</span> left != maxlen:</span><br><span class="line">	ans+=<span class="number">1</span></span><br><span class="line">		</span><br><span class="line">	<span class="built_in">print</span>(site,left,lens)</span><br><span class="line"><span class="built_in">print</span>(ans)</span><br></pre></td></tr></table></figure>
<p>回溯法</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="type">int</span> arr[<span class="number">105</span>][<span class="number">105</span>];</span><br><span class="line"><span class="type">int</span> L = <span class="number">1000</span>, W = <span class="number">50</span>;</span><br><span class="line"><span class="comment">// int len[105] = &#123;200,250,250,500,650,970&#125;, n = 6;</span></span><br><span class="line"><span class="type">int</span> len[<span class="number">105</span>] = &#123;<span class="number">970</span>,<span class="number">650</span>,<span class="number">500</span>,<span class="number">250</span>,<span class="number">250</span>,<span class="number">200</span>&#125;, n = <span class="number">6</span>;</span><br><span class="line"><span class="type">int</span> mark[<span class="number">105</span>];</span><br><span class="line"><span class="type">int</span> ans = <span class="number">6</span>;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">foo</span><span class="params">(<span class="type">int</span> k)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; k; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; n; j++)</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">&quot;%d%c&quot;</span>, arr[i][j], j == n - <span class="number">1</span> ? <span class="string">&#x27;\n&#x27;</span> : <span class="string">&#x27; &#x27;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;------------\n&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">dfs</span><span class="params">(<span class="type">int</span> row, <span class="type">int</span> col, <span class="type">int</span> remain)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (row == n)</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> (!mark[i])</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">if</span> (i == n - <span class="number">1</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span> (row + <span class="number">1</span> &lt; ans)</span><br><span class="line">            &#123;</span><br><span class="line">                ans = row + <span class="number">1</span>;</span><br><span class="line">                <span class="built_in">foo</span>(ans);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = col + <span class="number">1</span>; i &lt; n; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> (mark[i] || remain &lt; len[i])</span><br><span class="line">            <span class="keyword">continue</span>;</span><br><span class="line">        arr[row][i] = <span class="number">1</span>, mark[i] = <span class="number">1</span>;</span><br><span class="line">        <span class="built_in">dfs</span>(row, i, remain - len[i] - W);</span><br><span class="line">        arr[row][i] = <span class="number">0</span>, mark[i] = <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">dfs</span>(row + <span class="number">1</span>, <span class="number">-1</span>, L);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; n; j++)</span><br><span class="line">            arr[i][j] = <span class="number">0</span>;</span><br><span class="line">        mark[i] = <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">dfs</span>(<span class="number">0</span>, <span class="number">-1</span>, L);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>目标函数： min n  n’L can split to len[]  最小的n,使的n个长度为L的木板能够分割为长度为数组len的木板。</p>
<p>目标函数的界限：size(len)  最多需要n个木料就能切割出n块木板。</p>
<p>约束条件：remain&gt;=len[i]  剩余的木料长度大于等与需要切割的木板长度。</p>
<p>限界函数：当前使用木板数量+剩余需要切割木板数量  这里用的递归所以没用到限界函数。</p>
]]></content>
      <categories>
        <category>acm</category>
      </categories>
      <tags>
        <tag>贪心</tag>
      </tags>
  </entry>
  <entry>
    <title>格雷码</title>
    <url>/2020/12/29/%E6%A0%BC%E9%9B%B7%E7%A0%81/</url>
    <content><![CDATA[<h1 id="格雷码"><a href="#格雷码" class="headerlink" title="格雷码"></a>格雷码</h1><p>格雷码就是前一个数的格雷码和后一个数的格雷码只有一位二进制不一样。第一个数和最后一个也一样。</p>
<p>格雷码的生成有很多种方式</p>
<p>第一种是直接从二进制获取格雷码，首位保留，后面的第i位格雷码为第i-1位二进制和第i位的异或值。</p>
<p>代码如下</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="type">int</span> n;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">bin</span><span class="params">(<span class="type">int</span> x)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(x==<span class="number">0</span>) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    <span class="type">int</span> pre_b = <span class="built_in">bin</span>(x/<span class="number">2</span>);</span><br><span class="line">    <span class="type">int</span> b = x%<span class="number">2</span>;</span><br><span class="line">    cout&lt;&lt;(b^pre_b);</span><br><span class="line">    <span class="keyword">return</span> b;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">while</span>(cin&gt;&gt;n)&#123;</span><br><span class="line">        <span class="built_in">bin</span>(n);</span><br><span class="line">        cout&lt;&lt;endl;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>不过这道题是规定了二进制码的长度然后输出所有的格雷码。</p>
<p>（1） 初始化0的格雷码为全0</p>
<p>（2） 改变最右边的位元值</p>
<p>（3） 改变右起第一个为1的位元的左边位元</p>
<p>（4） 重复上面的步骤直到所有格雷码都产生</p>
<p>时间复杂度$O(n^2)$</p>
<p>代码如下</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="type">int</span> n;</span><br><span class="line"><span class="type">int</span> gray[<span class="number">20</span>];</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">cout_gray</span><span class="params">(<span class="type">int</span>*  gray,<span class="type">int</span> n)</span></span>&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;n;i++)</span><br><span class="line">        cout&lt;&lt;gray[i];</span><br><span class="line">    cout&lt;&lt;endl;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">while</span>(cin&gt;&gt;n)&#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;n;i++) gray[i]=<span class="number">0</span>;</span><br><span class="line">        <span class="built_in">cout_gray</span>(gray,n);</span><br><span class="line">        <span class="type">int</span> count=<span class="number">1</span>;</span><br><span class="line">        <span class="keyword">do</span>&#123;</span><br><span class="line">            <span class="keyword">if</span>(count%<span class="number">2</span>==<span class="number">1</span>)&#123;</span><br><span class="line">                gray[n<span class="number">-1</span>]^=<span class="number">1</span>;</span><br><span class="line">            &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                <span class="type">int</span> site = n<span class="number">-1</span>;</span><br><span class="line">                <span class="keyword">while</span>(gray[site]==<span class="number">0</span>) site--;</span><br><span class="line">                gray[(site<span class="number">-1</span>+n)%n]^=<span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="built_in">cout_gray</span>(gray,n);</span><br><span class="line">            count++;</span><br><span class="line">        &#125;<span class="keyword">while</span>(count&lt;<span class="built_in">pow</span>(<span class="number">2</span>,n));</span><br><span class="line">        cout&lt;&lt;endl;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>acm</category>
      </categories>
      <tags>
        <tag>格雷码</tag>
      </tags>
  </entry>
  <entry>
    <title>求两圆相交面积</title>
    <url>/2020/12/29/%E6%B1%82%E4%B8%A4%E5%9C%86%E7%9B%B8%E4%BA%A4%E9%9D%A2%E7%A7%AF/</url>
    <content><![CDATA[<h1 id="求两圆相交的面积"><a href="#求两圆相交的面积" class="headerlink" title="求两圆相交的面积"></a>求两圆相交的面积</h1><p>输入两圆的圆心和半径，输出两圆相交的面积。</p>
<h2 id="输入"><a href="#输入" class="headerlink" title="输入"></a>输入</h2><p>第一行为 x1,y1,r1 表示第一个圆的圆心坐标和半径。<br>第二行为 x2,y2,r2 表示第二个圆的圆心坐标和半径。</p>
<h2 id="输出"><a href="#输出" class="headerlink" title="输出"></a>输出</h2><p>一个数，表示两圆相交的面积。</p>
<h2 id="（1）-思路"><a href="#（1）-思路" class="headerlink" title="（1） 思路 "></a><strong>（1） 思路 </strong></h2><p>画个图就能看出来相交面积就是两个扇形面积减去四边形面积。</p>
<p>求出两圆心的距离，然后判断相离，包含，还是覆盖，然后覆盖的话 </p>
<p>首先用余弦公式求出两扇形的圆心角度 $\cos A = (b^2+c^2-a^2)/2bc$,然后算出扇形面积。</p>
<p>然后使用海伦公式通过三角形，相减得到相交面积。</p>
<p>这里注意反三角函数返回的是弧度制。 1度=π/180≈0.01745弧度，1弧度=180/π≈57.3度。</p>
<h2 id="2-代码"><a href="#2-代码" class="headerlink" title="(2) 代码"></a><strong>(2) 代码</strong></h2><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;cmath&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> PI 3.1415926</span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="type">int</span> x_1,y_1,r_1,x_2,y_2,r_2;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">while</span>(cin &gt;&gt; x_1 &gt;&gt; y_1 &gt;&gt; r_1)&#123;</span><br><span class="line">        cin &gt;&gt; y_1 &gt;&gt; y_2 &gt;&gt; r_2;</span><br><span class="line">        <span class="type">double</span> ans=<span class="number">0</span>;</span><br><span class="line">        <span class="type">double</span> dis = <span class="built_in">sqrt</span>(<span class="built_in">pow</span>(x_1-y_1,<span class="number">2</span>)+<span class="built_in">pow</span>(y_1-y_2,<span class="number">2</span>));</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>(dis &gt;= r_1+r_2)&#123;</span><br><span class="line">            ans = <span class="number">0</span>;</span><br><span class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span>(dis &lt;= <span class="built_in">abs</span>(r_1-r_2))&#123;</span><br><span class="line">            ans = PI*<span class="built_in">pow</span>(<span class="built_in">min</span>(r_1,r_2),<span class="number">2</span>);</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            <span class="type">double</span> p = (dis+r_1+r_2)/<span class="number">2</span>;</span><br><span class="line">            <span class="type">double</span> angle1 = <span class="number">2</span>*<span class="built_in">acos</span>((r_1*r_1+dis*dis-r_2*r_2)/(<span class="number">2</span>*r_1*dis));</span><br><span class="line">            <span class="type">double</span> angle2 = <span class="number">2</span>*<span class="built_in">acos</span>((r_2*r_2+dis*dis-r_1*r_1)/(<span class="number">2</span>*r_2*dis));</span><br><span class="line">            <span class="type">double</span> area1 = (angle1*r_1*r_1+angle2*r_2*r_2)/<span class="number">2</span>;</span><br><span class="line">            <span class="type">double</span> area2 = <span class="built_in">sqrt</span>(p*(p-dis)*(p-r_1)*(p-r_2));</span><br><span class="line"></span><br><span class="line">            ans = area1-area2;</span><br><span class="line">        &#125;</span><br><span class="line">        cout&lt;&lt;ans&lt;&lt;endl;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>acm</category>
      </categories>
      <tags>
        <tag>几何</tag>
        <tag>余弦公式</tag>
        <tag>海伦秦九韶公式</tag>
      </tags>
  </entry>
  <entry>
    <title>跳跃问题</title>
    <url>/2020/12/30/%E8%B7%B3%E8%B7%83%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<h1 id="跳跃问题"><a href="#跳跃问题" class="headerlink" title="跳跃问题"></a>跳跃问题</h1><p> 某n*n的棋盘的每个格子中都有1到9个整数。从棋盘的左上角出发，向右或向下每次跳跃格子所指定的格数，判断是否能够到达棋盘的右下角。如果能，输出true, 否则输出false.</p>
<h2 id="输入："><a href="#输入：" class="headerlink" title="输入："></a>输入：</h2><p>输入第一行为整数n，表示棋盘方格的数量。其后的n行，各行均有n个数字。表示在该方格中可以向下或向右跳跃的方格数。</p>
<h2 id="输出："><a href="#输出：" class="headerlink" title="输出："></a>输出：</h2><p>输出1行，true或者false. true表示从左上角可以跳跃到右下角,否则为false。</p>
<h2 id="样例输入："><a href="#样例输入：" class="headerlink" title="样例输入："></a>样例输入：</h2><pre><code>7 
2 5 1 6 1 4 1
6 1 1 2 2 9 3
7 2 3 2 1 3 1
1 1 3 1 7 1 2
4 1 2 3 4 1 2
3 3 1 2 3 4 1
1 5 2 9 4 7 1
</code></pre><h2 id="样例输出："><a href="#样例输出：" class="headerlink" title="样例输出："></a>样例输出：</h2><pre><code>true  
</code></pre><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>没什么好说的，如果dp[i][j]能到达则dp[i+data[i][j]][j] , dp[i][j+data[i][j]] 也能到达。</p>
<p>时间复杂度$O(n^2)$</p>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line"> <span class="type">int</span> n;</span><br><span class="line"> <span class="type">int</span> data[<span class="number">100</span>][<span class="number">100</span>];</span><br><span class="line"> <span class="type">bool</span> dp[<span class="number">100</span>][<span class="number">100</span>];</span><br><span class="line"> cin&gt;&gt;n;</span><br><span class="line"> <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;n;i++)&#123;</span><br><span class="line">     <span class="keyword">for</span>(<span class="type">int</span> j=<span class="number">0</span>;j&lt;n;j++)&#123;</span><br><span class="line">         cin&gt;&gt;data[i][j];</span><br><span class="line">         dp[i][j]=<span class="literal">false</span>;</span><br><span class="line">     &#125;</span><br><span class="line"> &#125;</span><br><span class="line"> dp[<span class="number">0</span>][<span class="number">0</span>]=<span class="literal">true</span>;</span><br><span class="line"> <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;n;i++)&#123;</span><br><span class="line">     <span class="keyword">for</span>(<span class="type">int</span> j=<span class="number">0</span>;j&lt;n;j++)&#123;</span><br><span class="line">         <span class="keyword">if</span>(dp[i][j]==<span class="literal">true</span>)&#123;</span><br><span class="line">             dp[i+data[i][j]][j]=<span class="literal">true</span>;</span><br><span class="line">             dp[i][j+data[i][j]]=<span class="literal">true</span>;</span><br><span class="line">         &#125;</span><br><span class="line">     &#125;</span><br><span class="line"> &#125;</span><br><span class="line"> cout&lt;&lt;dp[n<span class="number">-1</span>][n<span class="number">-1</span>]&lt;&lt;endl;</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>acm</category>
      </categories>
      <tags>
        <tag>dp</tag>
      </tags>
  </entry>
  <entry>
    <title>GoneFishing</title>
    <url>/2021/01/03/GoneFishing/</url>
    <content><![CDATA[<h1 id="Gone-Fishing"><a href="#Gone-Fishing" class="headerlink" title="Gone Fishing"></a>Gone Fishing</h1><p>John is going on a fishing trip. He has h hours available (1 &lt;= h &lt;= 16), and there are n lakes in the area (2 &lt;= n &lt;= 25) all reachable along a single, one-way road. John starts at lake 1, but he can finish at any lake he wants. He can only travel from one lake to the next one, but he does not have to stop at any lake unless he wishes to. For each i = 1,…,n - 1, the number of 5-minute intervals it takes to travel from lake i to lake i + 1 is denoted ti (0 &lt; ti &lt;=192). For example, t3 = 4 means that it takes 20 minutes to travel from lake 3 to lake 4. To help plan his fishing trip, John has gathered some information about the lakes. For each lake i, the number of fish expected to be caught in the initial 5 minutes, denoted fi( fi &gt;= 0 ), is known. Each 5 minutes of fishing decreases the number of fish expected to be caught in the next 5-minute interval by a constant rate of di (di &gt;= 0). If the number of fish expected to be caught in an interval is less than or equal to di , there will be no more fish left in the lake in the next interval. To simplify the planning, John assumes that no one else will be fishing at the lakes to affect the number of fish he expects to catch. </p>
<p>Write a program to help John plan his fishing trip to maximize the number of fish expected to be caught. The number of minutes spent at each lake must be a multiple of 5. </p>
<h2 id="Input"><a href="#Input" class="headerlink" title="Input"></a>Input</h2><p>You will be given a number of cases in the input. Each case starts with a line containing n. This is followed by a line containing h. Next, there is a line of n integers specifying fi (1 &lt;= i &lt;=n), then a line of n integers di (1 &lt;=i &lt;=n), and finally, a line of n - 1 integers ti (1 &lt;=i &lt;=n - 1). Input is terminated by a case in which n = 0. </p>
<h2 id="Output"><a href="#Output" class="headerlink" title="Output"></a>Output</h2><p>For each test case, print the number of minutes spent at each lake, separated by commas, for the plan achieving the maximum number of fish expected to be caught (you should print the entire plan on one line even if it exceeds 80 characters). This is followed by a line containing the number of fish expected.<br>If multiple plans exist, choose the one that spends as long as possible at lake 1, even if no fish are expected to be caught in some intervals. If there is still a tie, choose the one that spends as long as possible at lake 2, and so on. Insert a blank line between cases. </p>
<h2 id="Sample-Input"><a href="#Sample-Input" class="headerlink" title="Sample Input"></a>Sample Input</h2><pre><code>2 
1 
10 1 
2 5 
2 
4 
4 
10 15 20 17 
0 3 4 3 
1 2 3 
4 
4 
10 15 50 30 
0 3 4 3 
1 2 3 
0 
</code></pre><h2 id="Sample-Output"><a href="#Sample-Output" class="headerlink" title="Sample Output"></a>Sample Output</h2><pre><code>45, 5 
Number of fish expected: 31 

240, 0, 0, 0 
Number of fish expected: 480 

115, 10, 50, 35 
Number of fish expected: 724 
</code></pre><p>约翰是个垂钓谜，星期天他决定外出钓鱼h小时（1≤h≤16），约翰家附近共有n个池塘（2≤n≤25），这些池塘分布在一条直线上，约翰将这些池塘按离家的距离由近到远编上号，依次为L1，L2，…，Ln，约翰家门外就是第一个池塘，所以他到第一个池塘是不用花时间的。</p>
<p>约翰可以任选若干个池塘由近到远地垂钓，并且在每个池塘他都可以呆上任意长的时间，但呆的时间必须为5分钟的倍数（即5分钟为一个单位时间），已知从池塘Li到池塘Li+1要化去约翰ti个单位时间。每个池塘的上鱼率预先也是已知的，池塘Li在第一个单位时间内能钓到的鱼为Fi（0≤Fi≤100），并且每当他在某一个鱼塘呆上一个单位时间后，该鱼塘单位时间内能钓到的鱼将减少一个常数di（0≤di≤100）。</p>
<p>现在请你编一个程序计算约翰最多能钓到多少鱼。</p>
<h2 id="思路：枚举每个池塘为终点，每次选择能掉到最多的鱼的池塘钓鱼。-输出能掉到最多的鱼的方案即可。"><a href="#思路：枚举每个池塘为终点，每次选择能掉到最多的鱼的池塘钓鱼。-输出能掉到最多的鱼的方案即可。" class="headerlink" title="思路：枚举每个池塘为终点，每次选择能掉到最多的鱼的池塘钓鱼。 输出能掉到最多的鱼的方案即可。"></a>思路：枚举每个池塘为终点，每次选择能掉到最多的鱼的池塘钓鱼。 输出能掉到最多的鱼的方案即可。</h2><h2 id="代码："><a href="#代码：" class="headerlink" title="代码："></a>代码：</h2><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> MAXN 30</span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="type">int</span> n,h,fi[MAXN],di[MAXN],ti[MAXN],ans[MAXN][MAXN],ft[MAXN],tw[MAXN];</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">while</span>(cin&gt;&gt;n&amp;&amp;n!=<span class="number">0</span>)&#123;</span><br><span class="line">        cin&gt;&gt;h;</span><br><span class="line">        tw[<span class="number">1</span>] = <span class="number">0</span>;</span><br><span class="line">        h *= <span class="number">12</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">1</span>;i&lt;=n;i++) cin&gt;&gt;fi[i];</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">1</span>;i&lt;=n;i++) cin&gt;&gt;di[i];</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">2</span>;i&lt;=n;i++) &#123;</span><br><span class="line">            cin&gt;&gt;ti[i];</span><br><span class="line">            tw[i] = tw[i<span class="number">-1</span>]+ti[i];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">memset</span>(ans,<span class="number">0</span>,<span class="built_in">sizeof</span>(ans));</span><br><span class="line">        <span class="type">int</span> ans_index = <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> end=<span class="number">1</span>;end&lt;=n;end++)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">1</span>;i&lt;=n;i++) ft[i] = fi[i];</span><br><span class="line">            <span class="type">int</span> ht = h;</span><br><span class="line">            ht -= tw[end];</span><br><span class="line"></span><br><span class="line">            <span class="type">int</span> max_index=<span class="number">1</span>,emp_c = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">while</span>(ht&gt;<span class="number">0</span>&amp;&amp;emp_c&lt;end)&#123;</span><br><span class="line">                <span class="keyword">for</span>(<span class="type">int</span> j=<span class="number">1</span>;j&lt;=end;j++)&#123;</span><br><span class="line">                    <span class="keyword">if</span>(ft[j]&gt;ft[max_index])&#123;</span><br><span class="line">                        max_index=j;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">                --ht;</span><br><span class="line">                ans[end][<span class="number">0</span>]+=ft[max_index];</span><br><span class="line">                ++ans[end][max_index];</span><br><span class="line">                ft[max_index]-=di[max_index];</span><br><span class="line">                <span class="keyword">if</span>(ft[max_index]&lt;=<span class="number">0</span>)&#123;</span><br><span class="line">                    emp_c++;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span>(ht&gt;<span class="number">0</span>) ans[end][<span class="number">1</span>]+=ht;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span>(ans[end][<span class="number">0</span>]&gt;ans[ans_index][<span class="number">0</span>]) ans_index=end;</span><br><span class="line">        &#125;</span><br><span class="line">        cout&lt;&lt;ans[ans_index][<span class="number">1</span>]*<span class="number">5</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">2</span>;i&lt;=n;i++)&#123;</span><br><span class="line">            cout&lt;&lt;<span class="string">&quot;, &quot;</span>&lt;&lt;ans[ans_index][i]*<span class="number">5</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        cout&lt;&lt;endl&lt;&lt;<span class="string">&quot;Number of fish expected: &quot;</span>&lt;&lt;ans[ans_index][<span class="number">0</span>]&lt;&lt;endl&lt;&lt;endl;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>acm</category>
      </categories>
      <tags>
        <tag>贪心</tag>
        <tag>枚举</tag>
      </tags>
  </entry>
  <entry>
    <title>博物馆守卫问题</title>
    <url>/2021/01/03/%E5%8D%9A%E7%89%A9%E9%A6%86%E5%AE%88%E5%8D%AB%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<h1 id="博物馆守卫问题-世界名画陈列馆问题"><a href="#博物馆守卫问题-世界名画陈列馆问题" class="headerlink" title="博物馆守卫问题/世界名画陈列馆问题"></a>博物馆守卫问题/世界名画陈列馆问题</h1><p>在某博物馆中摆放了非常重要的文物，为了节省人力，该博物馆专门购买了警卫机器人来看管这些文物。该博物馆的房间排列整齐，房间的大小相同。每个警卫机器人能够巡查的范围除本身所在房间外，还包括其起始安放的房间的上下左右四个房间。为了减少摆放的机器人的数量，请你设计一种最佳的摆放方案，使得摆放的机器人数量最少。</p>
<h2 id="输入："><a href="#输入：" class="headerlink" title="输入："></a>输入：</h2><p>输入一行，有两个整数m,n,分别表示该博物馆每行的房间数和每列的房间数。博物馆总房间数即为m*n。</p>
<h2 id="输出："><a href="#输出：" class="headerlink" title="输出："></a>输出：</h2><p>输出的第一行表示需要的机器人的数量，其后m行，每行有n个元素，每个元素的值为0或1，分别表示对应的房间是否摆放机器人。0表示不摆放，1表示需要摆放。</p>
<h2 id="样例输入："><a href="#样例输入：" class="headerlink" title="样例输入："></a>样例输入：</h2><pre><code>4 4
</code></pre><h2 id="样例输出："><a href="#样例输出：" class="headerlink" title="样例输出："></a>样例输出：</h2><pre><code>4
0 0 1 0
1 0 0 0
0 0 0 1
0 1 0 0 
</code></pre><p>这题的原名叫世界名画陈列馆问题，网上找着看了下，还是有点东西的。 这种问题分为，可重复坚守和不可重复坚守两类。 首先看看可重复坚守。</p>
<p>下面这种方法是分支限定法，这种方法复杂度太高。</p>
<h3 id="思路：找到一个未被监视的格子-i-j-则想要-i-j-被监视的话，我们可以在-i-j-i-1-j-i-1-j-i-j-1-i-j-1-这五个位置放置机器人，然后就更具界限函数放入有限队列对于每个状态继续找空位放置就行，中间需要根据约束条件和目标函数限界进行剪枝。"><a href="#思路：找到一个未被监视的格子-i-j-则想要-i-j-被监视的话，我们可以在-i-j-i-1-j-i-1-j-i-j-1-i-j-1-这五个位置放置机器人，然后就更具界限函数放入有限队列对于每个状态继续找空位放置就行，中间需要根据约束条件和目标函数限界进行剪枝。" class="headerlink" title="思路：找到一个未被监视的格子[i][j]则想要[i][j]被监视的话，我们可以在[i][j],[i-1][j],[i+1][j],[i][j-1],[i][j+1]这五个位置放置机器人，然后就更具界限函数放入有限队列对于每个状态继续找空位放置就行，中间需要根据约束条件和目标函数限界进行剪枝。"></a>思路：找到一个未被监视的格子[i][j]则想要[i][j]被监视的话，我们可以在[i][j],[i-1][j],[i+1][j],[i][j-1],[i][j+1]这五个位置放置机器人，然后就更具界限函数放入有限队列对于每个状态继续找空位放置就行，中间需要根据约束条件和目标函数限界进行剪枝。</h3><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;string.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;queue&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="type">int</span> n,m,f[<span class="number">5</span>][<span class="number">2</span>]=&#123;&#123;<span class="number">0</span>,<span class="number">0</span>&#125;,&#123;<span class="number">0</span>,<span class="number">1</span>&#125;,&#123;<span class="number">0</span>,<span class="number">-1</span>&#125;,&#123;<span class="number">1</span>,<span class="number">0</span>&#125;,&#123;<span class="number">-1</span>,<span class="number">0</span>&#125;&#125;; <span class="comment">//自己本身+上下左右 </span></span><br><span class="line"><span class="type">int</span> anx[<span class="number">30</span>][<span class="number">30</span>],ans; <span class="comment">//最优结果 ans-警卫个数 anx-警卫位置 </span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">Node</span>&#123;</span><br><span class="line">	<span class="type">int</span> pu[<span class="number">30</span>][<span class="number">30</span>];<span class="comment">//pu-警卫位置 </span></span><br><span class="line">	<span class="type">int</span> spy[<span class="number">30</span>][<span class="number">30</span>]; <span class="comment">//spy-被监视的展柜位置</span></span><br><span class="line">	<span class="type">int</span> i,j,k,t; <span class="comment">//(i,j)为当前坐标 k-警卫数 t-被监视的展柜个数 </span></span><br><span class="line">&#125;;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">cmp</span>&#123; <span class="comment">//重写比较函数</span></span><br><span class="line">    <span class="function"><span class="type">bool</span> <span class="title">operator</span><span class="params">()</span> <span class="params">(Node a, Node b)</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> a.t &gt; b.t; <span class="comment">//小顶堆</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="comment">//priority_queue&lt;Node&gt; q; //优先队列 </span></span><br><span class="line">priority_queue&lt;Node, vector&lt;Node&gt;, cmp&gt; q;</span><br><span class="line"><span class="function">Node <span class="title">init</span><span class="params">(Node node)</span></span>&#123;</span><br><span class="line">	<span class="built_in">memset</span>(node.pu,<span class="number">0</span>,<span class="built_in">sizeof</span>(node.pu)); </span><br><span class="line">	<span class="built_in">memset</span>(node.spy,<span class="number">0</span>,<span class="built_in">sizeof</span>(node.spy)); </span><br><span class="line">	node.i=<span class="number">1</span>;node.j=<span class="number">1</span>;</span><br><span class="line">	node.k=<span class="number">0</span>;node.t=<span class="number">0</span>;</span><br><span class="line">	<span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;=n+<span class="number">1</span>;i++)node.spy[i][<span class="number">0</span>]=node.spy[i][m+<span class="number">1</span>]=<span class="number">1</span>;</span><br><span class="line">	<span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;=m+<span class="number">1</span>;i++)node.spy[<span class="number">0</span>][i]=node.spy[n+<span class="number">1</span>][i]=<span class="number">1</span>;</span><br><span class="line">	<span class="keyword">return</span> node;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">puta</span><span class="params">(Node p,<span class="type">int</span> x,<span class="type">int</span> y)</span></span>&#123;</span><br><span class="line">	<span class="keyword">if</span>(p.pu[x][y]) <span class="keyword">return</span>;</span><br><span class="line">	Node node;</span><br><span class="line">	node=<span class="built_in">init</span>(node);</span><br><span class="line">	node.i=p.i;</span><br><span class="line">	node.j=p.j;</span><br><span class="line">	node.k=p.k+<span class="number">1</span>;</span><br><span class="line">	node.t=p.t;</span><br><span class="line">	<span class="built_in">memcpy</span>(node.pu, p.pu, <span class="built_in">sizeof</span>(p.pu));</span><br><span class="line">	<span class="built_in">memcpy</span>(node.spy, p.spy, <span class="built_in">sizeof</span>(p.spy));</span><br><span class="line">	node.pu[x][y]=<span class="number">1</span>;</span><br><span class="line">	<span class="keyword">for</span>(<span class="type">int</span> d=<span class="number">0</span>;d&lt;<span class="number">5</span>;d++)&#123;</span><br><span class="line">        <span class="type">int</span> xx=x+f[d][<span class="number">0</span>];</span><br><span class="line">        <span class="type">int</span> yy=y+f[d][<span class="number">1</span>];</span><br><span class="line">        node.spy[xx][yy]++;</span><br><span class="line">        <span class="keyword">if</span>(node.spy[xx][yy]==<span class="number">1</span>) &#123;</span><br><span class="line">			node.t++;</span><br><span class="line">		&#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">while</span>(node.i&lt;=n&amp;&amp;node.spy[node.i][node.j])&#123; <span class="comment">//已放置的不再被搜索</span></span><br><span class="line">		node.j++;</span><br><span class="line">		<span class="keyword">if</span>(node.j&gt;m)node.i++,node.j=<span class="number">1</span>; <span class="comment">//换行 </span></span><br><span class="line">	&#125;</span><br><span class="line">	q.<span class="built_in">push</span>(node);</span><br><span class="line">	<span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">	<span class="built_in">scanf</span>(<span class="string">&quot;%d%d&quot;</span>,&amp;n,&amp;m);</span><br><span class="line">	ans=n*m/<span class="number">3</span>+<span class="number">2</span>;</span><br><span class="line">	Node node;</span><br><span class="line">	node=<span class="built_in">init</span>(node);</span><br><span class="line">	q.<span class="built_in">push</span>(node);</span><br><span class="line">	<span class="keyword">while</span>(!q.<span class="built_in">empty</span>())&#123;<span class="comment">//队列非空 </span></span><br><span class="line">        Node p=q.<span class="built_in">top</span>();</span><br><span class="line">        q.<span class="built_in">pop</span>();</span><br><span class="line">        <span class="keyword">if</span>(p.t&gt;=n*m)&#123;</span><br><span class="line">        	<span class="keyword">if</span>(p.k&lt;ans)&#123;</span><br><span class="line">	        	ans=p.k;</span><br><span class="line">				<span class="built_in">memcpy</span>(anx, p.pu, <span class="built_in">sizeof</span>(p.pu)); <span class="comment">//把put内容复制给anx </span></span><br><span class="line">	        &#125;	</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span>&#123;</span><br><span class="line">			<span class="keyword">if</span>(p.i&lt;n) <span class="built_in">puta</span>(p,p.i+<span class="number">1</span>,p.j);</span><br><span class="line">			<span class="keyword">if</span>((p.i==n&amp;&amp;p.j==m)||p.spy[p.i][p.j+<span class="number">1</span>]==<span class="number">0</span>) <span class="built_in">puta</span>(p,p.i,p.j);</span><br><span class="line">			<span class="keyword">if</span>(p.j&lt;m&amp;&amp;(p.spy[p.i][p.j+<span class="number">1</span>]==<span class="number">0</span>||p.spy[p.i][p.j+<span class="number">2</span>]==<span class="number">0</span>)) <span class="built_in">puta</span>(p,p.i,p.j+<span class="number">1</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        	</span><br><span class="line">    &#125;</span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">&quot;%d\n&quot;</span>,ans);</span><br><span class="line">	<span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">1</span>;i&lt;=n;i++)&#123;</span><br><span class="line">		<span class="keyword">for</span>(<span class="type">int</span> j=<span class="number">1</span>;j&lt;=m;j++)</span><br><span class="line">			<span class="built_in">printf</span>(<span class="string">&quot;%d &quot;</span>,anx[i][j]);</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;\n&quot;</span>);</span><br><span class="line">	&#125;	</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>其它的后面有空再补。</p>
<p><strong>目标函数</strong>：min robot c&gt;=m<em>n , 使用最小的机器人监视完m</em>n的矩阵。<br><strong>目标函数的界限</strong>：（m<em>n/3）+1 ，最多使用（m</em>n）/3+1个机器人就能够完全监视。<br><strong>约束条件</strong>：robot[i][j]==0 没有放置机器人<br><strong>限界函数</strong>：robot+(m<em>n-c)/5  最少一共需要robot+(m</em>n-c)/5个机器人就能够完全监视完。</p>
]]></content>
      <categories>
        <category>acm</category>
      </categories>
      <tags>
        <tag>分支限定法</tag>
      </tags>
  </entry>
  <entry>
    <title>矩阵分析</title>
    <url>/2021/01/05/%E7%9F%A9%E9%98%B5%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<h3 id="过度矩阵"><a href="#过度矩阵" class="headerlink" title="过度矩阵"></a>过度矩阵</h3><p>$(a_1,a_2,…,a_n)A=(b_1,b_2,…,b_n)$</p>
<p>则称矩阵$A$为$(a_1,a_2,…,a_n)$到$(b_1,b_2,…,b_n)$的过度矩阵。</p>
<p>$A = (a_1,a_2,…,a_n)^{-1}(b_1,b_2,…,b_n)$</p>
<h3 id="向量在基变换下的坐标变化"><a href="#向量在基变换下的坐标变化" class="headerlink" title="向量在基变换下的坐标变化"></a>向量在基变换下的坐标变化</h3><p>$\alpha_1,\alpha_2,…,\alpha_n$ 以及 $\beta_1,\beta_2,…,\beta_n$是$V$中的两个基，$\alpha$在上述两个基下的表示分别为：</p>
<p>$\alpha = (\alpha_1,\alpha_2,…,\alpha_n) \begin{bmatrix}<br>    k_1\\k_2\\ … \\k_n<br>\end{bmatrix} \quad \alpha = (\beta_1,\beta_2,…,\beta_n) \begin{bmatrix}<br>    l_1\\l_2\\ … \\l_n<br>\end{bmatrix}$</p>
<p>则有：</p>
<p>$\begin{bmatrix}<br>    l_1\\l_2\\ … \\l_n<br>\end{bmatrix} = A^{-1}\begin{bmatrix}<br>    k_1\\k_2\\ … \\k_n<br>\end{bmatrix}$</p>
<p>$A$表示$(a_1,a_2,…,a_n)$到$(b_1,b_2,…,b_n)$的过度矩阵。</p>
<h3 id="线性变换在不同基下的表示"><a href="#线性变换在不同基下的表示" class="headerlink" title="线性变换在不同基下的表示"></a>线性变换在不同基下的表示</h3><p>$V$是数域$P$上的一个$n$维线性空间，$\alpha_1,\alpha_2,…,\alpha_n$ 以及 $\beta_1,\beta_2,…,\beta_n$是$V$中的两个基，从前一个基到后一个基的过度矩阵为$C$.又设$T$是$V$的一个线性变换，它在前后两个基下的矩阵表示分别是$A,B$,则有$B=C^{-1}AC$</p>
<h3 id="线性空间和维数"><a href="#线性空间和维数" class="headerlink" title="*线性空间和维数"></a>*线性空间和维数</h3><p>设$V_1,V_2$是线性空间$V$的两个子空间。</p>
<p>$V_1 \cap V_2 = \{ a|a\in V_1 \quad and \quad a\in V_2\}$ 交空间</p>
<p>$V_1+V_2 = \{a=a_1+a_2| a_1\in V_1 \quad and \quad a_2\in V_2\}$ 和子空间</p>
<p>和空间的基和维度：</p>
<p>$V_1 = span\{\alpha_1,\alpha_2,…,\alpha_s\}$</p>
<p>$V_2 = span\{\beta_1,\beta_2,…,\beta_k\}$</p>
<p>$V_1 + V_2 = span\{\alpha_1,\alpha_2,…,\alpha_s,\beta_1,\beta_2,…,\beta_k\}$</p>
<p>求$V_1+V_2$的线性无关组就行了。</p>
<p>交空间的基和维数：</p>
<p>$\xi = k_1\alpha_1+k_1\alpha_2+…+k_s\alpha_x = l_1\beta_1+l_2\beta_2+…+l_k\beta_k$</p>
<p>$k_1\alpha_1+k_1\alpha_2+…+k_s\alpha_x - l_1\beta_1-l_2\beta_2-…-l_k\beta_k = 0$</p>
<p>求出$k_i,l_i$</p>
<p>$dim(V_1+V_2) = dim(V_1)+dim(V_2)-dim(V_1 \cap V_2)$</p>
<h3 id="最小二乘法"><a href="#最小二乘法" class="headerlink" title="*最小二乘法"></a>*最小二乘法</h3><p>待求解方程组</p>
<p>$AX=B$</p>
<p>左乘$A^T$</p>
<p>$A^TAX=A^TB$</p>
<p>$X = (A^TA)^{-1}A^TB$</p>
<p>$X$不唯一，$AX$唯一。</p>
<h3 id="正规矩阵"><a href="#正规矩阵" class="headerlink" title="*正规矩阵"></a>*正规矩阵</h3><p>设$A \in R^{n\times n}$ 若$A^HA=AA^H$,则称$A$为正规矩阵。</p>
<h3 id="向量范数"><a href="#向量范数" class="headerlink" title="*向量范数"></a>*向量范数</h3><p>$||\overset{\rightarrow}{v}||_p = (|v_1|^p+|v_2|^p+…+|v_n|^p)^{\frac{1}{p}}$</p>
<p>零范数：</p>
<p>$||\overset{\rightarrow}{v}||_0$=非零元素个数</p>
<p>1范数：</p>
<p>$||\overset{\rightarrow}{v}||_1 = |v_1|+|v_2|+…+|v_n|$</p>
<p>2范数：</p>
<p>$||\overset{\rightarrow}{v}||_2 = (v_1^2+v_2^2+…+v_n^2)^{\frac{1}{2}}$</p>
<p>无穷范数：</p>
<p>$||\overset{\rightarrow}{v}||_{\infty} = max|v_i|$</p>
<h3 id="矩阵范数"><a href="#矩阵范数" class="headerlink" title="*矩阵范数"></a>*矩阵范数</h3><p>$||A||_1$:最大列和范数，所有矩阵列向量计算模长和，取最大值。</p>
<p>$||A||_{\infty}$:最大行和范数，所有矩阵行向量计算模长和，取最大值。</p>
<p>$||A||_2 = \sqrt{max \lambda(A^HA)}$: $A^HA$的最大特征值开平方。</p>
<p>$||A||_F$: 元素模长的平方和再开平方。</p>
<h3 id="奇异值"><a href="#奇异值" class="headerlink" title="*奇异值"></a>*奇异值</h3><p>$A$的最小奇异值。</p>
<p>$B=A^HA \quad or \quad AA^H$</p>
<p>$B$的特征值为$d_1^2,d_2^2,…,d_r^2$</p>
<p>其中$d_1 \geq d_2 \geq … \geq d_r &gt;0$ 为$A$的奇异值。</p>
<p>$d_r$为$A$的最小奇异值。</p>
<h3 id="最小多项式"><a href="#最小多项式" class="headerlink" title="*最小多项式"></a><font color="red">*最小多项式</font></h3><p>写出初级因子后，比如$(\lambda-a_i)^{b_i}$,将每个$a_i$对于$b_i$最大的项乘起来，就是最小多项式。</p>
<p>$A = \begin{bmatrix}<br>    2\\<br>    1&amp;2\\<br>    &amp;&amp;2\\<br>    &amp;&amp;&amp;2\\<br>    &amp;&amp;&amp;&amp;3\\<br>    &amp;&amp;&amp;&amp;1&amp;3\\<br>    &amp;&amp;&amp;&amp;&amp;1&amp;3\\<br>    &amp;&amp;&amp;&amp;&amp;&amp;&amp;3\\<br>    &amp;&amp;&amp;&amp;&amp;&amp;&amp;1&amp;3\\<br>\end{bmatrix}$</p>
<p>初级因子有$(\lambda-2)^2,(\lambda-2),(\lambda-2),(\lambda-3)^3,(\lambda-3)^2$</p>
<p>所以$m(\lambda) = d_n(\lambda) = (\lambda-2)^2(\lambda-3)^3$</p>
<p>$d_{n-1}(\lambda) = (\lambda-2)(\lambda-3)^2$</p>
<p>$d_{n-2}(\lambda) = (\lambda-2)$</p>
<h3 id="矩阵函数-f-A-f-At"><a href="#矩阵函数-f-A-f-At" class="headerlink" title="*矩阵函数$f(A),f(At)$"></a>*矩阵函数$f(A),f(At)$</h3><ol>
<li>先求的矩阵的最小多项式。 例如：$(\lambda-1)(\lambda-2)^2$</li>
<li>根据最小多项式中$\lambda$的阶数$r$，假设$e^A = a_0E+a_1A+a_2A^2+…+a_{r-1}A^{r-1}$。 例如$e^A = a_0E+a_1A+a_2A^2$</li>
<li>将最小多项式中的$\lambda$带入式子中，对于阶数大于1的 对式子求导后在代入。</li>
</ol>
<p>例如：</p>
<p>$e^A = a_0E+a_1A+a_2A^2|_{A=2} \rightarrow e^2 = a_0+a_12+a_24$</p>
<p>$e^A = a_0E+a_1A+a_2A^2|_{A=1} \rightarrow e^1 = a_0+a_11+a_21$</p>
<p>$(e^A = a_0E+a_1A+a_2A^2)’|_{A=1} \rightarrow e=a_1+2a_2A|_{A=1} \rightarrow e=a_1+a_22$</p>
<ol>
<li>解出$a_i$。</li>
<li>带入$e^A = a_0E+a_1A+a_2A^2+…+a_{r-1}A^{r-1}$求出e^A。</li>
</ol>
<h3 id="QR分解"><a href="#QR分解" class="headerlink" title="*QR分解"></a>*QR分解</h3><p>$A \in C^{n \times n}$都能写成$A=QR$, $QQ^H=E_n$ $R$是上三角矩阵。</p>
<p>$A \in R^{n \times n}$都能写成$A=QR$, $QQ^T=E_n$ $R$是上三角矩阵。</p>
<p><strong>求$Q$</strong></p>
<ol>
<li>对$A$进行斯密特正交化，</li>
<li>将正交化后的$\beta_i$中的零向量补齐为正交向量。</li>
<li>进行单位化</li>
<li>$Q=(\beta_1,\beta_2,…,\beta_n)$</li>
</ol>
<p><strong>求$R$</strong></p>
<p>$A=QR \qquad R=Q^{-1}A=Q^TA$</p>
<h3 id="斯密特正交化"><a href="#斯密特正交化" class="headerlink" title="斯密特正交化"></a>斯密特正交化</h3><p>$\beta_1 = \alpha_1$</p>
<p>$\beta_2 = \alpha_2 - \frac{(\beta_1,\alpha_2)}{(\beta_1,\beta_1)} \beta_1$</p>
<p>$\beta_3 = \alpha_3 - \frac{(\beta_1,\alpha_3)}{(\beta_1,\beta_1)}\beta_1 - \frac{(\beta_2,\alpha_3)}{(\beta_2,\beta_2)}\beta_2$</p>
<h3 id="约当标准型"><a href="#约当标准型" class="headerlink" title="约当标准型"></a>约当标准型</h3><h4 id="k-级行列式因子："><a href="#k-级行列式因子：" class="headerlink" title="$k$级行列式因子："></a>$k$级行列式因子：</h4><p>$A(\lambda)$中所有非零的$k$级子式的首项（最高次项）系数为1的最大公因式$D_k(\lambda)$称为$A(\lambda)$的一个$k$级行列式因子。</p>
<h4 id="不变因子"><a href="#不变因子" class="headerlink" title="不变因子"></a>不变因子</h4><p>$d_1(\lambda) = D_1(\lambda) \quad d_2(\lambda)=\frac{D_2(\lambda)}{D_1(\lambda)} \quad d_2(\lambda)=\frac{D_3(\lambda)}{D_3(\lambda)} \quad… \quad d_n(\lambda)=\frac{D_n(\lambda)}{D_{n-1}(\lambda)}$</p>
<p>称为$A(\lambda)$的不变因子。</p>
<h4 id="初级因子"><a href="#初级因子" class="headerlink" title="初级因子"></a>初级因子</h4><p>把每个次数大于零的不变因子分解为互不相同的一次因式的方幂的乘积，所有这些一次方幂（相同的必须安出现次数计算）称为$A(\lambda)$的初级因子。</p>
<h4 id="约当块"><a href="#约当块" class="headerlink" title="约当块"></a>约当块</h4><p>设$A=(a_{ij})_{n \times n}$的全部初级因子为：</p>
<p>$(\lambda - \lambda_1)^{k_1} \quad (\lambda - \lambda_2)^{k_2} \quad … \quad (\lambda - \lambda_s)^{k_s}$</p>
<p>每个初级因子$(\lambda - \lambda_i)^{k_i}$构成一个$k_i$阶矩阵（<strong>约当块</strong>）</p>
<p>$J_i = \begin{bmatrix}<br>    \lambda_i&amp; &amp; &amp; \\1&amp;\lambda_i \\ &amp; … &amp; … \\ &amp; &amp; 1&amp; \lambda_i<br>\end{bmatrix} (i=1,2,…,s)$</p>
<h4 id="约当标准型-1"><a href="#约当标准型-1" class="headerlink" title="约当标准型"></a>约当标准型</h4><p>将这些约当块构成分块对角矩阵</p>
<p>$J = \begin{bmatrix}<br>    J_1\\<br>    &amp;J_2\\<br>    &amp;&amp;…\\<br>    &amp;&amp;&amp; J_3<br>\end{bmatrix}$</p>
<p>称为$A$的约当标准型。</p>
<h4 id="求约当标准型的简便方法"><a href="#求约当标准型的简便方法" class="headerlink" title="求约当标准型的简便方法"></a><font color="red">求约当标准型的简便方法</font></h4><ol>
<li>求出所有特征值$\lambda_i$</li>
<li>$\lambda_i$的约当块的个数$=n-rank(\lambda_iE - A)$</li>
</ol>
<h4 id="题型1-求不变因子"><a href="#题型1-求不变因子" class="headerlink" title="题型1:求不变因子"></a>题型1:求不变因子</h4><ol>
<li>使用初等变换将矩阵变为对角矩阵：适用于低阶数字矩阵。</li>
<li>更具定义求出行列式因子，进而求出不变因子：适用于n阶带有未知量的明显特征的矩阵。</li>
</ol>
<h3 id="斯密特标准型"><a href="#斯密特标准型" class="headerlink" title="斯密特标准型"></a>斯密特标准型</h3><p>将上面约当标准型中的不变因子$d_i(\lambda)$排列成对角矩阵就是smith标准型。</p>
<h3 id="圆盘定理"><a href="#圆盘定理" class="headerlink" title="*圆盘定理"></a>*圆盘定理</h3><p>$z \in C$</p>
<p>$|z -a -bi| \leq 2$的圆盘为以$(a,b)$为圆心2为半径的圆。</p>
<p>$A\in C^{n \times n}$,$A$的特征值在下面圆盘内：</p>
<p>$|z-a_{ii}| \leq R_i = \sum\limits_{j=1,j\neq i}^{n}|a_{ij}|$</p>
<p>上面的$||$表示模长，不是绝对值。</p>
<h3 id="普半径"><a href="#普半径" class="headerlink" title="*普半径"></a>*普半径</h3><p>最大特征值的模厂</p>
<h3 id="广义逆矩阵-A"><a href="#广义逆矩阵-A" class="headerlink" title="*广义逆矩阵$A^+$"></a>*广义逆矩阵$A^+$</h3><p>$A\in C^{m \times n}$</p>
<p>$A^+ = \begin{cases}<br>    A^H(AA^H)^{-1}&amp; &amp;当rank(A)=m\\\\<br>    (A^HA)^{-1}A^H&amp; &amp;当rank(A)=n<br>\end{cases}$</p>
<p>当$A \neq 0$时，$A$可逆，则$A^H = A^{-1}$</p>
<p>当行列都不满秩的时候将$A$分解成为$LR$然后分别对矩阵$L,R$求$L^+,R^+$ 最终$A^+=L^+R^+$</p>
]]></content>
      <categories>
        <category>课程</category>
      </categories>
  </entry>
  <entry>
    <title>Tree</title>
    <url>/2021/01/07/Tree/</url>
    <content><![CDATA[<h1 id="Tree"><a href="#Tree" class="headerlink" title="Tree"></a>Tree</h1><h5 id="Time-Limit-1000MS-Memory-Limit-30000K"><a href="#Time-Limit-1000MS-Memory-Limit-30000K" class="headerlink" title="Time Limit: 1000MS        Memory Limit: 30000K"></a>Time Limit: 1000MS        Memory Limit: 30000K</h5><hr>
<h2 id="Description"><a href="#Description" class="headerlink" title="Description"></a>Description</h2><p>Give a tree with n vertices,each edge has a length(positive integer less than 1001).</p>
<p>Define dist(u,v)=The min distance between node u and v.</p>
<p>Give an integer k,for every pair (u,v) of vertices is called valid if and only if dist(u,v) not exceed k.<br>Write a program that will count how many pairs which are valid for a given tree.</p>
<h2 id="Input"><a href="#Input" class="headerlink" title="Input"></a>Input</h2><p>The input contains several test cases. The first line of each test case contains two integers n, k. (n&lt;=10000) The following n-1 lines each contains three integers u,v,l, which means there is an edge between node u and v of length l.<br>The last test case is followed by two zeros.</p>
<h2 id="Output"><a href="#Output" class="headerlink" title="Output"></a>Output</h2><p>For each test case output the answer on a single line.</p>
<h2 id="Sample-Input"><a href="#Sample-Input" class="headerlink" title="Sample Input"></a>Sample Input</h2><pre><code>5 4
1 2 3
1 3 1
1 4 2
3 5 1
0 0
</code></pre><h2 id="Sample-Output"><a href="#Sample-Output" class="headerlink" title="Sample Output"></a>Sample Output</h2><pre><code>8
</code></pre><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>假定选择一点1为根，那其他点到根的最短距离就有两种情况。</p>
<p>其一，它们在根的不同分支上，那他们的最近距离就是它们到它们的最近公共祖先的距离和。</p>
<p>其二，如果它们在根的同一个分支上，那么最近距离就是它们之间的距离。</p>
<p>我们分析到这，先求出其他点到根1点的距离用一个dfs求出，放进一个数组里a[maxn]，对它排一个序，这里面任意两点只有以上两种情况，来自1同一个分支，或是不同分支，而我们求得方案数一定是来自不同分支。直接求不好求，我们可以先求出</p>
<p>A：a[i]+a[j] &lt;=k的方案数（不管i，j是否来自同一个分支）这种点对是对答案有贡献的。</p>
<p>B：a[i]+a[j]&lt;=k的方案数（i,j来自同一个分支）同一分支，则a[i]+a[j]不是i,j之间的真是距离。 </p>
<p>所以代码中是先把A,B的情况同时计算了，然后减去B中的方案数。</p>
<p>B怎么求呢？我们只要在求a数组的过程在A的前提下加一个边的权值，这样就相当于控制i,j两条边的方向就是来自于这一条边的方向。（这里看代码细细体会）</p>
<p>排序时间复杂度为$O(N\log(N))$递归深度为$\log(N)$，整体时间复杂度为$O(Nlog^2(N))$</p>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cstdio&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="meta">#<span class="keyword">define</span> inf 0x3f3f3f3f</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> ll long long</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> N 100010</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//输入增强，减少输入时间。</span></span><br><span class="line"><span class="function"><span class="keyword">inline</span> <span class="type">void</span> <span class="title">in</span><span class="params">(<span class="type">int</span> &amp;x)</span> </span>&#123;</span><br><span class="line">    x = <span class="number">0</span>; <span class="type">int</span> f = <span class="number">1</span>;</span><br><span class="line">    <span class="type">char</span> c = <span class="built_in">getchar</span>();</span><br><span class="line">    <span class="keyword">while</span> (c &lt; <span class="string">&#x27;0&#x27;</span> || c &gt; <span class="string">&#x27;9&#x27;</span>) &#123;</span><br><span class="line">        <span class="keyword">if</span> (c == <span class="string">&#x27;-&#x27;</span>) f = <span class="number">-1</span>;</span><br><span class="line">        c = <span class="built_in">getchar</span>();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">while</span> (c &gt;= <span class="string">&#x27;0&#x27;</span> &amp;&amp; c &lt;= <span class="string">&#x27;9&#x27;</span>) &#123;</span><br><span class="line">        x = x * <span class="number">10</span> + c - <span class="string">&#x27;0&#x27;</span>;</span><br><span class="line">        c = <span class="built_in">getchar</span>();</span><br><span class="line">    &#125;</span><br><span class="line">    x *= f;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> n, k, d[N], cnt, head[N], ans;</span><br><span class="line"><span class="type">int</span> vis[N], siz[N];</span><br><span class="line"><span class="comment">//孩子兄弟表示法</span></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">edge</span> &#123;</span><br><span class="line">	<span class="type">int</span> to, nxt, v;</span><br><span class="line">&#125;e[N&lt;&lt;<span class="number">1</span>];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">ins</span><span class="params">(<span class="type">int</span> u, <span class="type">int</span> v, <span class="type">int</span> w)</span> </span>&#123;</span><br><span class="line">	e[++cnt] = (edge) &#123;v, head[u], w&#125;;</span><br><span class="line">	head[u] = cnt;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> now_sz = inf, root = <span class="number">0</span>, sz;</span><br><span class="line"><span class="comment">//找到树的重心</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">find_root</span><span class="params">(<span class="type">int</span> u, <span class="type">int</span> fa)</span> </span>&#123;</span><br><span class="line">	siz[u] = <span class="number">1</span>;</span><br><span class="line">	<span class="type">int</span> res = <span class="number">0</span>;</span><br><span class="line">	<span class="keyword">for</span>(<span class="type">int</span> i = head[u]; i; i = e[i].nxt) &#123;</span><br><span class="line">		<span class="keyword">if</span>(vis[e[i].to] || e[i].to == fa) <span class="keyword">continue</span>;</span><br><span class="line">		<span class="type">int</span> v = e[i].to;</span><br><span class="line">		<span class="built_in">find_root</span>(v, u);	<span class="comment">//递归获取子树孩子结点数量</span></span><br><span class="line">		siz[u] += siz[v];</span><br><span class="line">		res = <span class="built_in">max</span>(res, siz[v]);	<span class="comment">//保存最大的子树结点数。</span></span><br><span class="line">	&#125;</span><br><span class="line">	res = <span class="built_in">max</span>(res, sz - siz[u]); <span class="comment">//看看除了这颗子树的结点数和这颗子树的结点数那个更多， 其实就是让以该点为重心的化，这颗树个个分支更加平均。</span></span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span>(res &lt; now_sz) now_sz = res, root = u;	<span class="comment">//如果以该点为重心能使的使得个分支更加平均，更新重心。</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//a保存每个结点到重心u的距离。</span></span><br><span class="line"><span class="type">int</span> a[N], tot;</span><br><span class="line"><span class="comment">//递归获取u的子结点到u的距离。</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">get_dis</span><span class="params">(<span class="type">int</span> u, <span class="type">int</span> fa)</span> </span>&#123;</span><br><span class="line">	a[++tot] = d[u]; <span class="comment">//将所有的距离保存</span></span><br><span class="line">	<span class="keyword">for</span>(<span class="type">int</span> i = head[u]; i; i = e[i].nxt) &#123;</span><br><span class="line">		<span class="keyword">if</span>(vis[e[i].to] || e[i].to == fa) <span class="keyword">continue</span>;</span><br><span class="line">		<span class="type">int</span> v = e[i].to;</span><br><span class="line">		d[v] = d[u] + e[i].v;</span><br><span class="line">		<span class="built_in">get_dis</span>(v, u);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//这里是计算所有子树中到u的距离和小于k的点对（i，j）的数量，这里同时统计了位于一个子树和不同子树的点对， 位于同一个子树的后面会减掉。</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">solve</span><span class="params">(<span class="type">int</span> u, <span class="type">int</span> dis)</span> </span>&#123;</span><br><span class="line">	d[u] = dis; tot = <span class="number">0</span>;</span><br><span class="line">	<span class="built_in">get_dis</span>(u, u);</span><br><span class="line">	<span class="built_in">sort</span>(a + <span class="number">1</span>, a + tot + <span class="number">1</span>); </span><br><span class="line">	<span class="type">int</span> l = <span class="number">1</span>, r = tot, res = <span class="number">0</span>;</span><br><span class="line">	<span class="keyword">for</span>(; l &lt; r; ++l) &#123;</span><br><span class="line">		<span class="keyword">while</span>(l &lt; r &amp;&amp; a[l] + a[r] &gt; k) --r;</span><br><span class="line">		<span class="keyword">if</span>(l &lt; r) res += r - l;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">dfs</span><span class="params">(<span class="type">int</span> u)</span> </span>&#123;</span><br><span class="line">	vis[u] = <span class="number">1</span>;</span><br><span class="line">	ans += <span class="built_in">solve</span>(u, <span class="number">0</span>); <span class="comment">//以0为重心计算点对数量</span></span><br><span class="line">	<span class="keyword">for</span>(<span class="type">int</span> i = head[u]; i; i = e[i].nxt) &#123;</span><br><span class="line">		<span class="keyword">if</span>(vis[e[i].to]) <span class="keyword">continue</span>;</span><br><span class="line">		<span class="type">int</span> v = e[i].to;</span><br><span class="line">		ans -= <span class="built_in">solve</span>(v, e[i].v);  <span class="comment">//这里就是减去同一子树中点对贡献的答案</span></span><br><span class="line">		now_sz = inf, root = <span class="number">0</span>; sz = siz[v];</span><br><span class="line">		<span class="built_in">find_root</span>(v, <span class="number">0</span>); <span class="comment">//分治处理子树</span></span><br><span class="line">		<span class="built_in">dfs</span>(root);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">	<span class="keyword">while</span>(~<span class="built_in">scanf</span>(<span class="string">&quot;%d%d&quot;</span>, &amp;n, &amp;k) &amp;&amp; n &amp;&amp; k) &#123;</span><br><span class="line">		ans = <span class="number">0</span>; cnt = <span class="number">0</span>; </span><br><span class="line">		<span class="built_in">memset</span>(head, <span class="number">0</span>, <span class="built_in">sizeof</span>(head));</span><br><span class="line">		<span class="built_in">memset</span>(vis, <span class="number">0</span>, <span class="built_in">sizeof</span>(vis));</span><br><span class="line">		<span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">1</span>; i &lt; n; ++i) &#123;</span><br><span class="line">			<span class="type">int</span> u, v, w; <span class="built_in">in</span>(u), <span class="built_in">in</span>(v), <span class="built_in">in</span>(w);</span><br><span class="line">			<span class="built_in">ins</span>(u, v, w), <span class="built_in">ins</span>(v, u, w); <span class="comment">//保存u-&gt;v,v-&gt;u两条边</span></span><br><span class="line">		&#125;</span><br><span class="line">		<span class="built_in">dfs</span>(<span class="number">1</span>);</span><br><span class="line">		<span class="built_in">printf</span>(<span class="string">&quot;%d\n&quot;</span>, ans);</span><br><span class="line">	&#125;</span><br><span class="line">&#125; </span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>acm</category>
      </categories>
      <tags>
        <tag>点分治</tag>
      </tags>
  </entry>
  <entry>
    <title>基本数学公式</title>
    <url>/2021/01/07/%E5%9F%BA%E6%9C%AC%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F/</url>
    <content><![CDATA[<h4 id="平方差公式"><a href="#平方差公式" class="headerlink" title="平方差公式"></a>平方差公式</h4><p>$a^2-b^2= (a+b)(a-b)$</p>
<h4 id="立方差公式"><a href="#立方差公式" class="headerlink" title="立方差公式"></a>立方差公式</h4><p>$a^3-b^3 = (a+b)(a^2-ab+b^2)$</p>
<h4 id="立方和公式"><a href="#立方和公式" class="headerlink" title="立方和公式"></a>立方和公式</h4><p>$a^3+b^3 = (a-b)(a^2+ab+b^2)$</p>
<h4 id="完全平方公式"><a href="#完全平方公式" class="headerlink" title="完全平方公式"></a>完全平方公式</h4><p>$(a + b)^2  = a^2 + 2ab+b^2$<br>$(a - b)^2  = a^2 - 2ab+b^2$</p>
<h4 id="一元二次方程求根公式"><a href="#一元二次方程求根公式" class="headerlink" title="一元二次方程求根公式"></a>一元二次方程求根公式</h4><p>$ax^2+bx+c =0$</p>
<p>$x_{1,2}=\frac{-b\pm\sqrt{b^2-4ac}}{2a}$</p>
<h4 id="韦达定理"><a href="#韦达定理" class="headerlink" title="韦达定理"></a>韦达定理</h4><p>设$x1,x2$是一元二次方程$ ax^2+bx+c =0$的两个根，则$x1,x2$满足:</p>
<p>$x1+x2 = -\frac{b}{a}$</p>
<p>$x1 \cdot x2 = \frac{c}{a}$</p>
<h3 id="有关集合的公式"><a href="#有关集合的公式" class="headerlink" title="有关集合的公式"></a>有关集合的公式</h3><p>设$I$为全集，$\varnothing$为空集，如果$A$是$I$的子集,$B$是$I$的子集，则$A \subset I $, $ B \subset I $。<br>于是，</p>
<p>$I \cup A = I$<br>$I \cap \varnothing = \varnothing$</p>
<p>若</p>
<p>$\bar A =\{x|x \in I且x \notin A,A \subseteq I \}$,<br>　$\bar B =\{x|x \in I且x \notin B,B \subseteq I \}$<br>则</p>
<p>$A \cup  \bar A = I$</p>
<p>$A \cap \bar A = \varnothing$</p>
<p>$\overline {A \cap B} = \bar A \cup \bar B$</p>
<p>$\overline {A \cup B} = \bar A \cap \bar B$</p>
<h3 id="不等式"><a href="#不等式" class="headerlink" title="不等式"></a>不等式</h3><p>$|a| \ge 0$</p>
<p>$|a|-|b| \le |a+b| \le |a|+|b|$</p>
<p>$|a| \le b$</p>
<p>$|a| \le b \Leftrightarrow -b \le a \le b　 (b&gt;0)$</p>
<p>$a^2+b^2 \gt 2ab　(a,b \in R)$</p>
<p>$\frac{a+b}{2}  \ge \sqrt{ab}　(a,b \in R^+)$</p>
<p>$\frac{b}{a} + \frac{a}{b} \ge 2　(ab&gt;0)$</p>
<p>$\frac{a+b+c}{3} \ge \sqrt[3]{abc}　(a,b,c \in R)$</p>
<p>$\frac{a_1+a_2+ \ldots +a_n}{n} \ge \sqrt[n]{a_1a_2\ldots a_n}　a_1,a_2, \ldots,a_n \in R^+ n \in N且n&gt;1$</p>
<h3 id="排列组合"><a href="#排列组合" class="headerlink" title="排列组合"></a>排列组合</h3><p>$A_n^m = \frac{n!}{(n-m)!}$</p>
<p>$C_n^m = \frac{n!}{m!(n-m)!}$</p>
<p>$C_n^m = C_n^(n-m)$</p>
<p>$C_{n+1}^m = C_n^m + C_n^{m-1}$</p>
<p>$C_n^0+C_n^1+\ldots +C_n^n = 2^n$</p>
<h3 id="积分公式"><a href="#积分公式" class="headerlink" title="积分公式"></a>积分公式</h3><p>$\int sin(x)dx = cos(x) +C$</p>
<p>$\int cos(x)dx = -sin(x) +C$</p>
<p>$\int tan(x)dx = -ln|cos(x)| +C$</p>
<p>$\int cot(s)dx = ln|sin(x)| +C$</p>
<p>$\int sec^2(x)dx = tan(x) +C$</p>
<h3 id="导数公式"><a href="#导数公式" class="headerlink" title="导数公式"></a>导数公式</h3><p>$(C)’=0$</p>
<p>$(sin \,x)’ = cos \,x$</p>
<p>$(tan \,x)’ = sec^2 \,x$</p>
<p>$(sec \,x)’ = sec \,xtan \,x$</p>
<p>$(a^x)’ = a^xln \,x$</p>
<p>$(x^{\mu}) = \mu x^{\mu-1}$</p>
<p>$(cos \,x)’ = -sin \,x$</p>
<p>$(cot \,x)’ = -csc^2 \,x$</p>
<p>$(csc \,x)’ = -csc \,xcot \,x$</p>
<p>$(e^x)’ = e^x$</p>
<p>$(log_ax)’ = \frac{1}{xln \,a}$</p>
<p>$(ln \,x)’ = \frac{1}{x}$</p>
<p>$(arcsin \,x)’ = \frac{1}{\sqrt{1-x^2}}$</p>
<p>$(arccos \,x)’ = -\frac{1}{\sqrt{1-x^2}}$</p>
<p>$(arctan \,x)’ = \frac{1}{1+x^2}$</p>
<p>$(arccot \,x)’ = -\frac{1}{1+x^2}$</p>
<h3 id="重要的极限"><a href="#重要的极限" class="headerlink" title="重要的极限"></a>重要的极限</h3><script type="math/tex; mode=display">\lim_{x\to 0} \,{\frac{\sin x}{x}} = 1</script><script type="math/tex; mode=display">\lim_{x\to \infty} \,{({1+ \frac{1}{x}})}^x =  e</script>]]></content>
      <categories>
        <category>数学</category>
      </categories>
  </entry>
  <entry>
    <title>计算时间复杂度T(N)</title>
    <url>/2021/01/10/%E8%AE%A1%E7%AE%97%E6%97%B6%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6T(N)/</url>
    <content><![CDATA[<p>对于$T(n) = a \times T(n/b)+c\times n^k$ 这种类型的。</p>
<p>$if(a&gt;b^k) \quad T(n)=O(n^{\log_ba})$</p>
<p>$if(a=b^k) \quad T(n)=O(n^{k \times \log n})$</p>
<p>$if(a&lt;b^k) \quad T(n)=O(n^k)$</p>
<p><strong>$T(n) = 3T(n/4)+n^3$</strong></p>
<p>$3&lt;4^2$ 所以 $T(n) = O(n^3)$</p>
<p><strong>$T(n)= T(\sqrt{n})+n$</strong></p>
<p>$T(n)=n+n^{\frac{1}{2}}+n^{\frac{1}{4}}+…..=n$</p>
]]></content>
      <categories>
        <category>acm</category>
      </categories>
  </entry>
  <entry>
    <title>解空间-搜索空间-目标函数-约束条件-限界函数</title>
    <url>/2021/01/11/%E8%A7%A3%E7%A9%BA%E9%97%B4-%E6%90%9C%E7%B4%A2%E7%A9%BA%E9%97%B4-%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0-%E7%BA%A6%E6%9D%9F%E6%9D%A1%E4%BB%B6-%E9%99%90%E7%95%8C%E5%87%BD%E6%95%B0/</url>
    <content><![CDATA[<p><strong>解空间</strong>：就是进行穷举的搜索空间，解空间中包含所有的可能解。</p>
<p><strong>搜索空间</strong>：搜索过程中涉及的结点叫做搜索空间，所以搜索空间树是解空间的一部分。在搜索过程中通过约束条件剪去无法得到可行解的子树，通过目标函数的界限剪去得不到最优解的子树。</p>
<p><strong>目标函数</strong>：表示我们程序搜索的最优目标。<em>比如用最少的机器人监视n</em>m的矩阵。 min(robot) c&gt;=n<em>m</em></p>
<p><strong>目标函数的界限</strong>：表示最差的解决方法的解。 <em>比如用n个木料肯定能切割出n个木板。</em></p>
<p><strong>约束条件</strong>：表示对搜索过程的限制。<em>比如八皇后的时候棋子不能放在同一行，同一列上。</em>  约束条件是为了去掉不可行解。</p>
<p><strong>限界函数</strong>：在搜索过程中对最终结果的一种预测，从而来判断是否需要继续生成。限界函数就是在找，最好可能是什么情况,不同点在于目标函数界限的是一般整个问题全局都是一样的，<br>但是限界函数是根据每个节点当前的情况进行计算的来的，每个节点的限界函数都可能不一样，也就是说每个节点可能达到的最优值都可能不一样，根据这个可能最优值就行排序(一般就是把这些节点放到一个排好序的数据结构里，比如说小根堆或者大根堆里)，谁可能最优值最好，就先遍历谁<br>比如说博物馆问题，比如现在使用了robot个机器人监视了c个方格。则最少需要robot+(m*n-c)/5个机器人才能完全监视，这就是这个节点可能最优的情况，根据这个节点进行排序，谁小先遍历谁</p>
]]></content>
      <categories>
        <category>acm</category>
      </categories>
  </entry>
  <entry>
    <title>Batch-Epoch</title>
    <url>/2021/01/25/Batch-Epoch/</url>
    <content><![CDATA[<h2 id="batch"><a href="#batch" class="headerlink" title="batch"></a>batch</h2><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p><strong>Batch：批处理，顾名思义就是对某对象进行批量的处理。——百度百科</strong></p>
<p>训练神经网络时，在数据集很大的情况下，不能一次性载入全部的数据进行训练，电脑会支撑不住，其次全样本训练对于非凸损失函数会出现局部最优，所以要将大的数据集分割进行分批处理。<br>batch_size就是每批处理的样本的个数。</p>
<p>备注：</p>
<p>对于凸函数而言，最优解一定是全局最优解；而对于非凸函数而言，最优解可能是局部最优解。</p>
<p>梯度下降法是最常见的优化算法，在最小化损失函数时，通过梯度下降法一步步的迭代求解，得到最小化的损失函数和模型参数值。</p>
<h3 id="如何设置batch-size"><a href="#如何设置batch-size" class="headerlink" title="如何设置batch_size?"></a>如何设置batch_size?</h3><p>过大的batch_size会降低梯度下降的随机性，以至于更稳定，训练更容易收敛，但因此更容易陷入局部最优点<br>较小的批次内存利用率极低，还会带来幅度和随机性较大的权重更新，比较容易震荡导致不稳定，但也因此能够跳出局部最小。</p>
<h2 id="epoch"><a href="#epoch" class="headerlink" title="epoch"></a>epoch</h2><h3 id="定义-1"><a href="#定义-1" class="headerlink" title="定义"></a>定义</h3><p>epoch是所有训练数据的训练次数。一个完整的数据集通过神经网络一次并返回了一次的过程(正向传递+反向传递)称为一个epoch。</p>
<p>备注：</p>
<p>在神经网络中传递完整的数据集一次是不够的，即使用一次迭代过程更新权重一次不够，epoch的数目需要&gt;1，epoch数量增加，神经网络中的权重更新次数增加，曲线从欠拟合变为过拟合。</p>
<h3 id="公式"><a href="#公式" class="headerlink" title="公式"></a>公式</h3><p>单次epoch =&gt;（全部训练样本/batch_size）= iteration<br>即迭代的次数等于batch的数目。</p>
<font color="red">我有1000个数据，batch_size设置为500，那么我需要2次iterations，完成1次epoch。</font>]]></content>
  </entry>
  <entry>
    <title>Torchtext:Iterator</title>
    <url>/2021/01/25/Torchtext-Iterator/</url>
    <content><![CDATA[<p>Iterator类是torchtext中用来对dataset进行处理产生batch使用的。 本来打算自己实现一个Iterator来实现。尝试了下发现行不通。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Iterator</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Defines an iterator that loads batches of data from a Dataset.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Attributes:</span></span><br><span class="line"><span class="string">        dataset: The Dataset object to load Examples from.</span></span><br><span class="line"><span class="string">        batch_size: Batch size.</span></span><br><span class="line"><span class="string">        batch_size_fn: Function of three arguments (new example to add, current</span></span><br><span class="line"><span class="string">            count of examples in the batch, and current effective batch size)</span></span><br><span class="line"><span class="string">            that returns the new effective batch size resulting from adding</span></span><br><span class="line"><span class="string">            that example to a batch. This is useful for dynamic batching, where</span></span><br><span class="line"><span class="string">            this function would add to the current effective batch size the</span></span><br><span class="line"><span class="string">            number of tokens in the new example.</span></span><br><span class="line"><span class="string">        sort_key: A key to use for sorting examples in order to batch together</span></span><br><span class="line"><span class="string">            examples with similar lengths and minimize padding. The sort_key</span></span><br><span class="line"><span class="string">            provided to the Iterator constructor overrides the sort_key</span></span><br><span class="line"><span class="string">            attribute of the Dataset, or defers to it if None.</span></span><br><span class="line"><span class="string">        train: Whether the iterator represents a train set.</span></span><br><span class="line"><span class="string">        repeat: Whether to repeat the iterator for multiple epochs. Default: False.</span></span><br><span class="line"><span class="string">        shuffle: Whether to shuffle examples between epochs.</span></span><br><span class="line"><span class="string">        sort: Whether to sort examples according to self.sort_key.</span></span><br><span class="line"><span class="string">            Note that shuffle and sort default to train and (not train).</span></span><br><span class="line"><span class="string">        sort_within_batch: Whether to sort (in descending order according to</span></span><br><span class="line"><span class="string">            self.sort_key) within each batch. If None, defaults to self.sort.</span></span><br><span class="line"><span class="string">            If self.sort is True and this is False, the batch is left in the</span></span><br><span class="line"><span class="string">            original (ascending) sorted order.</span></span><br><span class="line"><span class="string">        device (str or `torch.device`): A string or instance of `torch.device`</span></span><br><span class="line"><span class="string">            specifying which device the Variables are going to be created on.</span></span><br><span class="line"><span class="string">            If left as default, the tensors will be created on cpu. Default: None.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dataset, batch_size, sort_key=<span class="literal">None</span>, device=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 batch_size_fn=<span class="literal">None</span>, train=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">                 repeat=<span class="literal">False</span>, shuffle=<span class="literal">None</span>, sort=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 sort_within_batch=<span class="literal">None</span></span>):</span><br><span class="line">        warnings.warn(<span class="string">&#x27;&#123;&#125; class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.&#x27;</span>.<span class="built_in">format</span>(self.__class__.__name__), UserWarning)</span><br><span class="line">        self.batch_size, self.train, self.dataset = batch_size, train, dataset</span><br><span class="line">        self.batch_size_fn = batch_size_fn</span><br><span class="line">        self.iterations = <span class="number">0</span></span><br><span class="line">        self.repeat = repeat</span><br><span class="line">        self.shuffle = train <span class="keyword">if</span> shuffle <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">else</span> shuffle</span><br><span class="line">        self.sort = <span class="keyword">not</span> train <span class="keyword">if</span> sort <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">else</span> sort</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> sort_within_batch <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            self.sort_within_batch = self.sort</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.sort_within_batch = sort_within_batch</span><br><span class="line">        <span class="keyword">if</span> sort_key <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            self.sort_key = dataset.sort_key</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.sort_key = sort_key</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(device, <span class="built_in">int</span>):</span><br><span class="line">            logger.warning(<span class="string">&quot;The `device` argument should be set by using `torch.device`&quot;</span></span><br><span class="line">                           + <span class="string">&quot; or passing a string as an argument. This behavior will be&quot;</span></span><br><span class="line">                           + <span class="string">&quot; deprecated soon and currently defaults to cpu.&quot;</span>)</span><br><span class="line">            device = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> device <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            device = torch.device(<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">        <span class="keyword">elif</span> <span class="built_in">isinstance</span>(device, <span class="built_in">str</span>):</span><br><span class="line">            device = torch.device(device)</span><br><span class="line"></span><br><span class="line">        self.device = device</span><br><span class="line">        self.random_shuffler = RandomShuffler()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># For state loading/saving only</span></span><br><span class="line">        self._iterations_this_epoch = <span class="number">0</span></span><br><span class="line">        self._random_state_this_epoch = <span class="literal">None</span></span><br><span class="line">        self._restored_from_state = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">splits</span>(<span class="params">cls, datasets, batch_sizes=<span class="literal">None</span>, **kwargs</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Create Iterator objects for multiple splits of a dataset.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Arguments:</span></span><br><span class="line"><span class="string">            datasets: Tuple of Dataset objects corresponding to the splits. The</span></span><br><span class="line"><span class="string">                first such object should be the train set.</span></span><br><span class="line"><span class="string">            batch_sizes: Tuple of batch sizes to use for the different splits,</span></span><br><span class="line"><span class="string">                or None to use the same batch_size for all splits.</span></span><br><span class="line"><span class="string">            Remaining keyword arguments: Passed to the constructor of the</span></span><br><span class="line"><span class="string">                iterator class being used.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> batch_sizes <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            batch_sizes = [kwargs.pop(<span class="string">&#x27;batch_size&#x27;</span>)] * <span class="built_in">len</span>(datasets)</span><br><span class="line">        ret = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(datasets)):</span><br><span class="line">            train = i == <span class="number">0</span></span><br><span class="line">            ret.append(cls(</span><br><span class="line">                datasets[i], batch_size=batch_sizes[i], train=train, **kwargs))</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">tuple</span>(ret)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">data</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Return the examples in the dataset in order, sorted, or shuffled.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> self.sort:</span><br><span class="line">            xs = <span class="built_in">sorted</span>(self.dataset, key=self.sort_key)</span><br><span class="line">        <span class="keyword">elif</span> self.shuffle:</span><br><span class="line">            xs = [self.dataset[i] <span class="keyword">for</span> i <span class="keyword">in</span> self.random_shuffler(<span class="built_in">range</span>(<span class="built_in">len</span>(self.dataset)))]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            xs = self.dataset</span><br><span class="line">        <span class="keyword">return</span> xs</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_epoch</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Set up the batch generator for a new epoch.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self._restored_from_state:</span><br><span class="line">            self.random_shuffler.random_state = self._random_state_this_epoch</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self._random_state_this_epoch = self.random_shuffler.random_state</span><br><span class="line"></span><br><span class="line">        self.create_batches()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self._restored_from_state:</span><br><span class="line">            self._restored_from_state = <span class="literal">False</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self._iterations_this_epoch = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.repeat:</span><br><span class="line">            self.iterations = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">create_batches</span>(<span class="params">self</span>):</span><br><span class="line">        self.batches = batch(self.data(), self.batch_size, self.batch_size_fn)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">epoch</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> math.floor(self.iterations / <span class="built_in">len</span>(self))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">if</span> self.batch_size_fn <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">raise</span> NotImplementedError</span><br><span class="line">        <span class="keyword">return</span> math.ceil(<span class="built_in">len</span>(self.dataset) / self.batch_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__iter__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            self.init_epoch()</span><br><span class="line">            <span class="keyword">for</span> idx, minibatch <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.batches):</span><br><span class="line">                <span class="comment"># fast-forward if loaded from state</span></span><br><span class="line">                <span class="keyword">if</span> self._iterations_this_epoch &gt; idx:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                self.iterations += <span class="number">1</span></span><br><span class="line">                self._iterations_this_epoch += <span class="number">1</span></span><br><span class="line">                <span class="keyword">if</span> self.sort_within_batch:</span><br><span class="line">                    <span class="comment"># <span class="doctag">NOTE:</span> `rnn.pack_padded_sequence` requires that a minibatch</span></span><br><span class="line">                    <span class="comment"># be sorted by decreasing order, which requires reversing</span></span><br><span class="line">                    <span class="comment"># relative to typical sort keys</span></span><br><span class="line">                    <span class="keyword">if</span> self.sort:</span><br><span class="line">                        minibatch.reverse()</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        minibatch.sort(key=self.sort_key, reverse=<span class="literal">True</span>)</span><br><span class="line">                <span class="keyword">yield</span> Batch(minibatch, self.dataset, self.device)</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> self.repeat:</span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">state_dict</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> &#123;</span><br><span class="line">            <span class="string">&quot;iterations&quot;</span>: self.iterations,</span><br><span class="line">            <span class="string">&quot;iterations_this_epoch&quot;</span>: self._iterations_this_epoch,</span><br><span class="line">            <span class="string">&quot;random_state_this_epoch&quot;</span>: self._random_state_this_epoch&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">load_state_dict</span>(<span class="params">self, state_dict</span>):</span><br><span class="line">        self.iterations = state_dict[<span class="string">&quot;iterations&quot;</span>]</span><br><span class="line">        self._iterations_this_epoch = state_dict[<span class="string">&quot;iterations_this_epoch&quot;</span>]</span><br><span class="line">        self._random_state_this_epoch = state_dict[<span class="string">&quot;random_state_this_epoch&quot;</span>]</span><br><span class="line">        self._restored_from_state = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BPTTIterator</span>(<span class="title class_ inherited__">Iterator</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Defines an iterator for language modeling tasks that use BPTT.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Provides contiguous streams of examples together with targets that are</span></span><br><span class="line"><span class="string">    one timestep further forward, for language modeling training with</span></span><br><span class="line"><span class="string">    backpropagation through time (BPTT). Expects a Dataset with a single</span></span><br><span class="line"><span class="string">    example and a single field called &#x27;text&#x27; and produces Batches with text and</span></span><br><span class="line"><span class="string">    target attributes.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Attributes:</span></span><br><span class="line"><span class="string">        dataset: The Dataset object to load Examples from.</span></span><br><span class="line"><span class="string">        batch_size: Batch size.</span></span><br><span class="line"><span class="string">        bptt_len: Length of sequences for backpropagation through time.</span></span><br><span class="line"><span class="string">        sort_key: A key to use for sorting examples in order to batch together</span></span><br><span class="line"><span class="string">            examples with similar lengths and minimize padding. The sort_key</span></span><br><span class="line"><span class="string">            provided to the Iterator constructor overrides the sort_key</span></span><br><span class="line"><span class="string">            attribute of the Dataset, or defers to it if None.</span></span><br><span class="line"><span class="string">        train: Whether the iterator represents a train set.</span></span><br><span class="line"><span class="string">        repeat: Whether to repeat the iterator for multiple epochs. Default: False.</span></span><br><span class="line"><span class="string">        shuffle: Whether to shuffle examples between epochs.</span></span><br><span class="line"><span class="string">        sort: Whether to sort examples according to self.sort_key.</span></span><br><span class="line"><span class="string">            Note that shuffle and sort default to train and (not train).</span></span><br><span class="line"><span class="string">        device (str or torch.device): A string or instance of `torch.device`</span></span><br><span class="line"><span class="string">            specifying which device the Variables are going to be created on.</span></span><br><span class="line"><span class="string">            If left as default, the tensors will be created on cpu. Default: None.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dataset, batch_size, bptt_len, **kwargs</span>):</span><br><span class="line">        self.bptt_len = bptt_len</span><br><span class="line">        <span class="built_in">super</span>(BPTTIterator, self).__init__(dataset, batch_size, **kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> math.ceil((<span class="built_in">len</span>(self.dataset[<span class="number">0</span>].text) / self.batch_size - <span class="number">1</span>)</span><br><span class="line">                         / self.bptt_len)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__iter__</span>(<span class="params">self</span>):</span><br><span class="line">        text = self.dataset[<span class="number">0</span>].text</span><br><span class="line">        TEXT = self.dataset.fields[<span class="string">&#x27;text&#x27;</span>]</span><br><span class="line">        TEXT.eos_token = <span class="literal">None</span></span><br><span class="line">        text = text + ([TEXT.pad_token] * <span class="built_in">int</span>(math.ceil(<span class="built_in">len</span>(text) / self.batch_size)</span><br><span class="line">                                              * self.batch_size - <span class="built_in">len</span>(text)))</span><br><span class="line">        data = TEXT.numericalize(</span><br><span class="line">            [text], device=self.device)</span><br><span class="line">        data = data.view(self.batch_size, -<span class="number">1</span>).t().contiguous()</span><br><span class="line">        dataset = Dataset(examples=self.dataset.examples, fields=[</span><br><span class="line">            (<span class="string">&#x27;text&#x27;</span>, TEXT), (<span class="string">&#x27;target&#x27;</span>, TEXT)])</span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(self) * self.bptt_len, self.bptt_len):</span><br><span class="line">                self.iterations += <span class="number">1</span></span><br><span class="line">                seq_len = <span class="built_in">min</span>(self.bptt_len, <span class="built_in">len</span>(data) - i - <span class="number">1</span>)</span><br><span class="line">                batch_text = data[i:i + seq_len]</span><br><span class="line">                batch_target = data[i + <span class="number">1</span>:i + <span class="number">1</span> + seq_len]</span><br><span class="line">                <span class="keyword">if</span> TEXT.batch_first:</span><br><span class="line">                    batch_text = batch_text.t().contiguous()</span><br><span class="line">                    batch_target = batch_target.t().contiguous()</span><br><span class="line">                <span class="keyword">yield</span> Batch.fromvars(</span><br><span class="line">                    dataset, self.batch_size,</span><br><span class="line">                    text=batch_text,</span><br><span class="line">                    target=batch_target)</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> self.repeat:</span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BucketIterator</span>(<span class="title class_ inherited__">Iterator</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Defines an iterator that batches examples of similar lengths together.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Minimizes amount of padding needed while producing freshly shuffled</span></span><br><span class="line"><span class="string">    batches for each new epoch. See pool for the bucketing procedure used.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">create_batches</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">if</span> self.sort:</span><br><span class="line">            self.batches = batch(self.data(), self.batch_size,</span><br><span class="line">                                 self.batch_size_fn)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.batches = pool(self.data(), self.batch_size,</span><br><span class="line">                                self.sort_key, self.batch_size_fn,</span><br><span class="line">                                random_shuffler=self.random_shuffler,</span><br><span class="line">                                shuffle=self.shuffle,</span><br><span class="line">                                sort_within_batch=self.sort_within_batch)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">batch</span>(<span class="params">data, batch_size, batch_size_fn=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Yield elements from data in chunks of batch_size.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> batch_size_fn <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">batch_size_fn</span>(<span class="params">new, count, sofar</span>):</span><br><span class="line">            <span class="keyword">return</span> count</span><br><span class="line">    minibatch, size_so_far = [], <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> ex <span class="keyword">in</span> data:</span><br><span class="line">        minibatch.append(ex)</span><br><span class="line">        size_so_far = batch_size_fn(ex, <span class="built_in">len</span>(minibatch), size_so_far)</span><br><span class="line">        <span class="keyword">if</span> size_so_far == batch_size:</span><br><span class="line">            <span class="keyword">yield</span> minibatch</span><br><span class="line">            minibatch, size_so_far = [], <span class="number">0</span></span><br><span class="line">        <span class="keyword">elif</span> size_so_far &gt; batch_size:</span><br><span class="line">            <span class="keyword">yield</span> minibatch[:-<span class="number">1</span>]</span><br><span class="line">            minibatch, size_so_far = minibatch[-<span class="number">1</span>:], batch_size_fn(ex, <span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">if</span> minibatch:</span><br><span class="line">        <span class="keyword">yield</span> minibatch</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pool</span>(<span class="params">data, batch_size, key, batch_size_fn=<span class="keyword">lambda</span> new, count, sofar: count,</span></span><br><span class="line"><span class="params">         random_shuffler=<span class="literal">None</span>, shuffle=<span class="literal">False</span>, sort_within_batch=<span class="literal">False</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Sort within buckets, then batch, then shuffle batches.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Partitions data into chunks of size 100*batch_size, sorts examples within</span></span><br><span class="line"><span class="string">    each chunk using sort_key, then batch these examples and shuffle the</span></span><br><span class="line"><span class="string">    batches.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> random_shuffler <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        random_shuffler = random.shuffle</span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> batch(data, batch_size * <span class="number">100</span>, batch_size_fn):</span><br><span class="line">        p_batch = batch(<span class="built_in">sorted</span>(p, key=key), batch_size, batch_size_fn) \</span><br><span class="line">            <span class="keyword">if</span> sort_within_batch \</span><br><span class="line">            <span class="keyword">else</span> batch(p, batch_size, batch_size_fn)</span><br><span class="line">        <span class="keyword">if</span> shuffle:</span><br><span class="line">            <span class="keyword">for</span> b <span class="keyword">in</span> random_shuffler(<span class="built_in">list</span>(p_batch)):</span><br><span class="line">                <span class="keyword">yield</span> b</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">for</span> b <span class="keyword">in</span> <span class="built_in">list</span>(p_batch):</span><br><span class="line">                <span class="keyword">yield</span> b</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      <tags>
        <tag>torchtext</tag>
      </tags>
  </entry>
  <entry>
    <title>Torchtext学习</title>
    <url>/2021/01/25/Torchtext%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<h1 id="Torchtext学习"><a href="#Torchtext学习" class="headerlink" title="Torchtext学习"></a>Torchtext学习</h1><hr>
<h2 id="Torchtext的作用"><a href="#Torchtext的作用" class="headerlink" title="Torchtext的作用"></a>Torchtext的作用</h2><p>在进行模型训练时，我们需要对数据进行处理将其处理为batch然后传递给model进行训练。 torchtext就是这样一个帮助我们快速处理数据的工具。</p>
<h2 id="NLP数据预处理"><a href="#NLP数据预处理" class="headerlink" title="NLP数据预处理"></a>NLP数据预处理</h2><p>自然语言处理的数据预处理过程主要包括如下步骤：</p>
<ol>
<li>文本数据集的划分（训练集、验证集和测试集）；</li>
<li>文本数据的导入；</li>
<li>分词；</li>
<li>词汇表的构建；</li>
<li>文本数据对于词汇表的编码和映射；</li>
<li>词向量的生成；</li>
<li>批文本数据的生成。</li>
</ol>
<p>torchtext是一个高效、有力的文本预处理库（其对NLP的作用类似于torchvision之于CV），提供了涵盖上述诸步骤的一站式文本预处理功能。其预处理结果不仅可用于pytorch，也可用于其他框架的使用。</p>
<p><img src="/2021/01/25/Torchtext%E5%AD%A6%E4%B9%A0/process.png" width="90%"></p>
<p>torchtext的整体处理流程如上图所示，其主要的类对象包括：</p>
<ol>
<li>DataSet， 作用类似于torch.utils.data.DataSet，提供了完整的数据载入和切片功能；</li>
<li>Example，为DataSet的一个文本数据，其可有text和对应的label构成；</li>
<li>Field，定义了文本处理的配置信息；</li>
<li>Vocab，记录了文本中涉及的词汇表单</li>
<li>Vector，词向量</li>
<li>Iterator，作用类似于torch.unitls.data.DataLoader，为一个迭代器，每次迭代记录了一个batch的文本数据</li>
<li>Pipeline，将各子功能模块进行连接的管道工具。</li>
</ol>
<p>用torchtext来处理文本的的一般流程归纳如下：</p>
<ol>
<li>定义Field对象，配置文本处理（包括text和label）的分词、停用词、文本长度、填充词、未知词等一系列工具；</li>
<li>定义DataSets对象，加载原始语料（字符串数据），利用参数fields设置相关预处理工具，并利用splits实现各数据集的划分，划分结果中为一个个的Example对象；</li>
<li>利用Field.build_vocab创建词汇表对象Vocab；</li>
<li>定义Iteator对象，实现对DataSets对象的封装，返回可迭代的批量化文本向量数据，供下游NLP任务使用。</li>
</ol>
<p>下面通过一个例子，来简单实践下上述的基本流程：</p>
<p><strong>步骤一：准备好合适的原始语料</strong></p>
<p>这里通过将文本存为csv格式，方便后续的处理。csv主要包括两个字段，texts为文本，label为文本分类。对于不同的NLP任务，需要定义不同的字段和原始语料格式。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">raw_corpus</span>():</span><br><span class="line">	<span class="comment"># 进行一些语料的清洗和预处理工作</span></span><br><span class="line">	corpus = pd.DataFrame(&#123;<span class="string">&#x27;text&#x27;</span>:texts, <span class="string">&#x27;label&#x27;</span>:label&#125;)</span><br><span class="line">	corpus.to_csv(<span class="string">&#x27;corpus.csv&#x27;</span>, index=<span class="literal">False</span>, sep=<span class="string">&#x27;\t&#x27;</span>)   <span class="comment"># 注意这里的sep符号，后面要用到</span></span><br></pre></td></tr></table></figure>
<p><strong>步骤二：定义分词工具、停用词等，然后配置Field对象，进行数据集的划分和词汇表生成</strong><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.text <span class="keyword">import</span> data</span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义分词工具</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">tokenizer</span>(<span class="params">text</span>):    </span><br><span class="line">    <span class="keyword">return</span> [wd <span class="keyword">for</span> wd <span class="keyword">in</span> jieba.cut(text, cut_all=<span class="literal">False</span>)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义停用词表</span></span><br><span class="line">stopwords = <span class="built_in">open</span>(<span class="string">&#x27;stopwords&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>).read().strip().split(<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#　定义text和label的Field对象，相关参数说明见下</span></span><br><span class="line">TEXT = data.Field(sequential=<span class="literal">True</span>, use_vocab=<span class="literal">True</span>, fix_length=<span class="number">20</span>, tokenize=tokenizer, stop_words=stopwords, batch_first=<span class="literal">True</span>)</span><br><span class="line">LABEL = data.Field(sequential=<span class="literal">False</span>, use_vocab=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#　划分数据集，相关参数说明见下</span></span><br><span class="line">train, val = data.TabularDataset.splits(</span><br><span class="line">path=<span class="string">&#x27;./data&#x27;</span>, train=<span class="string">&#x27;train_corpus.csv&#x27;</span>,validation=<span class="string">&#x27;valid_corpus.csv&#x27;</span>,<span class="built_in">format</span>=<span class="string">&#x27;csv&#x27;</span>, </span><br><span class="line">skip_header=<span class="literal">True</span>, csv_reader_params=&#123;<span class="string">&#x27;delimiter&#x27;</span>: <span class="string">&#x27;\t&#x27;</span>&#125;,fields =[(<span class="string">&#x27;text&#x27;</span>,TEXT),(<span class="string">&#x27;label&#x27;</span>,LABEL)] )</span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据训练数据，生成词汇表</span></span><br><span class="line">TEXT.build_vocab(train)</span><br></pre></td></tr></table></figure></p>
<p>上面用到了两个重要对象Field和TabularDataset，Field对象前文已提及用于定义文本预处理的配置，可将语料一站式生成张量；而TabularDataset是前文DataSet对象的子类，专门用于处理表格类型原始语料（如json、csv和tsv）的加载。下面分别进行介绍：</p>
<ol>
<li>Field对象，</li>
</ol>
<p><strong>其主要参数配置包括：</strong></p>
<ul>
<li>sequential：是否将文本数据表示为序列，默认True；对于分类label，需设置为False；</li>
<li>use_vocab：是否基于文本数据创建词汇表，默认True；对于分类label，需设置为False；</li>
<li>init_token：每段文本的起始字符<SOS>，默认None，即不设置；</SOS></li>
<li>eos_token：每段文本的终止字符<EOS>，默认None，即不设置；</EOS></li>
<li>fix_length：是否固定文本长度，默认None；若设为某个int，则对长于该参数的文本进行句末截断操作，对短于该参数的文本进行pad操作，pad的方向和字符参照参数pad_first和pad_token；</li>
<li>pad_first：从头开始pad，默认False</li>
<li>pad_token：pad所采用的字符，默认值为”<pad>“</pad></li>
<li>unk_token：未知分词（适用于验证集和测试集）所采用的字符，默认值为”<unk>“</unk></li>
<li>tokenizer：分词函数对象，默认值str.split</li>
<li>include_lengths：是否返回每段文本的长度值，默认值False</li>
<li>lower: 是否需要把数据转化为小写，适用于英文的NLP任务，默认值False</li>
<li>batch_first：控制返回的文本张量中batch的维度，默认Fasle，需结合下游网络进行配置，也可不用设置该参数，在下游任务中通过张量的维度转换实现</li>
<li>tensor_type：定义文本数据最终返回的tensor类型 默认值torch.LongTensor</li>
<li>preprocessing：在分词之后和数值化之前使用的管道处理对象，默认值None</li>
<li>postprocessing: 数值化之后和转化成tensor之前使用的管道处理对象，默认值None</li>
</ul>
<p><strong>Field对象可实现如下重要方法：</strong></p>
<ul>
<li>pad(minibatch)：将batch中的文本数据进行填充对齐；</li>
<li>build_vocab(): 建立词典</li>
<li>numericalize(): 把文本数据数值化，返回tensor</li>
<li></li>
</ul>
<ol>
<li>TabularDataset对象，其利用splits函数进行语料的划分和Field处理，主要参数配置包括：</li>
</ol>
<ul>
<li>path: 语料路径文件夹</li>
<li>train: 训练数据文件名</li>
<li>validation: 验证数据文件名</li>
<li>test: 测试数据文件名</li>
<li>fields: 对于label和text等配置的Field对象，是一个列表对象，列表对象中每一个为（别名，Field对象）元组，其中别名在迭代器的迭代中会用到</li>
</ul>
<p><strong>步骤三：创建数据迭代器Iterator，并通过splits函数进行划分</strong><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_iter, val_iter = data.Iterator.splits((train, val), batch_sizes=(<span class="number">10</span>, <span class="number">5</span>), device=<span class="string">&quot;cuda&quot;</span>, </span><br><span class="line">sort_key=<span class="keyword">lambda</span> x:<span class="built_in">len</span>(x.text),sort_within_batch=<span class="literal">False</span>, repeat=<span class="literal">False</span>)  </span><br></pre></td></tr></table></figure></p>
<p>其中的重要参数设置包括：</p>
<ul>
<li>datasets: 即为上述各Dataset对象构成的元组；</li>
<li>batch_sizes：各Dataset对象对象的batch尺寸元组；</li>
<li>device：将数据移入cpu或gpu硬件中；</li>
<li>sort_key：将数据排序的参照函数，以方便pad操作</li>
<li>sort_within_batch：各batch是否需要排序；</li>
<li>repeat：是否需要重复多个epoch</li>
</ul>
<p><strong>步骤四：将各batch数据导入下游任务中</strong><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">	<span class="keyword">for</span> batch <span class="keyword">in</span> train_iter:</span><br><span class="line">		feature, target = batch.text, batch.label  <span class="comment"># 这里的text和label即为TabularDataset.splits中feild参数里的别名</span></span><br><span class="line">		<span class="comment">#　后续任务</span></span><br></pre></td></tr></table></figure></p>
<h2 id="Field-Vocab-Vectors"><a href="#Field-Vocab-Vectors" class="headerlink" title="Field, Vocab, Vectors"></a>Field, Vocab, Vectors</h2><p>为什么使用 Field 抽象：</p>
<ol>
<li><p>torchtext 认为一个样本是由多个字段（文本字段，标签字段）组成，不同的字段可能会有不同的处理方式，所以才会有 Field 抽象。</p>
</li>
<li><p>Field: 定义对应字段的处理操作<br>Vocab: 定义了 词汇表<br>Vectors: 用来保存预训练好的 word vectors<br>所以，</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">TEXT.build_vocab(train, vectors=<span class="string">&quot;glove.6B.100d&quot;</span>)</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>的解释为： 从预训练的 vectors 中，将当前 corpus 词汇表的词向量抽取出来，构成当前 corpus 的 Vocab（词汇表）。</p>
]]></content>
      <tags>
        <tag>torchtext</tag>
      </tags>
  </entry>
  <entry>
    <title>PCA降维</title>
    <url>/2021/01/26/PCA%E9%99%8D%E7%BB%B4/</url>
    <content><![CDATA[<h1 id="PCA降维"><a href="#PCA降维" class="headerlink" title="PCA降维"></a>PCA降维</h1><h5 id="参考视频：https-www-bilibili-com-video-BV1vW411S7tH"><a href="#参考视频：https-www-bilibili-com-video-BV1vW411S7tH" class="headerlink" title="参考视频：https://www.bilibili.com/video/BV1vW411S7tH"></a>参考视频：<a href="https://www.bilibili.com/video/BV1vW411S7tH">https://www.bilibili.com/video/BV1vW411S7tH</a></h5><h5 id="PDF笔记来源：https-github-com-ws13685555932-machine-learning-derivation"><a href="#PDF笔记来源：https-github-com-ws13685555932-machine-learning-derivation" class="headerlink" title="PDF笔记来源：https://github.com/ws13685555932/machine_learning_derivation"></a>PDF笔记来源：<a href="https://github.com/ws13685555932/machine_learning_derivation">https://github.com/ws13685555932/machine_learning_derivation</a></h5><h2 id="维度灾难"><a href="#维度灾难" class="headerlink" title="维度灾难"></a>维度灾难</h2><p><strong>当样本的维度增加时，Cover整个样本空间所需要的样本数量将会成指数级增加。</strong></p>
<p>当维度过大时就会导致数据稀疏和数据分布不均匀。                            </p>


	<div class="row">
    <embed src="../../../../file/05降维.pdf" width="100%" height="550" type="application/pdf">
	</div>


]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>降维</tag>
      </tags>
  </entry>
  <entry>
    <title>SVM支持向量机</title>
    <url>/2021/01/27/SVM%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/</url>
    <content><![CDATA[<h1 id="SVM支持向量机"><a href="#SVM支持向量机" class="headerlink" title="SVM支持向量机"></a>SVM支持向量机</h1><h5 id="参考视频：https-www-bilibili-com-video-BV1Hs411w7ci"><a href="#参考视频：https-www-bilibili-com-video-BV1Hs411w7ci" class="headerlink" title="参考视频：https://www.bilibili.com/video/BV1Hs411w7ci"></a>参考视频：<a href="https://www.bilibili.com/video/BV1Hs411w7ci">https://www.bilibili.com/video/BV1Hs411w7ci</a></h5><h5 id="PDF笔记来源：https-github-com-ws13685555932-machine-learning-derivation"><a href="#PDF笔记来源：https-github-com-ws13685555932-machine-learning-derivation" class="headerlink" title="PDF笔记来源：https://github.com/ws13685555932/machine_learning_derivation"></a>PDF笔记来源：<a href="https://github.com/ws13685555932/machine_learning_derivation">https://github.com/ws13685555932/machine_learning_derivation</a></h5><h5 id="讲解：https-github-com-NLP-LOVE-ML-NLP-blob-master-Machine-20Learning-4-20SVM-4-20SVM-md"><a href="#讲解：https-github-com-NLP-LOVE-ML-NLP-blob-master-Machine-20Learning-4-20SVM-4-20SVM-md" class="headerlink" title="讲解：https://github.com/NLP-LOVE/ML-NLP/blob/master/Machine%20Learning/4.%20SVM/4.%20SVM.md"></a>讲解：<a href="https://github.com/NLP-LOVE/ML-NLP/blob/master/Machine%20Learning/4.%20SVM/4.%20SVM.md">https://github.com/NLP-LOVE/ML-NLP/blob/master/Machine%20Learning/4.%20SVM/4.%20SVM.md</a></h5><p>支持向量机就是寻找一个超平面使得到这个超平面最近的点距离最大。</p>
<p>本片涉及的知识点有：</p>
<ol>
<li>SVM有三宝：间隔，对偶，核技巧</li>
<li>硬间隔SVM</li>
<li>KKT条件</li>
<li>约束优化问题</li>
<li>对偶关系的几何解释</li>
<li>松弛条件</li>
<li>KKT条件</li>
</ol>


	<div class="row">
    <embed src="../../../../file/06支持向量机.pdf" width="100%" height="550" type="application/pdf">
	</div>


]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>二分类</tag>
      </tags>
  </entry>
  <entry>
    <title>KL散度</title>
    <url>/2021/01/30/KL%E6%95%A3%E5%BA%A6/</url>
    <content><![CDATA[<h1 id="KL散度（KL-divergence）"><a href="#KL散度（KL-divergence）" class="headerlink" title="KL散度（KL divergence）"></a>KL散度（KL divergence）</h1><p>全称：Kullback-Leibler Divergence。</p>
<p>用途：比较两个概率分布的接近程度。<br>在统计应用中，我们经常需要用一个简单的，近似的概率分布 f * 来描述。</p>
<p>观察数据 D 或者另一个复杂的概率分布 f 。这个时候，我们需要一个量来衡量我们选择的近似分布 f * 相比原分布 f 究竟损失了多少信息量，这就是KL散度起作用的地方。</p>
<h2 id="信息量"><a href="#信息量" class="headerlink" title="信息量"></a>信息量</h2><p>一个事件x的信息量是：</p>
<p>$I ( x ) = − \log ( p ( x ) )$</p>
<p>解读：如果一个事件发生的概率越大，那么信息量就越小。如果是1，也就是100%发生，那么信息量为0。</p>
<h2 id="熵"><a href="#熵" class="headerlink" title="熵"></a>熵</h2><p>就是对信息量求期望值。</p>
<p>$H(X)=E[I(x)]=-\sum\limits_{x∈X}p(x)\log p(x)$</p>
<p>举例： 如果10次考试9次不及格，一次及格。 假设事件为$x_A$代表及格事件，那么这个事件的熵为：</p>
<p>$H_A(x)=-[p(x_A)\log(p(x_A))+(1-p(x_A))\log(1-p(x_A))]=0.4690$</p>
<p>其实也和后续的逻辑回归的二分类的损失函数有类似。</p>
<h2 id="KL散度"><a href="#KL散度" class="headerlink" title="KL散度"></a>KL散度</h2><p>现在，我们能够量化数据中的信息量了，就可以来衡量近似分布带来的信息损失了。</p>
<p>KL散度的计算公式其实是熵计算公式的简单变形,在原有概率分布 $p$ 上，加入我们的近似概率分布 $q$ ，计算他们的每个取值对应对数的差：</p>
<p>$D_KL(p||q) = \sum\limits_{i=1}^Np(x_i) \cdot (\log p(x_i)-\log q(x_i))$</p>
<p>换句话说，KL散度计算的就是数据的原分布与近似分布的概率的对数差的期望值。</p>
<p>在对数以2为底时， $log_2$ ，可以理解为“我们损失了多少位的信息”。</p>
<p>写成期望形式：</p>
<p>$D_KL(p||q) = E[\log p(x)-\log q(x)]$</p>
<p>$D_KL(p||q) = \sum\limits_{i=1}^Np(x_i) \cdot \log \frac{p(x_i)}{q(x_i)}$</p>
<p>现在，我们就可以使用KL散度衡量我们选择的近似分布与数据原分布有多大差异了。</p>
<h2 id="散度不是距离"><a href="#散度不是距离" class="headerlink" title="散度不是距离"></a>散度不是距离</h2><p>$D_KL(p||q) \neq D_KL(q||p)$</p>
<p>因为KL散度不具有交换性，所以不能理解为“距离”的概念，衡量的并不是两个分布在空间中的远近，更准确的理解还是衡量一个分布相比另一个分布的信息损失(infomation lost)。</p>
<h2 id="使用KL散度进行优化"><a href="#使用KL散度进行优化" class="headerlink" title="使用KL散度进行优化"></a>使用KL散度进行优化</h2><p>通过不断改变预估分布的参数，我们可以得到不同的KL散度的值。</p>
<p>在某个变化范围内，KL散度取到最小值的时候，对应的参数是我们想要的最优参数。</p>
<p>这就是使用KL散度优化的过程。</p>
<p>神经网络进行的工作很大程度上就是“函数的近似”(function approximators)。</p>
<p>因此我们可以使用神经网络学习很多复杂函数，学习过程的关键就是设定一个目标函数来衡量学习效果。</p>
<p>也就是通过最小化目标函数的损失来训练网络(minimizing the loss of the objective function)。</p>
<p>而KL散度可以作为正则化项(regularization term)加入损失函数之中，即使用KL散度来最小化我们近似分布时的信息损失，让我们的网络可以学习很多复杂的分布。</p>
<p>一个典型应用是VAE(变分自动编码)。</p>
<hr>
<p><strong>参考：</strong></p>
<p><a href="https://www.cnblogs.com/boceng/p/11519381.html">https://www.cnblogs.com/boceng/p/11519381.html</a></p>
<p><a href="https://blog.csdn.net/iterate7/article/details/80061850">https://blog.csdn.net/iterate7/article/details/80061850</a></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title>EM算法</title>
    <url>/2021/01/31/EM%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<h1 id="EM算法"><a href="#EM算法" class="headerlink" title="EM算法"></a>EM算法</h1><h5 id="参考视频：https-www-bilibili-com-video-BV1qW411k7ao"><a href="#参考视频：https-www-bilibili-com-video-BV1qW411k7ao" class="headerlink" title="参考视频：https://www.bilibili.com/video/BV1qW411k7ao"></a>参考视频：<a href="https://www.bilibili.com/video/BV1qW411k7ao">https://www.bilibili.com/video/BV1qW411k7ao</a></h5><h5 id="PDF笔记来源：https-github-com-ws13685555932-machine-learning-derivation"><a href="#PDF笔记来源：https-github-com-ws13685555932-machine-learning-derivation" class="headerlink" title="PDF笔记来源：https://github.com/ws13685555932/machine_learning_derivation"></a>PDF笔记来源：<a href="https://github.com/ws13685555932/machine_learning_derivation">https://github.com/ws13685555932/machine_learning_derivation</a></h5><h5 id="讲解：https-github-com-NLP-LOVE-ML-NLP-tree-master-Machine-20Learning-6-20EM"><a href="#讲解：https-github-com-NLP-LOVE-ML-NLP-tree-master-Machine-20Learning-6-20EM" class="headerlink" title="讲解：https://github.com/NLP-LOVE/ML-NLP/tree/master/Machine%20Learning/6.%20EM"></a>讲解：<a href="https://github.com/NLP-LOVE/ML-NLP/tree/master/Machine%20Learning/6.%20EM">https://github.com/NLP-LOVE/ML-NLP/tree/master/Machine%20Learning/6.%20EM</a></h5><p>最大期望算法（Expectation-maximization algorithm，又译为期望最大化算法），是在概率模型中寻找参数最大似然估计或者最大后验估计的算法，其中概率模型依赖于无法观测的隐性变量。</p>
<p>最大期望算法经过两个步骤交替进行计算，</p>
<p>第一步是计算期望（E），利用对隐藏变量的现有估计值，计算其最大似然估计值； 第二步是最大化（M），最大化在E步上求得的最大似然值来计算参数的值。M步上找到的参数估计值被用于下一个E步计算中，这个过程不断交替进行。</p>
<p>极大似然估计用一句话概括就是：知道结果，反推条件θ。</p>


	<div class="row">
    <embed src="../../../../file/10EM算法.pdf" width="100%" height="550" type="application/pdf">
	</div>


]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title>HMM隐马尔可夫模型</title>
    <url>/2021/01/31/HMM%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<h1 id="EM算法"><a href="#EM算法" class="headerlink" title="EM算法"></a>EM算法</h1><h5 id="参考视频：https-www-bilibili-com-video-BV1MW41167Rf"><a href="#参考视频：https-www-bilibili-com-video-BV1MW41167Rf" class="headerlink" title="参考视频：https://www.bilibili.com/video/BV1MW41167Rf"></a>参考视频：<a href="https://www.bilibili.com/video/BV1MW41167Rf">https://www.bilibili.com/video/BV1MW41167Rf</a></h5><h5 id="PDF笔记来源：https-github-com-ws13685555932-machine-learning-derivation"><a href="#PDF笔记来源：https-github-com-ws13685555932-machine-learning-derivation" class="headerlink" title="PDF笔记来源：https://github.com/ws13685555932/machine_learning_derivation"></a>PDF笔记来源：<a href="https://github.com/ws13685555932/machine_learning_derivation">https://github.com/ws13685555932/machine_learning_derivation</a></h5><h5 id="讲解：https-github-com-NLP-LOVE-ML-NLP-blob-master-Machine-20Learning-5-2-20Markov-5-2-20Markov-md"><a href="#讲解：https-github-com-NLP-LOVE-ML-NLP-blob-master-Machine-20Learning-5-2-20Markov-5-2-20Markov-md" class="headerlink" title="讲解：https://github.com/NLP-LOVE/ML-NLP/blob/master/Machine%20Learning/5.2%20Markov/5.2%20Markov.md"></a>讲解：<a href="https://github.com/NLP-LOVE/ML-NLP/blob/master/Machine%20Learning/5.2%20Markov/5.2%20Markov.md">https://github.com/NLP-LOVE/ML-NLP/blob/master/Machine%20Learning/5.2%20Markov/5.2%20Markov.md</a></h5><p>隐马尔可夫三大问题</p>
<ol>
<li>给定模型，如何有效计算产生观测序列的概率？换言之，如何评估模型与观测序列之间的匹配程度？</li>
<li>给定模型和观测序列，如何找到与此观测序列最匹配的状态序列？换言之，如何根据观测序列推断出隐藏的模型状态？</li>
<li>给定观测序列，如何调整模型参数使得该序列出现的概率最大？换言之，如何训练模型使其能最好地描述观测数据？</li>
</ol>
<p>前两个问题是模式识别的问题：</p>
<ol>
<li>根据隐马尔科夫模型得到一个可观察状态序列的概率(评价)；</li>
<li>找到一个隐藏状态的序列使得这个序列产生一个可观察状态序列的概率最大(解码)。</li>
</ol>
<p>第三个问题就是根据一个可以观察到的状态序列集产生一个隐马尔科夫模型（学习）。</p>
<p>对应的三大问题解法：</p>
<ol>
<li>向前算法(Forward Algorithm)、向后算法(Backward Algorithm)</li>
<li>维特比算法(Viterbi Algorithm)</li>
<li>鲍姆-韦尔奇算法(Baum-Welch Algorithm) (约等于EM算法)</li>
</ol>


	<div class="row">
    <embed src="../../../../file/14隐马尔可夫模型.pdf" width="100%" height="550" type="application/pdf">
	</div>


]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title>GloVe</title>
    <url>/2021/02/01/GloVe/</url>
    <content><![CDATA[<h1 id="GloVe"><a href="#GloVe" class="headerlink" title="GloVe"></a>GloVe</h1><h5 id="参考链接："><a href="#参考链接：" class="headerlink" title="参考链接："></a>参考链接：</h5><p><a href="https://www.fanyeong.com/2018/02/19/glove-in-detail/">https://www.fanyeong.com/2018/02/19/glove-in-detail/</a></p>
<p><a href="https://blog.csdn.net/csdn_inside/article/details/86507697">https://blog.csdn.net/csdn_inside/article/details/86507697</a></p>
<hr>
<h2 id="什么是GloVe？"><a href="#什么是GloVe？" class="headerlink" title="什么是GloVe？"></a>什么是GloVe？</h2><p>正如论文的标题而言，GloVe的全称叫Global Vectors for Word Representation，它是一个基于<strong>全局词频统计</strong>（count-based &amp; overall statistics）的词表征（word representation）工具，它可以把一个单词表达成一个由实数组成的向量，这些向量捕捉到了单词之间一些语义特性，比如相似性（similarity）、类比性（analogy）等。我们通过对向量的运算，比如欧几里得距离或者cosine相似度，可以计算出两个单词之间的语义相似性。</p>
<h2 id="GloVe是如何实现的？"><a href="#GloVe是如何实现的？" class="headerlink" title="GloVe是如何实现的？"></a>GloVe是如何实现的？</h2><p>GloVe的实现分为以下三步：</p>
<ul>
<li>根据语料库（corpus）构建一个共现矩阵（Co-ocurrence Matrix）$X$（什么是共现矩阵？），<strong>矩阵中的每一个元素代表单词和上下文单词在特定大小的上下文窗口（context window）内共同出现的次数。</strong>一般而言，这个次数的最小单位是1，但是GloVe不这么认为：它根据两个单词在上下文窗口的距离$d$，提出了一个衰减函数（decreasing weighting）：$decay = 1/d$ 用于计算权重，也就是说<strong>距离越远的两个单词所占总计数（total count）的权重越小。</strong></li>
</ul>
<p>In all cases we use a decreasing weighting function, so that word pairs that are d words apart contribute 1/d to the total count.</p>
<ul>
<li>构建词向量（Word Vector）和共现矩阵（Co-ocurrence Matrix）之间的近似关系，论文的作者提出以下的公式可以近似地表达两者之间的关系：</li>
</ul>
<script type="math/tex; mode=display">w_i^T \tilde{w_j} + b_i + \tilde{b_j} = log(X_{ij}) \qquad 公式（1）</script><p>其中，$w_i^T$和$\tilde{w_j}$<br>是我们最终要求解的词向量；$b_i$和$\tilde{b_j}$<br>分别是两个词向量的bias term。</p>
<p>当然你对这个公式一定有非常多的疑问，比如它到底是怎么来的，为什么要使用这个公式，为什么要构造两个词向量<br>$w_i^T$和$\tilde{w_j}$<br>？下文我们会详细介绍。</p>
<ul>
<li>有了公式1之后我们就可以构造它的loss function了：</li>
</ul>
<script type="math/tex; mode=display">J=\sum\limits_{i,j=1}^V f(X_{ij})(w_i^T\tilde{w_j}+b_i+\tilde{b_j}-\log (X_{ij}))^2 \qquad 公式（2）</script><p>这个loss function的基本形式就是最简单的mean square loss，只不过在此基础上加了一个权重函数$f(X_{ij})$，那么这个函数起了什么作用，为什么要添加这个函数呢？我们知道在一个语料库中，肯定存在很多单词他们在一起出现的次数是很多的（frequent co-occurrences），那么我们希望：</p>
<ul>
<li>这些单词的权重要大于那些很少在一起出现的单词（rare co-occurrences），所以这个函数要是非递减函数（non-decreasing）；</li>
<li>但我们也不希望这个权重过大（overweighted），当到达一定程度之后应该不再增加；</li>
<li>如果两个单词没有在一起出现，也就是$X_{ij}=0$，那么他们应该不参与到loss function的计算当中去，也就$f(x)$是要满足$f(0)=0$</li>
</ul>
<p>满足以上两个条件的函数有很多，作者采用了如下形式的分段函数：</p>
<script type="math/tex; mode=display">f(x)= \begin{cases}
    (x/x_{max})^\alpha && if x<x_{max} \\\\
    1 & & otherwise
\end{cases} \qquad 公式（3）</script><p>这个函数图像如下所示：</p>
<p><img src="/2021/02/01/GloVe/img1.jpg" width="80%"></p>
<p>这篇论文中的所有实验，$\alpha$的取值都是0.75，而$x_{max}$取值都是100。以上就是GloVe的实现细节，那么GloVe是如何训练的呢？</p>
<h2 id="GloVe是如何训练的？"><a href="#GloVe是如何训练的？" class="headerlink" title="GloVe是如何训练的？"></a>GloVe是如何训练的？</h2><p>虽然很多人声称GloVe是一种无监督（unsupervised learing）的学习方式（因为它确实不需要人工标注label），但其实它还是有label的，这个label就是公式2中的，而公式2中的$\log(X_{ij})$向量$w$和$\tilde{w}$<br>就是要不断更新/学习的参数，所以本质上它的训练方式跟监督学习的训练方法没什么不一样，都是基于梯度下降的。具体地，这篇论文里的实验是这么做的：<br><strong>采用了AdaGrad的梯度下降算法，对矩阵$X$中的所有非零元素进行随机采样，学习曲率（learning rate）设为0.05，在vector size小于300的情况下迭代了50次，其他大小的vectors上迭代了100次，直至收敛。</strong><br>最终学习得到的是两个vector是$w$和$\tilde{w}$<br>，因为$X$是对称的（symmetric），所以从原理上讲$w$和$\tilde{w}$<br>是也是对称的，他们唯一的区别是初始化的值不一样，而导致最终的值不一样。所以这两者其实是等价的，都可以当成最终的结果来使用。<strong>但是为了提高鲁棒性，我们最终会选择两者之和$w+\tilde{w}$<br>作为最终的vector（两者的初始化不同相当于加了不同的随机噪声，所以能提高鲁棒性）。</strong> 在训练了400亿个token组成的语料后，得到的实验结果如下图所示：<br><img src="/2021/02/01/GloVe/img2.jpg" width="80%"></p>
<p>这个图一共采用了三个指标：语义准确度，语法准确度以及总体准确度。那么我们不难发现Vector Dimension在300时能达到最佳，而context Windows size大致在6到10之间。</p>
<h2 id="Glove与LSA、word2vec的比较"><a href="#Glove与LSA、word2vec的比较" class="headerlink" title="Glove与LSA、word2vec的比较"></a>Glove与LSA、word2vec的比较</h2><p>LSA（Latent Semantic Analysis）是一种比较早的count-based的词向量表征工具，它也是基于co-occurance matrix的，只不过采用了基于奇异值分解（SVD）的矩阵分解技术对大矩阵进行降维，而我们知道SVD的复杂度是很高的，所以它的计算代价比较大。还有一点是它对所有单词的统计权重都是一致的。而这些缺点在GloVe中被一一克服了。而word2vec最大的缺点则是没有充分利用所有的语料，所以GloVe其实是把两者的优点结合了起来。从这篇论文给出的实验结果来看，GloVe的性能是远超LSA和word2vec的，但网上也有人说GloVe和word2vec实际表现其实差不多。</p>
<h2 id="公式推导"><a href="#公式推导" class="headerlink" title="公式推导"></a>公式推导</h2><ul>
<li>$X_{ij}$表示单词$j$出现在单词的上下文中的次数；</li>
<li>$X_i$表示单词$i$的上下文中所有单词出现的总次数，即$X_i=\sum^kX_{ik}$；</li>
<li>$P_{ij}=P(j|i)=X_{ij}/X_i$，即表示单词出现在单词的上下文中的概率；</li>
</ul>
<p>有了这些定义之后，我们来看一个表格：<br><img src="/2021/02/01/GloVe/img3.png" width="80%"></p>
<p>理解这个表格的重点在最后一行，它表示的是两个概率的比值（ratio），<strong>我们可以使用它观察出两个单词$i$和$j$相对于单词$k$哪个更相关（relevant）。</strong> 比如，ice和solid更相关，而stream和solid明显不相关，于是我们会发现$P(solid|ice)/P(solid|steam)$比1大更多。同样的gas和steam更相关，而和ice不相关，那么$P(gas|ice)/P(gas|steam)$就远小于1；当都有关（比如water）或者都没有关(fashion)的时候，两者的比例接近于1；这个是很直观的。因此，<strong>以上推断可以说明通过概率的比例而不是概率本身去学习词向量可能是一个更恰当的方法，</strong> 因此下文所有内容都围绕这一点展开。<br>于是为了捕捉上面提到的概率比例，我们可以构造如下函数：</p>
<script type="math/tex; mode=display">F(w_i,w_j,\tilde{w_k})=\frac{P_{ik}}{P_{jk}} \qquad 公式（4）</script><p>其中，函数$F$的参数和具体形式未定，它有三个参数<br>$w_i$,$w_j$<br>和$\tilde{w_k}$<br>，$w$和$\tilde{w}$<br>是不同的向量；<br>因为向量空间是线性结构的，所以要表达出两个概率的比例差，最简单的办法是作差，于是我们得到：</p>
<script type="math/tex; mode=display">F(w_i-w_j,\tilde{w_k}) = \frac{P_{ik}}{P_{jk}} \qquad 公式（5）</script><p>这时我们发现公式5的右侧是一个数量，而左侧则是一个向量，于是我们把左侧转换成两个向量的内积形式：</p>
<script type="math/tex; mode=display">F((w_i-w_j)^T \tilde{w_k})=\frac{P_{ik}}{P_{jk}} \qquad 公式（6）</script><p>我们知道$X$是个对称矩阵，单词和上下文单词其实是相对的，也就是如果我们做如下交换：$w\leftrightarrow\tilde{w_k}$<br>，$X \leftrightarrow X^T$<br>公式6应该保持不变，那么很显然，现在的公式是不满足的。为了满足这个条件，首先，我们要求函数$F$要满足同态特性（homomorphism）：</p>
<script type="math/tex; mode=display">F((w_i-w_j)^T \tilde{w_k})=\frac{F(w_i^T\tilde{w_k})}{F(w_j^T \tilde{w_k})} \qquad 公式（7）</script><p>结合公式6，我们可以得到：</p>
<script type="math/tex; mode=display">F(w_i^T \tilde{w_k}) = P_{ik}=\frac{X_{ik}}{X_i} \qquad 公式（8）</script><p>解公式（7），发现$F=exp()$。</p>
<p>把$F=exp$，带入公式（8），得到：</p>
<script type="math/tex; mode=display">w_i^T\tilde{w_k}=\log (P_{ik}) = \log(X_{ik})-\log (X_i)</script><p>对于j和k，得到的形式是一致的：</p>
<script type="math/tex; mode=display">w_j^T\tilde{w_k}=\log (P_{jk}) = \log(X_{jk})-\log (X_j)</script><p>既然语境词跟中心词是相对的，把k换成i或者j：</p>
<script type="math/tex; mode=display">w_i^T\tilde{w_j}=\log (P_{ij}) = \log(X_{ij})-\log (X_i)</script><script type="math/tex; mode=display">w_j^T\tilde{w_i}=\log (P_{ji}) = \log(X_{ji})-\log (X_j)</script><p>现在，两个等式的左边是相等的（都是内积相乘），右边不等。回到上面的那个共现矩阵，假设$i=I，j=like，X_{ij}=X_{ji}=2$，但是$X_i=3,X_j=4$，不等。问题就出在$log(X_i)$或者$log(X_j)$项，那么添加偏置$b_i，b_j$，用来抵消$log(X_i)和log(X_j)$，两者不就一样了。所以就得到最终的结果：</p>
<script type="math/tex; mode=display">w_i^T\tilde{w_k}+b_i+b_k=\log (X_{ik})</script><p>原来我们的目标，是找到一个词向量的函数F，来逼近比率$\frac{P_{ik}}{P_{jk}}$：</p>
<p>现在我们已经找到这个函数了，如何来刻画逼近的效果呢？用类似线性回归的MSE（均方误差）不就行了：</p>
<script type="math/tex; mode=display">J=\sum\limits_{i,j=1}^V (w_i^T\tilde{w_j}+b_i+\tilde{b_j}-\log (X_{ij}))^2 \qquad</script><p>矩阵分解方法，有个缺点，就是各个词的权重是一样的，处理这个问题，就是添加一个根据词频决定的权重项：</p>
<script type="math/tex; mode=display">J=\sum\limits_{i,j=1}^V f(X_{ij})(w_i^T\tilde{w_j}+b_i+\tilde{b_j}-\log (X_{ij}))^2 \qquad</script><p>这个权重项$F(X_{ij})$应该满足：</p>
<ol>
<li><p>$F(0)=0$，没出现过的词直接忽略</p>
</li>
<li><p>$F(x)$应该是非减函数，比如常出现的词，其权重应该不小，不常见的词，其权重应该不会过大。权重多少跟出现的频次正相关。</p>
</li>
<li><p>对于大的输入，$F(x)$应该不至于太大，不然像the、he这类词，权重就会过大。</p>
</li>
</ol>
<p>显然，满足这些条件的函数有很多个，但是作者经过试验，发现下面这个函数，在α=3/4时，效果最好。</p>
<script type="math/tex; mode=display">f(x)= \begin{cases}
    (x/x_{max})^\alpha && if x<x_{max} \\\\
    1 & & otherwise
\end{cases} \qquad</script>]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title>美女与男人的游戏</title>
    <url>/2021/02/02/%E7%BE%8E%E5%A5%B3%E4%B8%8E%E7%94%B7%E4%BA%BA%E7%9A%84%E6%B8%B8%E6%88%8F/</url>
    <content><![CDATA[<h1 id="美女与男人的游戏"><a href="#美女与男人的游戏" class="headerlink" title="美女与男人的游戏"></a>美女与男人的游戏</h1><p>美女与男人每个人各拿一枚硬币，两人同时出一面（类似于剪刀石头布）。 如果两枚硬币都是正面，则美女给男人3块。如果两枚硬币都是反面，则美女给男人1块。如果是一正一负，男人给美女两块钱。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>男/女</th>
<th>正</th>
<th>反</th>
</tr>
</thead>
<tbody>
<tr>
<td>正</td>
<td>+3</td>
<td>-2</td>
</tr>
<tr>
<td>反</td>
<td>-2</td>
<td>+1</td>
</tr>
</tbody>
</table>
</div>
<p>在这个游戏规则下，如果从概率的角度来看。总共是四个组合，++或者—都是1/4概率，男人赢的概率是1/2，得到+4；<br>+-或-+也是1/4，女人赢的概率也是1/2，得到+4。看起来是蛮公平的。</p>
<p>下面从博弈论的角度分析下这个问题</p>
<p>假设</p>
<p>男人出正面的概率是$x$,出反面的概率是$1-x$</p>
<p>美女出正面的概率是$y$,出反面的概率是$1-y$</p>
<p>则男人收益的数学期望$E$等于：</p>
<p>$E = 3<em>x</em>y+1<em>(1-x)(1-y)-2</em>x<em>(1-y)-2</em>y*(1-x)$</p>
<p>$\quad=8xy-3x-3y+1$</p>
<p>这里思考有没有一种对$y$取值的方法能让$E$尽可能的小于0呢？ 也就是让男人以更大的概率亏损。</p>
<p>$E=(8x-3)y-3x+1&lt;0$</p>
<p>$(8x-3)y&lt;3x-1$</p>
<p>$s1:(8x-3)&gt;0 \quad \rightarrow \quad x&gt;\frac{3}{8}$</p>
<p>$y&lt;\frac{3x-1}{8x-3}:减函数$</p>
<p>所以$y&lt;\frac{3x-1}{8x-3}|_{x=1}=\frac{2}{5}$时能够保证当$x&gt;\frac{3}{8}$时女人赢钱。</p>
<p>$s2:(8x-3)&lt;0 \quad \rightarrow \quad x&lt;\frac{3}{8}$</p>
<p>$y&gt;\frac{3x-1}{8x-3}:减函数$</p>
<p>所以$y&gt;\frac{3x-1}{8x-3}|_{x=0}=\frac{1}{3}$时能够保证当$x&lt;\frac{3}{8}$时女人赢钱。</p>
<p>$s3:(8x-3)=0 \quad \rightarrow x=\frac{3}{8}$</p>
<p>$E=-\frac{1}{8}&lt;0$</p>
<p>这里$y&lt;\frac{2}{5},\quad y&gt;\frac{1}{3}$是有交集的。</p>
<p>所以只要美女保证她出正面的概率在$\frac{1}{3}$到$\frac{2}{5}$之间就能够保证赢钱。</p>
]]></content>
      <categories>
        <category>acm</category>
      </categories>
      <tags>
        <tag>博弈论</tag>
      </tags>
  </entry>
  <entry>
    <title>Getting Grilhood Right</title>
    <url>/2021/02/03/Getting-Grilhood-Right/</url>
    <content><![CDATA[<h1 id="Getting-girlhood-right"><a href="#Getting-girlhood-right" class="headerlink" title="Getting girlhood right"></a>Getting girlhood right</h1><h3 id="Covid-19-threatens-girls’-gigantic-global-gains"><a href="#Covid-19-threatens-girls’-gigantic-global-gains" class="headerlink" title="Covid-19 threatens girls’ gigantic global gains"></a>Covid-19 threatens girls’ gigantic global gains</h3><hr>
<p>原文链接：<a href="https://www.economist.com/leaders/2020/12/19/covid-19-threatens-girls-gigantic-global-gains">https://www.economist.com/leaders/2020/12/19/covid-19-threatens-girls-gigantic-global-gains</a></p>
<h2 id="stymie"><a href="#stymie" class="headerlink" title="stymie:"></a>stymie:</h2><p>to prevent something from happening or someone from achieving a purpose</p>
<p>-Hamper/block/hinder/obstruct</p>
<p>-《经济学人》高频词：</p>
<p>Eg. Yet the banks have been reluctant suspicious, like the IMF, that politics will stymie reform, drive up inflation and sink their newly bought bonds. </p>
<h2 id="subordinate-səˈboː-dɪ-nət"><a href="#subordinate-səˈboː-dɪ-nət" class="headerlink" title="subordinate: /səˈbɔː.dɪ.nət/"></a>subordinate: /səˈbɔː.dɪ.nət/</h2><p>-The individual’s needs are subordinate to those of the group.</p>
<p>-underling/inferior</p>
<h2 id="worthwhile：-ˌwɜːθˈwaɪl"><a href="#worthwhile：-ˌwɜːθˈwaɪl" class="headerlink" title="worthwhile：/ˌwɜːθˈwaɪl/"></a>worthwhile：/ˌwɜːθˈwaɪl/</h2><p>-Good and important enough to spend time, effort, or money on.</p>
<p>-Making this video to help others is truly worthwhile.</p>
<p>-worth it/ which supermarket offers the best bang for your buck</p>
<h2 id="IMF"><a href="#IMF" class="headerlink" title="IMF:"></a>IMF:</h2><p>International Monetary Fund 国际货币基金组织</p>
<h2 id="outnumber-outperform"><a href="#outnumber-outperform" class="headerlink" title="outnumber/ outperform"></a>outnumber/ outperform</h2><p>-Democrats outnumber Republicans there</p>
<h2 id="blistering"><a href="#blistering" class="headerlink" title="blistering"></a>blistering</h2><p>-extremely fast</p>
<p>-During the campaign, Cameron pledged a first 100 days of blistering action (The Guardian)</p>
<p>-The runners set off at a blistering pace.</p>
<h2 id="on-a-par-with"><a href="#on-a-par-with" class="headerlink" title="on a par with:"></a>on a par with:</h2><p>-equal in importance or quality to.</p>
<p>-“I also hope that politicians of all parties develop a better understanding of alcoholism, take it more seriously and devise policies to treat it as a disease on a par with the other major diseases. (The Guardian)</p>
<h2 id="fetuses"><a href="#fetuses" class="headerlink" title="fetuses:"></a>fetuses:</h2><p>-a young human being or animal before birth, after the organs have started to develop</p>
<h2 id="rife"><a href="#rife" class="headerlink" title="rife:"></a>rife:</h2><p>-If something unpleasant is rife, it is very common or happens a lot:</p>
<p>-Dysentery and malaria are rife in the refugee camps.</p>
<p>-rife with sth</p>
<p>-full of something unpleasant</p>
<p>-The office was rife with rumours.</p>
<h2 id="hitched"><a href="#hitched" class="headerlink" title="hitched:"></a>hitched:</h2><p>to get married:</p>
<p>-Is Tracy really getting hitched?</p>
<h2 id="sexually-active"><a href="#sexually-active" class="headerlink" title="sexually active:"></a>sexually active:</h2><p>-engaging in sexual relations</p>
<p>-He became sexually active at the age of 21.</p>
<h2 id="contraception"><a href="#contraception" class="headerlink" title="contraception:"></a>contraception:</h2><p>-(the use of) any of various methods intended to prevent a woman becoming pregnant:</p>
<p>-What is the most reliable form/method of contraception?</p>
<h2 id="mutilation"><a href="#mutilation" class="headerlink" title="mutilation:"></a>mutilation:</h2><p>the act of damaging something severely, especially by violently removing a part:</p>
<p>-She views cosmetic surgery as unwarranted mutilation of the body.</p>
<h2 id="vocally"><a href="#vocally" class="headerlink" title="vocally:"></a>vocally:</h2><p>-by expressing opinions and complaints often:</p>
<p>-She was vocally opposed to the war.</p>
<h2 id="female-genital-mutilation"><a href="#female-genital-mutilation" class="headerlink" title="female genital mutilation"></a>female genital mutilation</h2><p>女性生殖器切割</p>
<h2 id="knock-on-effects"><a href="#knock-on-effects" class="headerlink" title="knock-on effects:"></a>knock-on effects:</h2><p>a secondary, indirect, or cumulative effect.</p>
<p>-“a decline in butterflies would have a knock-on effect on other British species”</p>
<h2 id="astounding"><a href="#astounding" class="headerlink" title="astounding:"></a>astounding:</h2><p>very surprising or shocking:</p>
<p>-an astounding fact/decision/revelation</p>
<h2 id="smorgasbord"><a href="#smorgasbord" class="headerlink" title="smorgasbord:"></a>smorgasbord:</h2><p>a mixture of many different hot and cold dishes that are arranged so that you can serve yourself</p>
<h2 id="stunted"><a href="#stunted" class="headerlink" title="stunted:"></a>stunted:</h2><p>-prevented from growing or developing to the usual size:</p>
<p>-A few stunted trees were the only vegetation visible.</p>
<p>-children with stunted growth</p>
<h2 id="Citi"><a href="#Citi" class="headerlink" title="Citi:"></a>Citi:</h2><p>花旗，1812</p>
<h2 id="hobble"><a href="#hobble" class="headerlink" title="hobble:"></a>hobble:</h2><p>-to limit something or control the freedom of someone:</p>
<p>-A long list of amendments have hobbled the new legislation.</p>
<h2 id="UNICEF"><a href="#UNICEF" class="headerlink" title="UNICEF:"></a>UNICEF:</h2><p>-abbreviation for United Nations Children’s Fund: a department of the United Nations whose aim is improving children’s health and education, especially in poor countries</p>
<h2 id="avert"><a href="#avert" class="headerlink" title="avert:"></a>avert:</h2><p>-to prevent something bad from happening:</p>
<p>-to avert a crisis/conflict/strike/famine</p>
<h2 id="genitals"><a href="#genitals" class="headerlink" title="genitals:"></a>genitals:</h2><p>the outer sexual organs, especially the penis or vulva<br>-private parts polite word</p>
<h2 id="regression"><a href="#regression" class="headerlink" title="regression:"></a>regression:</h2><p>-a return to a previous and less advanced or worse state, condition, or way of behaving:</p>
<p>-A regression has occurred in the overall political situation.</p>
<h2 id="consent"><a href="#consent" class="headerlink" title="consent:"></a>consent:</h2><p>-permission or agreement:</p>
<p>-No one can publish my words without my consent.</p>
<h2 id="self-assertive"><a href="#self-assertive" class="headerlink" title="self-assertive:"></a>self-assertive:</h2><p>-giving your opinions in a powerful way so that other people will notice</p>
<h2 id="juncture"><a href="#juncture" class="headerlink" title="juncture:"></a>juncture:</h2><p>-a particular point in time:</p>
<p>-At this juncture, it is impossible to say whether she will make a full recovery.</p>
<h2 id="truancy"><a href="#truancy" class="headerlink" title="truancy:"></a>truancy:</h2><p>-the problem or situation of children being absent from school regularly without permission:</p>
<p>-My daughter’s school has very good exam results and hardly any truancy.</p>
<h2 id="coercion-kəʊˈɜː-ʃən"><a href="#coercion-kəʊˈɜː-ʃən" class="headerlink" title="coercion: /kəʊˈɜː.ʃən/"></a>coercion: /kəʊˈɜː.ʃən/</h2><p>-the use of force to persuade someone to do something that they are unwilling to do:</p>
<p>-He claimed the police had used coercion, threats, and promises to obtain the statement illegally.</p>
]]></content>
      <categories>
        <category>TheEconomist</category>
      </categories>
  </entry>
  <entry>
    <title>《小王子》—I</title>
    <url>/2021/02/12/%E3%80%8A%E5%B0%8F%E7%8E%8B%E5%AD%90%E3%80%8B%E2%80%94I/</url>
    <content><![CDATA[<h1 id="《小王子》—I"><a href="#《小王子》—I" class="headerlink" title="《小王子》—I"></a>《小王子》—I</h1><h5 id="英文音频：https-www-bilibili-com-video-BV1H64y1f73s-p-2"><a href="#英文音频：https-www-bilibili-com-video-BV1H64y1f73s-p-2" class="headerlink" title="英文音频：https://www.bilibili.com/video/BV1H64y1f73s?p=2"></a>英文音频：<a href="https://www.bilibili.com/video/BV1H64y1f73s?p=2">https://www.bilibili.com/video/BV1H64y1f73s?p=2</a></h5><hr>
<p>当我还只有六岁的时候，在一本描写原始森林的名叫《真实的故事》的书中， 看到了一副精彩的插画，画的是一条蟒蛇正在吞食一只大野兽。页头上就是那副 画的摹本。</p>
<p><img src="/2021/02/12/%E3%80%8A%E5%B0%8F%E7%8E%8B%E5%AD%90%E3%80%8B%E2%80%94I/xwz01.jpg" alt="一条蟒蛇正在吞食一只大野兽"></p>
<p>这本书中写道：“这些蟒蛇把它们的猎获物不加咀嚼地囫囵吞下，尔后就不 能再动弹了；它们就在长长的六个月的睡眠中消化这些食物。”</p>
<p>当时，我对丛林中的奇遇想得很多，于是，我也用彩色铅笔画出了我的第一 副图画。我的第一号作品。它是这样的：</p>
<p><img src="/2021/02/12/%E3%80%8A%E5%B0%8F%E7%8E%8B%E5%AD%90%E3%80%8B%E2%80%94I/xwz02.jpg" alt="小王子的第一号作品"></p>
<p>我把我的这副杰作拿给大人看，我问他们我的画是不是叫他们害怕。</p>
<p>他们回答我说：“一顶帽子有什么可怕的？”</p>
<p>我画的不是帽子，是一条巨蟒在消化着一头大象。于是我又把巨蟒肚子里的情况画了出来，以便让大人们能够看懂。这些大人总是需要解释。我的第二号作品是这样的：</p>
<p><img src="/2021/02/12/%E3%80%8A%E5%B0%8F%E7%8E%8B%E5%AD%90%E3%80%8B%E2%80%94I/xwz03.jpg" alt="小王子的第二号作品"></p>
<p>大人们劝我把这些画着开着肚皮的，或闭上肚皮的蟒蛇的图画放在一边，还 是把兴趣放在地理、历史、算术、语法上。就这样，在六岁的那年，我就放弃了 当画家这一美好的职业。我的第一号、第二号作品的不成功，使我泄了气。这些 大人们，靠他们自己什么也弄不懂，还得老是不断地给他们作解释。这真叫孩子 们腻味。</p>
<p>后来，我只好选择了另外一个职业，我学会了开飞机，世界各地差不多都飞 到过。的确，地理学帮了我很大的忙。我一眼就能分辨出中国和亚里桑那。要是 夜里迷失了航向，这是很有用的。</p>
<p>这样，在我的生活中，我跟许多严肃的人有过很多的接触。我在大人们中间 生活过很长时间。我仔细地观察过他们，但这并没有使我对他们的看法有多大的 改变。</p>
<p>当我遇到一个头脑看来稍微清楚的大人时，我就拿出一直保存着的我那第一 号作品来测试测试他。我想知道他是否真的有理解能力。可是，得到的回答总是： “这是顶帽子。”我就不和他谈巨蟒呀，原始森林呀，或者星星之类的事。我只 得迁就他们的水平，和他们谈些桥牌呀，高尔夫球呀，政治呀，领带呀这些。于 是大人们就十分高兴能认识我这样一个通情达理的人。</p>
]]></content>
      <categories>
        <category>小王子</category>
      </categories>
  </entry>
  <entry>
    <title>《小王子》—II</title>
    <url>/2021/02/12/%E3%80%8A%E5%B0%8F%E7%8E%8B%E5%AD%90%E3%80%8B%E2%80%94II/</url>
    <content><![CDATA[<h1 id="《小王子》—II"><a href="#《小王子》—II" class="headerlink" title="《小王子》—II"></a>《小王子》—II</h1><h5 id="英文音频：https-www-bilibili-com-video-BV1H64y1f73s-p-3"><a href="#英文音频：https-www-bilibili-com-video-BV1H64y1f73s-p-3" class="headerlink" title="英文音频：https://www.bilibili.com/video/BV1H64y1f73s?p=3"></a>英文音频：<a href="https://www.bilibili.com/video/BV1H64y1f73s?p=3">https://www.bilibili.com/video/BV1H64y1f73s?p=3</a></h5><hr>
<p>我就这样孤独地生活着，没有一个能真正谈得来的人，一直到六年前在撒哈 拉沙漠上发生了那次故障。我的发动机里有个东西损坏了。当时由于我既没有带 机械师也没有带旅客，我就试图独自完成这个困难的维修工作。这对我来说是个 生与死的问题。我随身带的水只够饮用一星期。</p>
<p>第一天晚上我就睡在这远离人间烟火的大沙漠上。我比大海中伏在小木排上 的遇难者还要孤独得多。而在第二天拂晓，当一个奇怪的小声音叫醒我的时候， 你们可以想见我当时是多么吃惊。这小小的声音说道：</p>
<p>“请你给我画一只羊，好吗？”</p>
<p>“啊！”</p>
<p>“给我画一只羊……”</p>
<p>我象是受到惊雷轰击一般，一下子就站立起来。我使劲地揉了揉眼睛，仔细 地看了看。我看见一个十分奇怪的小家伙严肃地朝我凝眸望着。这是后来我给他 画出来的最好的一副画像。可是，我的画当然要比他本人的模样逊色得多。这不 是我的过错。六岁时，大人们使我对我的画家生涯失去了勇气，除了画过开着肚皮和闭着肚皮的蟒蛇，后来再没有学过画。</p>
<p><img src="/2021/02/12/%E3%80%8A%E5%B0%8F%E7%8E%8B%E5%AD%90%E3%80%8B%E2%80%94II/xwz04.jpg" alt="《小王子》童话小说图片"></p>
<p>我惊奇地睁大着眼睛看着这突然出现的小家伙。你们不要忘记，我当时处在 远离人烟千里之外的地方。而这个小家伙给我的印象是，他既不象迷了路的样子， 也没有半点疲乏、饥渴、惧怕的神情。他丝毫不象是一个迷失在旷无人烟的大沙 漠中的孩子。当我在惊讶之中终于又能说出话来的时候，对他说道：</p>
<p>“唉，你在这儿干什么？”</p>
<p>可是他却不慌不忙地好象有一件重要的事一般，对我重复地说道：</p>
<p>“请……给我画一只羊……”</p>
<p>当一种神秘的东西把你镇住的时候，你是不敢不听从它的支配的，在这旷无 人烟的沙漠上，面临死亡的危险的情况下，尽管这样的举动使我感到十分荒诞， 我还是掏出了一张纸和一支钢笔。这时我却又记起，我只学过地理、历史、算术 和语法，就有点不大高兴地对小家伙说我不会画画。他回答我说：</p>
<p>“没有关系，给我画一只羊吧！”</p>
<p>因为我从来没有画过羊，我就给他重画我所仅仅会画的两副画中的那副闭着 肚皮的巨蟒。</p>
<p>“不，不！我不要蟒蛇，它肚子里还有一头象。”</p>
<p>我听了他的话，简直目瞪口呆。他接着说：“巨蟒这东西太危险，大象又太 占地方。我住的地方非常小，我需要一只羊。给我画一只羊吧。”</p>
<p>我就给他画了。</p>
<p><img src="/2021/02/12/%E3%80%8A%E5%B0%8F%E7%8E%8B%E5%AD%90%E3%80%8B%E2%80%94II/xwz05.jpg" alt></p>
<p>他专心地看着，随后又说：</p>
<p>“我不要，这只羊已经病得很重了。给我重新画一只。”</p>
<p>我又画了起来。</p>
<p><img src="/2021/02/12/%E3%80%8A%E5%B0%8F%E7%8E%8B%E5%AD%90%E3%80%8B%E2%80%94II/xwz06.jpg" alt></p>
<p>我的这位朋友天真可爱地笑了，并且客气地拒绝道：“你看，你画的不是小羊，是头公羊，还有犄角呢。”</p>
<p>于是我又重新画了一张。</p>
<p><img src="/2021/02/12/%E3%80%8A%E5%B0%8F%E7%8E%8B%E5%AD%90%E3%80%8B%E2%80%94II/xwz08.jpg" alt></p>
<p>这副画同前几副一样又被拒绝了。</p>
<p>“这一只太老了。我想要一只能活得长的羊。”</p>
<p>我不耐烦了。因为我急于要检修发动机，于是就草草画了这张画，并且匆匆 地对他说道：</p>
<p><img src="/2021/02/12/%E3%80%8A%E5%B0%8F%E7%8E%8B%E5%AD%90%E3%80%8B%E2%80%94II/xwz07.jpg" alt></p>
<p>“这是一只箱子，你要的羊就在里面。”</p>
<p>这时我十分惊奇地看到我的这位小评判员喜笑颜开。他说：</p>
<p>“这正是我想要的，……你说这只羊需要很多草吗？”</p>
<p>“为什么问这个呢？”</p>
<p>“因为我那里地方非常小……”</p>
<p>“我给你画的是一只很小的小羊，地方小也够喂养它的。”</p>
<p>他把脑袋靠近这张画。</p>
<p>“并不象你说的那么小……瞧！它睡着了……”</p>
<p>就这样，我认识了小王子。</p>
]]></content>
      <categories>
        <category>小王子</category>
      </categories>
  </entry>
  <entry>
    <title>《小王子》—III</title>
    <url>/2021/02/12/%E3%80%8A%E5%B0%8F%E7%8E%8B%E5%AD%90%E3%80%8B%E2%80%94III/</url>
    <content><![CDATA[<h1 id="《小王子》—III"><a href="#《小王子》—III" class="headerlink" title="《小王子》—III"></a>《小王子》—III</h1><h5 id="英文音频：https-www-bilibili-com-video-BV1H64y1f73s-p-4"><a href="#英文音频：https-www-bilibili-com-video-BV1H64y1f73s-p-4" class="headerlink" title="英文音频：https://www.bilibili.com/video/BV1H64y1f73s?p=4"></a>英文音频：<a href="https://www.bilibili.com/video/BV1H64y1f73s?p=4">https://www.bilibili.com/video/BV1H64y1f73s?p=4</a></h5><hr>
<p>我费了好长时间才弄清楚他是从哪里来的。小王子向我提出了很多问题，可 是，对我提出的问题，他好象压根没有听见似的。他无意中吐露的一些话逐渐使我搞清了他的来历。例如，当他第一次瞅见我的飞机时（我就不画出我的飞机了， 因为这种图画对我来说太复杂），他问我道：</p>
<p>“这是个啥玩艺？”</p>
<p>“这不是‘玩艺儿’。它能飞。这是飞机。是我的飞机。”</p>
<p>我当时很骄傲地告诉他我能飞。于是他惊奇地说道：</p>
<p>“怎么？你是从天上掉下来的？”</p>
<p>“是的”。我谦逊地答道。</p>
<p>“啊？这真滑稽。”</p>
<p>此时小王子发出一阵清脆的笑声。这使我很不高兴。我要求别人严肃地对待 我的不幸。然后，他又说道：</p>
<p>“那么，你也是从天上来的了！你是哪个星球上的？”</p>
<p>即刻，对于他是从哪里来的这个秘密我隐约发现到了一点线索；于是，我就 突然问道：</p>
<p>“你是从另一个星球上来的吗？”</p>
<p>可是他不回答我的问题。他一面看着我的飞机，一面微微地点点头，接着说道：</p>
<p>“可不是么，乘坐这玩艺儿，你不可能是从很远的地方来的……”</p>
<p>说到这里，他就长时间地陷入沉思之中。然后，从口袋里掏出了我画的小羊， 看着他的宝贝入了神。</p>
<p>你们可以想见这种关于“别的星球”的若明若暗的话语使我心里多么好奇。 因此我竭力地想知道其中更多的奥秘。</p>
<p>“你是从哪里来的，我的小家伙？你的家在什么地方？你要把我的小羊带到 哪里去？”</p>
<p><img src="/2021/02/12/%E3%80%8A%E5%B0%8F%E7%8E%8B%E5%AD%90%E3%80%8B%E2%80%94III/xwz09.jpg" alt="《小王子》童话小说图片"></p>
<p>他沉思了一会，然后回答我说：</p>
<p>“好在有你给我的那只箱子，夜晚可以给小羊当房子用。”</p>
<p>“那当然。如果你听话的话，我再给你画一根绳子，白天可以栓住它。再加 上一根扦杆。”</p>
<p>我的建议看来有点使小王子反感。</p>
<p>“栓住它，多么奇怪的主意。”</p>
<p>“如果你不栓住它，它就到处跑，那么它会跑丢的。”</p>
<p>我的这位朋友又笑出了声：</p>
<p>“你想要它跑到哪里去呀？”</p>
<p>“不管什么地方。它一直往前跑……”</p>
<p>这时，小王子郑重其事地说：</p>
<p>“这没有什么关系，我那里很小很小。”</p>
<p>接着，他略带伤感地又补充了一句：</p>
<p>“一直朝前走，也不会走出多远……”</p>
]]></content>
      <categories>
        <category>小王子</category>
      </categories>
  </entry>
  <entry>
    <title>《小王子》—IV</title>
    <url>/2021/02/12/%E3%80%8A%E5%B0%8F%E7%8E%8B%E5%AD%90%E3%80%8B%E2%80%94IV/</url>
    <content><![CDATA[<h1 id="《小王子》—IV"><a href="#《小王子》—IV" class="headerlink" title="《小王子》—IV"></a>《小王子》—IV</h1><h5 id="英文音频：https-www-bilibili-com-video-BV1H64y1f73s-p-5"><a href="#英文音频：https-www-bilibili-com-video-BV1H64y1f73s-p-5" class="headerlink" title="英文音频：https://www.bilibili.com/video/BV1H64y1f73s?p=5"></a>英文音频：<a href="https://www.bilibili.com/video/BV1H64y1f73s?p=5">https://www.bilibili.com/video/BV1H64y1f73s?p=5</a></h5><hr>
<p>我还了解到另一件重要的事，就是他老家所在的那个星球比一座房子大不了多少。</p>
<p><img src="/2021/02/12/%E3%80%8A%E5%B0%8F%E7%8E%8B%E5%AD%90%E3%80%8B%E2%80%94IV/xwz10.jpg" alt="《小王子》童话小说图片"></p>
<p>这倒并没有使我感到太奇怪。我知道除地球、木星、火星、金星这几个有名 称的大行星以外，还有成百个别的星球，它们有的小得很，就是用望远镜也很难 看见。当一个天文学者发现了其中一个星星，他就给它编上一个号码，例如把它 称作“325小行星”。</p>
<p>我有重要的根据认为小王子所来自的那个星球是小行星B612。这颗小行星仅 仅在1909年被一个土耳其天文学家用望远镜看见过一次。</p>
<p><img src="/2021/02/12/%E3%80%8A%E5%B0%8F%E7%8E%8B%E5%AD%90%E3%80%8B%E2%80%94IV/xwz11.jpg" alt="《小王子》童话小说土耳其天文学家图片"></p>
<p>当时他曾经在一次国际天文学家代表大会上对他的发现作了重要的论证。但 由于他所穿衣服的缘故，那时没有人相信他。那些大人们就是这样。</p>
<p><img src="/2021/02/12/%E3%80%8A%E5%B0%8F%E7%8E%8B%E5%AD%90%E3%80%8B%E2%80%94IV/xwz12.jpg" alt="《小王子》童话小说土耳其天文学家图片"></p>
<p>幸好，土耳其的一个独裁者，为了小行星B612的声誉，迫使他的人民都要穿 欧式服装，否则就处以死刑。1920年，这位天文学家穿了一身非常漂亮的服装， 重新作了一次论证。这一次所有的人都同意他的看法。</p>
<p><img src="/2021/02/12/%E3%80%8A%E5%B0%8F%E7%8E%8B%E5%AD%90%E3%80%8B%E2%80%94IV/xwz13.jpg" alt="《小王子》土耳其天文学家"></p>
<p>我给你们讲关于小行星B612的这些细节，并且告诉你们它的编号，这是由于 这些大人的缘故。这些大人们就爱数目字。当你对大人们讲起你的一个新朋友时， 他们从来不向你提出实质性的问题。他们从来不讲：“他说话声音如何啊？他喜 爱什么样的游戏啊？他是否收集蝴蝶标本呀？”他们却问你：“他多大年纪呀？ 弟兄几个呀？体重多少呀？他父亲挣多少钱呀？”他们以为这样才算了解朋友。 如果你对大人们说：“我看到一幢用玫瑰色的砖盖成的漂亮的房子，它的窗户上 有天竺葵，屋顶上还有鸽子……”他们怎么也想象不出这种房子有多么好。必须对 他们说：“我看见了一幢价值十万法郎的房子。”那么他们就惊叫道：“多么漂 亮的房子啊！”</p>
<p>要是你对他们说：“小王子存在的证据就是他非常漂亮，他笑着，想要一只 羊。他想要一只小羊，这就证明他的存在。”他们一定会耸耸肩膀，把你当作孩 子看待！但是，如果你对他们说：“小王子来自的星球就是小行星B612”，那么 他们就十分信服，他们就不会提出一大堆问题来和你纠缠。他们就是这样的。小 孩子们对大人们应该宽厚些，不要埋怨他们。</p>
<p>当然，对我们懂得生活的人来说，我们才不在乎那些编号呢！我真愿意象讲 神话那样来开始这个故事，我真想这样说：</p>
<p>“从前呀，有一个小王子，他住在一个和他身体差不多大的星球上，他希望 有一个朋友……”对懂得生活的人来说，这样说就显得真实。</p>
<p>我可不喜欢人们轻率地读我的书。我在讲述这些往事时心情是很难过的。我 的朋友带着他的小羊已经离去六年了。我之所以在这里尽力把他描写出来，就是 为了不要忘记他。忘记一个朋友，这太叫人悲伤了。并不是所有的人都有过一个 朋友。再说，我也可能变成那些大人那样，只对数字感兴趣。也正是为了这个缘 故，我买了一盒颜料和一些铅笔。象我这样年纪的人，而且除了六岁时画过闭着 肚皮的和开着肚皮的巨蟒外，别的什么也没有尝试过，现在，重新再来画画，真 费劲啊！当然，我一定要把这些画尽量地画得逼真，但我自己也没有把握。一张 画得还可以，另一张就不象了。还有身材大小，我画得有点不准确。在这个地方 小王子画得太大了些，另一个地方又画得太小了些。对他衣服的颜色我也拿不准。 于是我就摸索着这么试试那么改改，画个大概齐。我很可能在某些重要的细节上 画错了。这就得请大家原谅我了。因为我的这个朋友，从来也不加说明解释。他 认为我同他一样。可是，很遗憾，我却不能透过盒子看见小羊。我大概有点和大人们差不多。我一定是变老了。</p>
]]></content>
      <categories>
        <category>小王子</category>
      </categories>
  </entry>
  <entry>
    <title>《小王子》—V</title>
    <url>/2021/02/15/%E3%80%8A%E5%B0%8F%E7%8E%8B%E5%AD%90%E3%80%8B%E2%80%94V/</url>
    <content><![CDATA[<h1 id="《小王子》—V"><a href="#《小王子》—V" class="headerlink" title="《小王子》—V"></a>《小王子》—V</h1><h5 id="英文音频：https-www-bilibili-com-video-BV1H64y1f73s-p-6"><a href="#英文音频：https-www-bilibili-com-video-BV1H64y1f73s-p-6" class="headerlink" title="英文音频：https://www.bilibili.com/video/BV1H64y1f73s?p=6"></a>英文音频：<a href="https://www.bilibili.com/video/BV1H64y1f73s?p=6">https://www.bilibili.com/video/BV1H64y1f73s?p=6</a></h5><hr>
<p>每天我都了解到一些关于小王子的星球，他的出走和旅行等事情。这些都是 偶然从各种反应中慢慢得到的。就这样，第三天我就了解到关于猴面包树的悲剧。</p>
<p>这一次又是因为羊的事情，突然小王子好象是非常担心地问我道：</p>
<p>“羊吃小灌木，这是真的吗？”</p>
<p>“是的，是真的。”</p>
<p>“啊，我真高兴。”</p>
<p>我不明白羊吃小灌木这件事为什么如此重要。可小王子又说道：</p>
<p>“因此，它们也吃猴面包树罗？”<br>我对小王子说，猴面包树可不是小灌木，而是象教堂那么大的大树；即便是 带回一群大象，也啃不了一棵猴面包树。</p>
<p>一群大象这种想法使小王子发笑：</p>
<p>“那可得把这些大象一只叠一只地垒起来。”</p>
<p><img src="/2021/02/15/%E3%80%8A%E5%B0%8F%E7%8E%8B%E5%AD%90%E3%80%8B%E2%80%94V/xwz14.jpg" alt="《小王子》童话小说图片"></p>
<p>他很有见识地说：</p>
<p>“猴面包树在长大之前，开始也是小小的。”</p>
<p>“不错。可是为什么你想叫你的羊去吃小猴面包树呢？”</p>
<p>他回答我道：“唉！这还用说！”似乎这是不言而喻的。可是我自己要费很 大的心劲才能弄懂这个问题。</p>
<p>原来，在小王子的星球上就象其他所有星球上一样，有好草和坏草；因此， 也就有益草的草籽和毒草的草籽，可是草籽是看不见的。它们沉睡在泥土里，直 到其中的一粒忽然想要苏醒过来……于是它就伸展开身子，开始腼腆地朝着太阳长 出一棵秀丽可爱的小嫩苗。如果是小萝卜或是玫瑰的嫩苗，就让它去自由地生长。 如果是一棵坏苗，一旦被辨认出来，就应该马上把它拔掉。因为在小王子的星球 上，有些非常可怕的种子……这就是猴面包树的种子。在那里的泥土里，这种种子 多得成灾。而一棵猴面包树苗，假如你拔得太迟，就再也无法把它清除掉。它就 会盘踞整个星球。它的树根能把星球钻透，如果星球很小，而猴面包树很多，它 就把整个星球搞得支离破碎。</p>
<p><img src="/2021/02/15/%E3%80%8A%E5%B0%8F%E7%8E%8B%E5%AD%90%E3%80%8B%E2%80%94V/xwz16.jpg" alt="《小王子》中的猴面包树"></p>
<p>“这是个纪律问题。”小王子后来向我解释道。“当你早上梳洗完毕以后， 必须仔细地给星球梳洗，必须规定自己按时去拔掉猴面包树苗。这种树苗小的时 候与玫瑰苗差不多，一旦可以把它们区别开的时候，就要把它拔掉。这是一件非 常乏味的工作，但很容易。”</p>
<p><img src="/2021/02/15/%E3%80%8A%E5%B0%8F%E7%8E%8B%E5%AD%90%E3%80%8B%E2%80%94V/xwz15.jpg" alt="《小王子》童话小说图片"></p>
<p>有一天，他劝我用心地画一副漂亮的图画，好叫我家乡的孩子们对这件事有 一个深刻的印象。他还对我说：“如果将来有一天他们出外旅行，这对他们是很 有用的。有时候，人们把自己的工作推到以后去做，并没有什么妨害，但要遇到 拔猴面包树苗这种事，那就非造成大灾难不可。我遇到过一个星球，上面住着一 个懒家伙，他放过了三棵小树苗……”</p>
<p>于是，根据小王子的说明，我把这个星球画了下来。我从来不大愿意以道学 家的口吻来说话，可是猴面包树的危险，大家都不大了解，对迷失在小行星上的 人来说，危险性非常之大，因此这一回，我贸然打破了我的这种不喜欢教训人的 惯例。我说：“孩子们，要当心那些猴面包树呀！”为了叫我的朋友们警惕这种 危险——他们同我一样长期以来和这种危险接触，却没有意识到它的危险性—— 我花了很大的功夫画了这副画。我提出的这个教训意义是很重大的，花点功夫是 很值得的。你们也许要问，为什么这本书中别的画都没有这副画那么壮观呢？回 答很简单：别的画我也曾经试图画得好些，却没成功。而当我画猴面包树时，有 一种急切的心情在激励着我。</p>
]]></content>
      <categories>
        <category>小王子</category>
      </categories>
  </entry>
  <entry>
    <title>《小王子》—VI</title>
    <url>/2021/02/15/%E3%80%8A%E5%B0%8F%E7%8E%8B%E5%AD%90%E3%80%8B%E2%80%94VI/</url>
    <content><![CDATA[<h1 id="《小王子》—VI"><a href="#《小王子》—VI" class="headerlink" title="《小王子》—VI"></a>《小王子》—VI</h1><h5 id="英文音频：https-www-bilibili-com-video-BV1H64y1f73s-p-7"><a href="#英文音频：https-www-bilibili-com-video-BV1H64y1f73s-p-7" class="headerlink" title="英文音频：https://www.bilibili.com/video/BV1H64y1f73s?p=7"></a>英文音频：<a href="https://www.bilibili.com/video/BV1H64y1f73s?p=7">https://www.bilibili.com/video/BV1H64y1f73s?p=7</a></h5><hr>
<p>啊！小王子，就这样，我逐渐懂得了你那忧郁的生活。过去相当长的时间里 你唯一的乐趣就是观赏那夕阳西下的温柔晚景。这个新的细节，是我在第四天早 晨知道的。你当时对我说道：</p>
<p>“我喜欢看日落。我们去看一回日落吧！”</p>
<p>“可是得等着……”</p>
<p>“等什么？”</p>
<p>“等太阳落山。”</p>
<p>开始，你显得很惊奇的样子，随后你笑自己的糊涂。你对我说：</p>
<p>“我总以为是在我的家乡呢！”</p>
<p>确实，大家都知道，在美国是正午时分，在法国，正夕阳西下，只要在一分 钟内赶到法国就可看到日落。可惜法国是那么的遥远。而在你那样的小行星上， 你只要把你的椅子挪动几步就行了。这样，你便可随时看到你想看的夕阳余辉……</p>
<p>“一天，我看见过四十三次日落。”</p>
<p>过一会儿，你又说：</p>
<p>“你知道，当人们感到非常苦闷时，总是喜欢日落的。”</p>
<p>“一天四十三次，你怎么会这么苦闷？”</p>
<p>小王子没有回答。</p>
]]></content>
      <categories>
        <category>小王子</category>
      </categories>
  </entry>
  <entry>
    <title>《小王子》—IX</title>
    <url>/2021/02/26/%E3%80%8A%E5%B0%8F%E7%8E%8B%E5%AD%90%E3%80%8B%E2%80%94IX/</url>
    <content><![CDATA[<h1 id="《小王子》—IX"><a href="#《小王子》—IX" class="headerlink" title="《小王子》—IX"></a>《小王子》—IX</h1><h5 id="英文音频：https-www-bilibili-com-video-BV1H64y1f73s-p-10"><a href="#英文音频：https-www-bilibili-com-video-BV1H64y1f73s-p-10" class="headerlink" title="英文音频：https://www.bilibili.com/video/BV1H64y1f73s?p=10"></a>英文音频：<a href="https://www.bilibili.com/video/BV1H64y1f73s?p=10">https://www.bilibili.com/video/BV1H64y1f73s?p=10</a></h5><hr>
<p>我想小王子大概是利用一群候鸟迁徙的机会跑出来的。在他出发的那天早上， 他把他的星球收拾得整整齐齐，把它上头的活火山打扫得干干净净。——他有两 个活火山，早上热早点很方便。他还有一座死火山，他也把它打扫干净。他想， 说不定它还会活动呢！打扫干净了，它们就可以慢慢地有规律地燃烧，而不会突 然爆发。火山爆发就象烟囱里的火焰一样。当然，在我们地球上我们人太小，不 能打扫火山，所以火山给我们带来很多很多麻烦。</p>
<p><img src="/2021/02/26/%E3%80%8A%E5%B0%8F%E7%8E%8B%E5%AD%90%E3%80%8B%E2%80%94IX/xwz23.jpg" alt="童话《小王子》"> </p>
<p>小王子还把剩下的最后几颗猴面包树苗全拔了。他有点忧伤。他以为他再也 不会回来了。这天，这些家常活使他感到特别亲切。当他最后一次浇花时，准备 把她好好珍藏起来。他发觉自己要哭出来。</p>
<p>“再见了。”他对花儿说道。</p>
<p>可是花儿没有回答他。</p>
<p>“再见了。”他又说了一遍。</p>
<p>花儿咳嗽了一阵。但并不是由于感冒。</p>
<p>她终于对他说道：“我方才真蠢。请你原谅我。希望你能幸福。” 花儿对他毫不抱怨，他感到很惊讶。他举着罩子，不知所措地伫立在那里。 他不明白她为什么会这样温柔恬静。</p>
<p><img src="/2021/02/26/%E3%80%8A%E5%B0%8F%E7%8E%8B%E5%AD%90%E3%80%8B%E2%80%94IX/xwz24.jpg" alt="《小王子》">  </p>
<p>“的确，我爱你。”花儿对他说道：“但由于我的过错，你一点也没有理会。 这丝毫不重要。不过，你也和我一样的蠢。希望你今后能幸福。把罩子放在一边 吧，我用不着它了。”</p>
<p>“要是风来了怎么办？”</p>
<p>“我的感冒并不那么重……夜晚的凉风对我倒有好处。我是一朵花。”</p>
<p>“要是有虫子野兽呢？……”</p>
<p>“我要是想认识蝴蝶，经不起两三只尺蠖是不行的。据说这是很美的。不然 还有谁来看我呢？你就要到远处去了。至于说大动物，我并不怕，我有爪子。”</p>
<p>于是，她天真地显露出她那四根刺，随后又说道：<br>“别这么磨蹭了。真烦人！你既然决定离开这儿，那么，快走吧！”</p>
<p>她是怕小王子看见她在哭。她是一朵非常骄傲的花……</p>
]]></content>
      <categories>
        <category>小王子</category>
      </categories>
  </entry>
  <entry>
    <title>《小王子》—VII</title>
    <url>/2021/02/26/%E3%80%8A%E5%B0%8F%E7%8E%8B%E5%AD%90%E3%80%8B%E2%80%94VII/</url>
    <content><![CDATA[<h1 id="《小王子》—VII"><a href="#《小王子》—VII" class="headerlink" title="《小王子》—VII"></a>《小王子》—VII</h1><h5 id="英文音频：https-www-bilibili-com-video-BV1H64y1f73s-p-8"><a href="#英文音频：https-www-bilibili-com-video-BV1H64y1f73s-p-8" class="headerlink" title="英文音频：https://www.bilibili.com/video/BV1H64y1f73s?p=8"></a>英文音频：<a href="https://www.bilibili.com/video/BV1H64y1f73s?p=8">https://www.bilibili.com/video/BV1H64y1f73s?p=8</a></h5><hr>
<p>第五天，还是羊的事，把小王子的生活秘密向我揭开了。好象默默地思索了 很长时间以后，得出了什么结果一样，他突然没头没脑地问我：</p>
<p>“羊，要是吃小灌木，它也要吃花罗？”</p>
<p>“它碰到什么吃什么。” “连有刺的花也吃吗？”</p>
<p>“有刺的也吃！”</p>
<p>“那么刺有什么用呢？”</p>
<p>我不知道该怎么回答。那会儿我正忙着要从发动机上卸下一颗拧得太紧的螺 丝。我发现机器故障似乎很严重，饮水也快完了，担心可能发生最坏的情况，心 里很着急。</p>
<p>“那么刺有什么用呢？”</p>
<p>小王子一旦提出了问题，从来不会放过。这个该死的螺丝使我很恼火，我于 是就随便回答了他一句：</p>
<p>“刺么，什么用都没有，这纯粹是花的恶劣表现。”</p>
<p>“噢！”</p>
<p>可是他沉默了一会儿之后，怀着不满的心情冲我说：</p>
<p>“我不信！花是弱小的、淳朴的，它们总是设法保护自己，以为有了刺就可 以显出自己的厉害……”</p>
<p>我默不作声。我当时想的，如果这个螺丝再和我作对，我就一锤子敲掉它。 小王子又来打搅我的思绪了：</p>
<p>“你却认为花……”</p>
<p>“算了吧，算了吧！我什么也不认为！我是随便回答你的。我可有正经事要 做。” 他惊讶地看着我。</p>
<p>“正经事？”</p>
<p>他瞅着我手拿锤子，手指沾满了油污，伏在一个在他看来丑不可言的机件上。</p>
<p>“你说话就和那些大人一样！”</p>
<p>这话使我有点难堪。可是他又尖刻无情地说道：</p>
<p>“你什么都分不清……你把什么都混在一起！”</p>
<p>他着实非常恼火。摇动着脑袋，金黄色的头发随风颤动着。</p>
<p>“我到过一个星球，上面住着一个红脸先生。他从来没闻过一朵花。他从来 没有看过一颗星星。他什么人也没有喜欢过。除了算帐以外，他什么也没有做过。 他整天同你一样老是说：‘我有正经事，我是个严肃的人’。这使他傲气十足。 他简直不象是个人，他是个蘑菇。”</p>
<p>“是个什么？”</p>
<p>“是个蘑菇！”</p>
<p>小王子当时气得脸色发白。</p>
<p><img src="/2021/02/26/%E3%80%8A%E5%B0%8F%E7%8E%8B%E5%AD%90%E3%80%8B%E2%80%94VII/xwz17.jpg" alt="童话小说《小王子》"> </p>
<p>“几百万年以来花儿都在制造着刺，几百万年以来羊仍然在吃花。要搞清楚 为什么花儿费那么大劲给自己制造没有什么用的刺，这难道不是正经事？难道羊 和花之间的战争不重要？这难道不比那个大胖子红脸先生的帐目更重要？如果我 认识一朵人世间唯一的花，只有我的星球上有它，别的地方都不存在，而一只小 羊胡里胡涂就这样把它一下子毁掉了，这难道不重要？”</p>
<p>他的脸气得发红，然后又接着说道：</p>
<p>“如果有人爱上了在这亿万颗星星中独一无二的一株花，当他看着这些星星 的时候，这就足以使他感到幸福。他可以自言自语地说：‘我的那朵花就在其中 的一颗星星上……’，但是如果羊吃掉了这朵花，对他来说，好象所有的星星一下 子全都熄灭了一样！这难道也不重要吗？！”</p>
<p>他无法再说下去了，突然泣不成声。夜幕已经降临。我放下手中的工具。我 把锤子、螺钉、饥渴、死亡，全都抛在脑后。在一颗星球上，在一颗行星上，在 我的行星上，在地球上有一个小王子需要安慰！我把他抱在怀里。我摇着他，对 他说：“你爱的那朵花没有危险……我给你的小羊画一个罩子……我给你的花画一副 盔甲……我……”我也不太知道该说些什么。我觉得自己太笨拙。我不知道怎样才能 达到他的境界，怎样才能再进入他的境界……唉，泪水的世界是多么神秘啊！</p>
]]></content>
      <categories>
        <category>小王子</category>
      </categories>
  </entry>
  <entry>
    <title>《小王子》—VIII</title>
    <url>/2021/02/26/%E3%80%8A%E5%B0%8F%E7%8E%8B%E5%AD%90%E3%80%8B%E2%80%94VIII/</url>
    <content><![CDATA[<h1 id="《小王子》—VIII"><a href="#《小王子》—VIII" class="headerlink" title="《小王子》—VIII"></a>《小王子》—VIII</h1><h5 id="英文音频：https-www-bilibili-com-video-BV1H64y1f73s-p-9"><a href="#英文音频：https-www-bilibili-com-video-BV1H64y1f73s-p-9" class="headerlink" title="英文音频：https://www.bilibili.com/video/BV1H64y1f73s?p=9"></a>英文音频：<a href="https://www.bilibili.com/video/BV1H64y1f73s?p=9">https://www.bilibili.com/video/BV1H64y1f73s?p=9</a></h5><hr>
<p>很快我就进一步了解了这朵花儿。在小王子的星球上，过去一直都生长着一 些只有一层花瓣的很简单的花。这些花非常小，一点也不占地方，从来也不会去 打搅任何人。她们早晨在草丛中开放，晚上就凋谢了。不知从哪里来了一颗种子， 忽然一天这种子发了芽。小王子特别仔细地监视着这棵与众不同的小苗：这玩艺 说不定是一种新的猴面包树。但是，这小苗不久就不再长了，而且开始孕育着一 个花朵。看到在这棵苗上长出了一个很大很大的花蕾，小王子感觉到从这个花苞 中一定会出现一个奇迹。然而这朵花藏在它那绿茵茵的房间中用了很长的时间来 打扮自己。她精心选择着她将来的颜色，慢慢腾腾地妆饰着，一片片地搭配着她 的花瓣，她不愿象虞美人那样一出世就满脸皱纹。她要让自己带着光艳夺目的丽 姿来到世间。是的，她是非常爱俏的。她用好些好些日子天仙般地梳妆打扮。然 后，在一天的早晨，恰好在太阳升起的时候，她开放了。</p>
<p>她已经精细地做了那么长的准备工作，却打着哈欠说道：</p>
<p><img src="/2021/02/26/%E3%80%8A%E5%B0%8F%E7%8E%8B%E5%AD%90%E3%80%8B%E2%80%94VIII/xwz18.jpg" alt="小王子的玫瑰"></p>
<p>“我刚刚睡醒，真对不起，瞧我的头发还是乱蓬蓬的……”</p>
<p>小王子这时再也控制不住自己的爱慕心情：<br>“你是多么美丽啊！”</p>
<p>花儿悠然自得地说：</p>
<p>“是吧，我是与太阳同时出生的……”</p>
<p>小王子看出了这花儿不太谦虚，可是她确实丽姿动人。</p>
<p><img src="/2021/02/26/%E3%80%8A%E5%B0%8F%E7%8E%8B%E5%AD%90%E3%80%8B%E2%80%94VIII/xwz19.jpg" alt="小王子和玫瑰"></p>
<p>她随后又说道：“现在该是吃早点的时候了吧，请你也想着给我准备一点……”</p>
<p>小王子很有些不好意思，于是就拿着喷壶，打来了一壶清清的凉水，浇灌着 花儿。</p>
<p><img src="/2021/02/26/%E3%80%8A%E5%B0%8F%E7%8E%8B%E5%AD%90%E3%80%8B%E2%80%94VIII/xwz20.jpg" alt="小王子和玫瑰"></p>
<p>于是，就这样，这朵花儿就以她那有点敏感多疑的虚荣心折磨着小王子。例 如，有一天，她向小王子讲起她身上长的四根刺：</p>
<p>“老虎，让它张着爪子来吧！”</p>
<p><img src="/2021/02/26/%E3%80%8A%E5%B0%8F%E7%8E%8B%E5%AD%90%E3%80%8B%E2%80%94VIII/xwz21.jpg" alt="《小王子》童话小说"> </p>
<p>小王子顶了她一句：“在我这个星球上没有老虎，而且，老虎是不会吃草的”。</p>
<p>花儿轻声说道：“我并不是草。”</p>
<p>“真对不起。”</p>
<p>“我并不怕什么老虎，可我讨厌穿堂风。你没有屏风？”</p>
<p>小王子思忖着：“讨厌穿堂风……这对一株植物来说，真不走运，这朵花儿真 不大好伺候……”</p>
<p>“晚上您得把我保护好。你这地方太冷。在这里住得不好，我原来住的那个 地方……”</p>
<p>但她没有说下去。她来的时候是粒种子。她哪里见过什么别的世界。她叫人 发现她是在凑一个如此不太高明的谎话，她有点羞怒，咳嗽了两三声。她的这一 招是要小王子处于有过失的地位，她说道：<br>“屏风呢？”</p>
<p><img src="/2021/02/26/%E3%80%8A%E5%B0%8F%E7%8E%8B%E5%AD%90%E3%80%8B%E2%80%94VIII/xwz22.jpg" alt="《小王子》"> </p>
<p>“我这就去拿。可你刚才说的是……”</p>
<p>于是花儿放开嗓门咳嗽了几声，依然要使小王子后悔自己的过失。</p>
<p>尽管小王子本来诚心诚意地喜欢这朵花，可是，这一来，却使他马上对她产 生了怀疑。小王子对一些无关紧要的话看得太认真，结果使自己很苦恼。</p>
<p>有一天他告诉我说：“我不该听信她的话，绝不该听信那些花儿的话，看看花，闻闻它就得了。我的那朵花使我的星球芳香四溢，可我不会享受它。关于老 虎爪子的事，本应该使我产生同情，却反而使我恼火……”</p>
<p>他还告诉我说：</p>
<p>“我那时什么也不懂！我应该根据她的行为，而不是根据她的话来判断她。她使我的生活芬芳多彩，我真不该离开她跑出来。我本应该猜出在她那令人爱怜 的花招后面所隐藏的温情。花是多么自相矛盾！我当时太年青，还不懂得爱她。”</p>
]]></content>
      <categories>
        <category>小王子</category>
      </categories>
  </entry>
  <entry>
    <title>《小王子》—X</title>
    <url>/2021/02/26/%E3%80%8A%E5%B0%8F%E7%8E%8B%E5%AD%90%E3%80%8B%E2%80%94X/</url>
    <content><![CDATA[<h1 id="《小王子》—X"><a href="#《小王子》—X" class="headerlink" title="《小王子》—X"></a>《小王子》—X</h1><h5 id="英文音频：https-www-bilibili-com-video-BV1H64y1f73s-p-11"><a href="#英文音频：https-www-bilibili-com-video-BV1H64y1f73s-p-11" class="headerlink" title="英文音频：https://www.bilibili.com/video/BV1H64y1f73s?p=11"></a>英文音频：<a href="https://www.bilibili.com/video/BV1H64y1f73s?p=11">https://www.bilibili.com/video/BV1H64y1f73s?p=11</a></h5><hr>
<p>在附近的宇宙中，还有 325、326、327、328、329、330 等几颗小行星。他 就开始访问这几颗星球，想在那里找点事干，并且学习学习。</p>
<p>第一颗星球上住着一个国王。国王穿着用紫红色和白底黑花的毛皮做成的大 礼服，坐在一个很简单却又十分威严的宝座上。</p>
<p><img src="/2021/02/26/%E3%80%8A%E5%B0%8F%E7%8E%8B%E5%AD%90%E3%80%8B%E2%80%94X/xwz25.jpg" alt="《小王子》-- 国王"></p>
<p>当他看见小王子时，喊了起来：</p>
<p>“啊，来了一个臣民。”</p>
<p>小王子思量着：“他从来也没有见过我，怎么会认识我呢？”</p>
<p>他哪里知道，在那些国王的眼里，世界是非常简单的：所有的人都是臣民。</p>
<p>国王十分骄傲，因为他终于成了某个人的国王，他对小王子说道：“靠近些， 好让我好好看看你。”</p>
<p>小王子看看四周，想找个地方坐下来，可是整个星球被国王华丽的白底黑花 皮袍占满了。他只好站在那里，但是因为疲倦了，他打起哈欠来。</p>
<p>君王对他说：“在一个国王面前打哈欠是违反礼节的。我禁止你打哈欠。”</p>
<p>小王子羞愧地说道：“我实在忍不住，我长途跋涉来到这里，还没有睡觉呢。”</p>
<p>国王说：“那好吧，我命令你打哈欠。好些年来我没有看见过任何人打哈欠。 对我来说，打哈欠倒是新奇的事。来吧，再打个哈欠！这是命令。”</p>
<p>“这倒叫我有点紧张……我打不出哈欠来了……”小王子红着脸说。</p>
<p>“嗯！嗯！”国王回答道：“那么我……命令你忽而打哈欠，忽而……”</p>
<p>他嘟嘟囔囔，显出有点恼怒。</p>
<p>因为国王所要求的主要是保持他的威严受到尊敬。他不能容忍不听他的命令。 他是一位绝对的君主。可是，他却很善良，他下的命令都是有理智的。</p>
<p>他常常说：“如果我叫一位将军变成一只海鸟，而这位将军不服从我的命令， 那么这就不是将军的过错，而是我的过错。”</p>
<p>小王子腼腆地试探道：“我可以坐下吗？”</p>
<p>“我命令你坐下。”国王一边回答，一边庄重地把他那白底黑花皮袍大襟挪 动了一下。</p>
<p>可是小王子感到很奇怪。这么小的行星，国王他对什么进行统治呢？</p>
<p>他对国王说：“陛下……请原谅，我想问您……”</p>
<p>国王急忙抢着说道：“我命令你问我。”</p>
<p>“陛下……你统治什么呢？”</p>
<p>国王非常简单明了地说：“我统治一切。”</p>
<p>“一切？”</p>
<p>国王轻轻地用手指着他的行星和其他的行星，以及所有的星星。</p>
<p>小王子说：“统治这一切？”</p>
<p>“统治这一切。”</p>
<p>原来他不仅是一个绝对的君主，而且是整个宇宙的君主。</p>
<p>“那么，星星都服从您吗？”</p>
<p>“那当然！”国王对他说，“它们立即就得服从。我是不允许无纪律的。”</p>
<p>这样的权力使小王子惊叹不已。如果掌握了这样的权力，那么，他一天就不 只是看到四十三次日落，而可以看到七十二次，甚至一百次，或是二百次日落， 也不必要去挪动椅子了！由于他想起了他那被遗弃的小星球，心里有点难过，他 大胆地向国王提出了一个请求：</p>
<p>“我想看日落，请求您……命令太阳落山吧……”</p>
<p>国王说道：“如果我命令一个将军象一只蝴蝶那样从这朵花飞到那朵花，或 者命令他写作一个悲剧剧本或者变一只海鸟，而如果这位将军接到命令不执行的 话，那么，是他不对还是我不对呢？”</p>
<p>“那当然是您的不对。”小王子肯定地回答。</p>
<p>“一点也不错，”国王接着说，“向每个人提出的要求应该是他们所能做到 的。权威首先应该建立在理性的基础上。如果命令你的老百姓去投海，他们非起 来革命不可。我的命令是合理的，所以我有权要别人服从。”</p>
<p>“那么我提出的日落呢？”小王子一旦提出一个问题，他是不会忘记这个问 题的。</p>
<p>“日落么，你会看到的。我一定要太阳落山，不过按照我的统治科学，我得 等到条件成熟的时候。”</p>
<p>小王子问道：“这要等到什么时候呢？”</p>
<p>国王在回答之前，首先翻阅了一本厚厚的日历，嘴里慢慢说道：“嗯！嗯！ 日落大约……大约……在今晚七时四十分的时候！你将看到我的命令一定会被服从的。”</p>
<p>小王子又打起哈欠来了。他遗憾没有看到日落。他有点厌烦了，他对国王说： “我没有必要再呆在这儿了。我要走了。”</p>
<p>这位因为刚刚有了一个臣民而十分骄傲自得的国王说道：</p>
<p>“别走，别走。我任命你当大臣。”</p>
<p>“什么大臣”</p>
<p>“嗯……司法大臣！”</p>
<p>“可是，这儿没有一个要审判的人。”</p>
<p>“很难说呀，”国王说道。“我很老了，我这地方又小，没有放銮驾的地方， 另外，一走路我就累。因此我还没有巡视过我的王国呢！”</p>
<p>“噢！可是我已经看过了。”小王子说道，并探身朝星球的那一侧看了看。 那边也没有一个人……</p>
<p>“那么你就审判你自己呀！”国王回答他说。“这可是最难的了。审判自己 比审判别人要难得多啊！你要是能审判好自己，你就是一个真正有才智的人。”</p>
<p>“我吗，随便在什么地方我都可以审度自己。我没有必要留在这里。”</p>
<p>国王又说：“嗯……嗯……我想，在我的星球上有一只老耗子。夜里，我听见它 的声音。你可以审判它，不时地判处它死刑。因此它的生命取决于你的判决。可 是，你要有节制地使用这只耗子，每次判刑后都要赦免它，因为只有这一只耗子。”</p>
<p>“可是我不愿判死刑，我想我还是应该走。”小王子回答道。</p>
<p>“不行。”国王说。</p>
<p>但是小王子，准备完毕之后，不想使老君主难过，说道：</p>
<p>“如果国王陛下想要不折不扣地得到服从，你可以给我下一个合理的命令。 比如说，你可以命令我，一分钟之内必须离开。我认为这个条件是成熟的……”</p>
<p>国王什么也没有回答。起初，小王子有些犹疑不决，随后叹了口气，就离开 了……</p>
<p>“我派你当我的大使。”国王匆忙地喊道。</p>
<p>国王显出非常有权威的样子。</p>
<p>小王子在旅途中自言自语地说：“这些大人真奇怪。”</p>
]]></content>
      <categories>
        <category>小王子</category>
      </categories>
  </entry>
  <entry>
    <title>《小王子》—XI</title>
    <url>/2021/02/26/%E3%80%8A%E5%B0%8F%E7%8E%8B%E5%AD%90%E3%80%8B%E2%80%94XI/</url>
    <content><![CDATA[<h1 id="《小王子》—XI"><a href="#《小王子》—XI" class="headerlink" title="《小王子》—XI"></a>《小王子》—XI</h1><h5 id="英文音频：https-www-bilibili-com-video-BV1H64y1f73s-p-12"><a href="#英文音频：https-www-bilibili-com-video-BV1H64y1f73s-p-12" class="headerlink" title="英文音频：https://www.bilibili.com/video/BV1H64y1f73s?p=12"></a>英文音频：<a href="https://www.bilibili.com/video/BV1H64y1f73s?p=12">https://www.bilibili.com/video/BV1H64y1f73s?p=12</a></h5><hr>
<p>第二个行星上住着一个爱虚荣的人。</p>
<p>“喔唷！一个崇拜我的人来拜访了！”这个爱虚荣的人一见到小王子，老远 就叫喊起来。</p>
<p><img src="/2021/02/26/%E3%80%8A%E5%B0%8F%E7%8E%8B%E5%AD%90%E3%80%8B%E2%80%94XI/xwz26.jpg" alt="《小王子》-- 爱虚荣的人"></p>
<p>在那些爱虚荣的人眼里，别人都成了他们的崇拜者。</p>
<p>“你好！”小王子说道。“你的帽子很奇怪。”</p>
<p>“这是为了向人致意用的。”爱虚荣的人回答道，“当人们向我欢呼的时候， 我就用帽子向他们致意。可惜，没有一个人经过这里。”</p>
<p>小王子不解其意。说道：“啊？是吗？”</p>
<p>爱虚荣的人向小王子建议道：“你用一只手去拍另一只手。”</p>
<p>小王子就拍起巴掌来。这位爱虚荣者就谦逊地举起帽子向小王子致意。</p>
<p>小王子心想：“这比访问那位国王有趣。”于是他又拍起巴掌来。爱虚荣者 又举起帽子来向他致意。</p>
<p>小王子这样做了五分钟，之后对这种单调的把戏有点厌倦了，说道：</p>
<p>“要想叫你的帽子掉下来，该怎么做呢？”</p>
<p>可这回爱虚荣者听不进他的话，因为凡是爱虚荣的人只听得进赞美的话。</p>
<p>他问小王子道：“你真的钦佩我吗？”</p>
<p>“钦佩是什么意思？”</p>
<p>“钦佩么，就是承认我是星球上最美的人，服饰最好的人，最富有的人，最 聪明的人。”</p>
<p>“可您是您的星球上唯一的人呀！”</p>
<p>“让我高兴吧，请你还是来钦佩我吧！”</p>
<p>小王子轻轻地耸了耸肩膀，说道：“我钦佩你，可是，这有什么能使你这样 感兴趣的？”</p>
<p>于是小王子就走开了。</p>
<p>小王子在路上自言自语地说了一句：“这些大人，肯定是十分古怪的。”</p>
]]></content>
      <categories>
        <category>小王子</category>
      </categories>
  </entry>
  <entry>
    <title>《小王子》—XII</title>
    <url>/2021/02/26/%E3%80%8A%E5%B0%8F%E7%8E%8B%E5%AD%90%E3%80%8B%E2%80%94XII/</url>
    <content><![CDATA[<h1 id="《小王子》—XII"><a href="#《小王子》—XII" class="headerlink" title="《小王子》—XII"></a>《小王子》—XII</h1><h5 id="英文音频：https-www-bilibili-com-video-BV1H64y1f73s-p-13"><a href="#英文音频：https-www-bilibili-com-video-BV1H64y1f73s-p-13" class="headerlink" title="英文音频：https://www.bilibili.com/video/BV1H64y1f73s?p=13"></a>英文音频：<a href="https://www.bilibili.com/video/BV1H64y1f73s?p=13">https://www.bilibili.com/video/BV1H64y1f73s?p=13</a></h5><hr>
<p>小王子所访问的下一个星球上住着一个酒鬼。访问时间非常短，可是它却使 小王子非常忧伤。</p>
<p><img src="/2021/02/26/%E3%80%8A%E5%B0%8F%E7%8E%8B%E5%AD%90%E3%80%8B%E2%80%94XII/xwz27.jpg" alt="《小王子》-- 酒鬼"></p>
<p>“你在干什么？”小王子问酒鬼，这个酒鬼默默地坐在那里，面前有一堆酒 瓶子，有的装着酒，有的是空的。</p>
<p>“我喝酒。”他阴沉忧郁地回答道。</p>
<p>“你为什么喝酒？”小王子问道。</p>
<p>“为了忘却。”酒鬼回答。</p>
<p>小王子已经有些可怜酒鬼。他问道：“忘却什么呢？”</p>
<p>酒鬼垂下脑袋坦白道：“为了忘却我的羞愧。”</p>
<p>“你羞愧什么呢？”小王子很想救助他。</p>
<p>“我羞愧我喝酒。”酒鬼说完以后就再也不开口了。</p>
<p>小王子迷惑不解地离开了。</p>
<p>在旅途中，他自言自语地说道：“这些大人确实真叫怪。”</p>
]]></content>
      <categories>
        <category>小王子</category>
      </categories>
  </entry>
  <entry>
    <title>《小王子》—XIII</title>
    <url>/2021/02/26/%E3%80%8A%E5%B0%8F%E7%8E%8B%E5%AD%90%E3%80%8B%E2%80%94XIII/</url>
    <content><![CDATA[<h1 id="《小王子》—XIII"><a href="#《小王子》—XIII" class="headerlink" title="《小王子》—XIII"></a>《小王子》—XIII</h1><h5 id="英文音频：https-www-bilibili-com-video-BV1H64y1f73s-p-14"><a href="#英文音频：https-www-bilibili-com-video-BV1H64y1f73s-p-14" class="headerlink" title="英文音频：https://www.bilibili.com/video/BV1H64y1f73s?p=14"></a>英文音频：<a href="https://www.bilibili.com/video/BV1H64y1f73s?p=14">https://www.bilibili.com/video/BV1H64y1f73s?p=14</a></h5><hr>
<p>第四个行星是一个实业家的星球。这个人忙得不可开交，小王子到来的时候， 他甚至连头都没有抬一下。</p>
<p><img src="/2021/02/26/%E3%80%8A%E5%B0%8F%E7%8E%8B%E5%AD%90%E3%80%8B%E2%80%94XIII/xwz28.jpg" alt="《小王子》-- 实业家"> </p>
<p>小王子对他说：“您好。您的烟卷灭了。”</p>
<p>“三加二等于五。五加七等于十二。十二加三等于十五。你好。十五加七，二十二。二十二加六，二十八。没有时间去再点着它。二十六加五，三十一。哎 哟！一共是五亿一百六十二万二千七百三十一。”</p>
<p>“五亿什么呀？”</p>
<p>“嗯？你还待在这儿那？五亿一百万……我也不知道是什么了。我的工作很多…… 我是很严肃的，我可是从来也没有功夫去闲聊！二加五得七……”</p>
<p>“五亿一百万什么呀？”小王子重复问道。一旦他提出了一个问题，是从来 也不会放弃的。</p>
<p>这位实业家抬起头，说：</p>
<p>“我住在这个星球上五十四年以来，只被打搅过三次。第一次是二十二年前， 不知从哪里跑来了一只金龟子来打搅我。它发出一种可怕的噪音，使我在一笔帐 目中出了四个差错。第二次，在十一年前，是风湿病发作，因为我缺乏锻炼所致。 我没有功夫闲逛。我可是个严肃的人。现在……这是第三次！我计算的结果是五亿 一百万……”<br>“几百万什么？”</p>
<p>这位实业家知道要想安宁是无望的了，就说道：</p>
<p>“几百万个小东西，这些小东西有时出现在天空中。”</p>
<p>“苍蝇吗？”</p>
<p>“不是，是些闪闪发亮的小东西。”</p>
<p>“是蜜蜂吗？”</p>
<p>“不是，是金黄色的小东西，这些小东西叫那些懒汉们胡思乱想。我是个严 肃的人。我没有时间胡思乱想。”</p>
<p>“啊，是星星吗？”</p>
<p>“对了，就是星星。”</p>
<p>“你要拿这五亿星星做什么？”</p>
<p>“五亿一百六十二万七百三十一颗星星。我是严肃的人，我是非常精确的。”</p>
<p>“你拿这些星星做什么？”</p>
<p>“我要它做什么？”</p>
<p>“是呀。”</p>
<p>“什么也不做。它们都是属于我的。”</p>
<p>“星星是属于你的？”</p>
<p>“是的。”</p>
<p>“可是我已经见到过一个国王，他……”</p>
<p>“国王并不占有，他们只是进行‘统治’。这不是一码事。”</p>
<p>“你拥有这许多星星有什么用？”</p>
<p>“富了就可以去买别的星星，如果有人发现了别的星星的话。”</p>
<p>小王子自言自语地说：“这个人想问题有点象那个酒鬼一样。”</p>
<p>可是他又提了一些问题：</p>
<p>“你怎么能占有星星呢？”</p>
<p>“那么你说星星是谁的呀？”实业家不高兴地顶了小王子一句。</p>
<p>“我不知道，不属于任何人。”</p>
<p>“那么，它们就是我的，因为是我第一个想到了这件事情的。”</p>
<p>“这就行了吗？”</p>
<p>“那当然。如果你发现了一颗没有主人的钻石，那么这颗钻石就是属于你的。 当你发现一个岛是没有主的，那么这个岛就是你的。当你首先想出了一个办法， 你就去领一个专利证，这个办法就是属于你的。既然在我之前不曾有任何人想到 要占有这些星星，那我就占有这些星星。”</p>
<p>“这倒也是。可是你用它们来干什么？”小王子说。</p>
<p>“我经营管理这些星星。我一遍又一遍地计算它们的数目。这是一件困难的 事。但我是一个严肃认真的人！”</p>
<p>小王子仍然还不满足，他说：</p>
<p>“对我来说，如果我有一条围巾，我可以用它来围着我的脖子，并且能带走 它。我有一朵花的话，我就可以摘下我的花，并且把它带走。可你却不能摘下这 些星星呀！”</p>
<p>“我不能摘，但我可以把它们存在银行里。”</p>
<p>“这是什么意思呢？”</p>
<p>“这就是说，我把星星的数目写在一片小纸头上，然后把这片纸头锁在一个 抽屉里。”</p>
<p>“这就算完事了吗？”</p>
<p>“这样就行了。”</p>
<p>小王子想道：“真好玩。这倒蛮有诗意，可是，并不算是了不起的正经事。”</p>
<p>关于什么是正经事，小王子的看法与大人们的看法非常不同。他接着又说：</p>
<p>“我有一朵花，我每天都给她浇水。我还有三座火山，我每星期把它们全都 打扫一遍。连死火山也打扫。谁知道它会不会再复活。我拥有火山和花，这对我 的火山有益处，对我的花也有益处。但是你对星星并没有用处……”</p>
<p>实业家张口结舌无言以对。于是小王子就走了。</p>
<p>在旅途中，小王子只是自言自语地说了一句：“这些大人们真是奇怪极了。”</p>
]]></content>
      <categories>
        <category>小王子</category>
      </categories>
  </entry>
  <entry>
    <title>《小王子》—XIV</title>
    <url>/2021/02/26/%E3%80%8A%E5%B0%8F%E7%8E%8B%E5%AD%90%E3%80%8B%E2%80%94XIV/</url>
    <content><![CDATA[<h1 id="《小王子》—XIV"><a href="#《小王子》—XIV" class="headerlink" title="《小王子》—XIV"></a>《小王子》—XIV</h1><h5 id="英文音频：https-www-bilibili-com-video-BV1H64y1f73s-p-15"><a href="#英文音频：https-www-bilibili-com-video-BV1H64y1f73s-p-15" class="headerlink" title="英文音频：https://www.bilibili.com/video/BV1H64y1f73s?p=15"></a>英文音频：<a href="https://www.bilibili.com/video/BV1H64y1f73s?p=15">https://www.bilibili.com/video/BV1H64y1f73s?p=15</a></h5><hr>
<p>第五颗行星非常奇怪，是这些星星中最小的一颗。行星上刚好能容得下一盏 路灯和一个点路灯的人。小王子怎么也解释不通：这个坐落在天空某一角落，既<br>没有房屋又没有居民的行星上，要一盏路灯和一个点灯的人做什么用。</p>
<p><img src="/2021/02/26/%E3%80%8A%E5%B0%8F%E7%8E%8B%E5%AD%90%E3%80%8B%E2%80%94XIV/xwz29.jpg" alt="点路灯的人"></p>
<p>但他自己猜想：“可能这个人思想不正常。但他比起国王，比起那个爱虚荣 的人，那个实业家和酒鬼，却要好些。至少他的工作还有点意义。当他点着了他 的路灯时，就象他增添了一颗星星，或是一朵花。当他熄灭了路灯时，就象让星 星或花朵睡着了似的。这差事真美妙，就是真正有用的了。”</p>
<p>小王子一到了这个行星上，就很尊敬地向点路灯的人打招呼：</p>
<p>“早上好。——你刚才为什么把路灯灭了呢？”</p>
<p>“早上好。——这是命令。”点灯的回答道。</p>
<p>“命令是什么？”</p>
<p>“就是熄掉我的路灯。——晚上好。”</p>
<p>于是他又点燃了路灯。</p>
<p>“那么为什么你又把它点着了呢？”</p>
<p>“这是命令。”点灯的人回答道。</p>
<p>“我不明白。”小王子说。</p>
<p>“没什么要明白的。命令就是命令。”点灯的回答说。“早上好。”</p>
<p>于是他又熄灭了路灯。</p>
<p>然后他拿一块有红方格子的手绢擦着额头。</p>
<p>“我干的是一种可怕的职业。以前还说得过去，早上熄灯，晚上点灯，剩下 时间，白天我就休息，夜晚我就睡觉……”</p>
<p>“那么，后来命令改变了，是吗？”</p>
<p>点灯的人说：“命令没有改，惨就惨在这里了！这颗行星一年比一年转得更 快，而命令却没有改。”</p>
<p>“结果呢？”小王子问。</p>
<p>“结果现在每分钟转一圈，我连一秒钟的休息时间都没有了。每分钟我就要 点一次灯，熄一次灯！”</p>
<p>“真有趣，你这里每天只有一分钟长？”</p>
<p>“一点趣味也没有，”点灯的说，“我们俩在一块说话就已经有一个月的时 间了。”</p>
<p>“一个月？”</p>
<p>“对。三十分钟。三十天！——晚上好。”</p>
<p>于是他又点着了了他的路灯。</p>
<p>小王子瞅着他，他喜欢这个点灯人如此忠守命令。这时，他想起了他自己从 前挪动椅子寻找日落的事。他很想帮助他的这位朋友。 “告诉你，我知道一种能使你休息的办法，你要什么时候休息都可以。”</p>
<p>“我老是想休息。”点灯人说。</p>
<p>因为，一个人可以同时是忠实的，又是懒惰的。</p>
<p>小王子接着说：</p>
<p>“你的这颗行星这样小，你三步就可以绕它一圈。你只要慢慢地走，就可以 一直在太阳的照耀下，你想休息的时候，你就这样走……那么，你要白天又多长它 就有多长。”</p>
<p>“这办法帮不了我多打忙，生活中我喜欢的就是睡觉。”点灯人说。</p>
<p>“真不走运。”小王子说。</p>
<p>“真不走运。”点灯人说。“早上好。”</p>
<p>于是他又熄灭了路灯。<br>小王子在他继续往前旅行的途中，自言自语地说道：</p>
<p>“这个人一定会被其他那些人，国王呀，爱虚荣的呀，酒鬼呀，实业家呀， 所瞧不起。可是唯有他不使我感到荒唐可笑。这可能是因为他所关心的是别的事， 而不是他自己。”</p>
<p>他惋惜地叹了口气，并且又对自己说道：</p>
<p>“本来这是我唯一可以和他交成朋友的人。可是他的星球确实太小了，住不 下两个人……”</p>
<p>小王子没有勇气承认的是：他留恋这颗令人赞美的星星，特别是因为在那里 每二十四小时就有一千四百四十次日落！</p>
]]></content>
      <categories>
        <category>小王子</category>
      </categories>
  </entry>
  <entry>
    <title>《小王子》—XIX</title>
    <url>/2021/02/26/%E3%80%8A%E5%B0%8F%E7%8E%8B%E5%AD%90%E3%80%8B%E2%80%94XIX/</url>
    <content><![CDATA[<h1 id="《小王子》—XIX"><a href="#《小王子》—XIX" class="headerlink" title="《小王子》—XIX"></a>《小王子》—XIX</h1><h5 id="英文音频：https-www-bilibili-com-video-BV1H64y1f73s-p-20"><a href="#英文音频：https-www-bilibili-com-video-BV1H64y1f73s-p-20" class="headerlink" title="英文音频：https://www.bilibili.com/video/BV1H64y1f73s?p=20"></a>英文音频：<a href="https://www.bilibili.com/video/BV1H64y1f73s?p=20">https://www.bilibili.com/video/BV1H64y1f73s?p=20</a></h5><hr>
<p>小王子爬上一座高山。过去他所见过的山就是那三座只有他膝盖那么高的火 山，并且他把那座熄灭了的火山就当作凳子。小王子自言自语地说道：“从这么 高的山上，我一眼可以看到整个星球，以及所有的人。”可是，他所看到的只是 一些非常锋利的悬崖峭壁。</p>
<p><img src="/2021/02/26/%E3%80%8A%E5%B0%8F%E7%8E%8B%E5%AD%90%E3%80%8B%E2%80%94XIX/xwz34.jpg" alt="小王子爬上一座高山。"></p>
<p>“你好。”小王子试探地问道。</p>
<p>“你好……你好……你好……”回音在回答道。</p>
<p>“你们是什么人？”小王子问。</p>
<p>“你们是什么人……你们是什么人……你们是什么人……”回音又回答道。<br>“请你们做我的朋友吧，我很孤独。”他说。</p>
<p>“我很孤独……我很孤独……我很孤独……”回音又回答着。</p>
<p>小王子想道：“这颗行星真奇怪！它上面全是干巴巴的，而且又尖利又咸涩， 人们一点想象力都没有。他们只是重复别人对他们说的话……在我的家乡，我有一 朵花。她总是自己先说话……”</p>
]]></content>
      <categories>
        <category>小王子</category>
      </categories>
  </entry>
  <entry>
    <title>《小王子》—XV</title>
    <url>/2021/02/26/%E3%80%8A%E5%B0%8F%E7%8E%8B%E5%AD%90%E3%80%8B%E2%80%94XV/</url>
    <content><![CDATA[<h1 id="《小王子》—XV"><a href="#《小王子》—XV" class="headerlink" title="《小王子》—XV"></a>《小王子》—XV</h1><h5 id="英文音频：https-www-bilibili-com-video-BV1H64y1f73s-p-16"><a href="#英文音频：https-www-bilibili-com-video-BV1H64y1f73s-p-16" class="headerlink" title="英文音频：https://www.bilibili.com/video/BV1H64y1f73s?p=16"></a>英文音频：<a href="https://www.bilibili.com/video/BV1H64y1f73s?p=16">https://www.bilibili.com/video/BV1H64y1f73s?p=16</a></h5><hr>
<p>第六颗行星则要大十倍。上面住着一位老先生，他在写作大部头的书。</p>
<p><img src="/2021/02/26/%E3%80%8A%E5%B0%8F%E7%8E%8B%E5%AD%90%E3%80%8B%E2%80%94XV/xwz30.jpg" alt="地理学家"></p>
<p>“瞧！来了一位探险家。”老先生看到小王子时，叫了起来。</p>
<p>小王子在桌旁坐下，有点气喘吁吁。他跑了多少路啊！</p>
<p>“你从哪里来的呀？”老先生问小王子。</p>
<p>“这一大本是什么书？你在这里干什么？”小王子问道。</p>
<p>“我是地理学家。”老先生答道。</p>
<p>“什么是地理学家？”</p>
<p>“地理学家，就是一种学者，他知道哪里有海洋，哪里有江河、城市、山脉、 沙漠。”</p>
<p>“这倒挺有意思。”小王子说。“这才是一种真正的行当。”他朝四周围看 了看这位地理学家的星球。他还从来没有见过一颗如此壮观的行星。</p>
<p>“您的星球真美呀。上面有海洋吗？”</p>
<p>“这我没法知道。”地理学家说。</p>
<p>“啊！”小王子大失所望。“那么，山脉呢？”</p>
<p>“这，我没法知道。”地理学家说。</p>
<p>“那么，有城市、河流、沙漠吗？”<br>“这，我也没法知道。”地理学家说。</p>
<p>“可您还是地理学家呢！”</p>
<p>“一点不错，”地理学家说，“但是我不是探察家。我手下一个探察家都没 有。地理学家是不去计算城市、河流、山脉、海洋、沙漠的。地理学家很重要， 不能到处跑。他不能离开他的办公室。但他可以在办公室里接见探察家。他询问 探察家，把他们的回忆记录下来。如果他认为其中有个探察家的回忆是有意思的， 那么地理学家就对这个探察家的品德做一番调查。”</p>
<p>“这是为什么呢？”</p>
<p>“因为一个说假话的探察家会给地理书带来灾难性的后果。同样，一个太爱 喝酒的探察家也是如此。”</p>
<p>“这又是为什么？”小王子说。</p>
<p>“因为喝醉了酒的人把一个看成两个，那么，地理学家就会把只有一座山的 地方写成两座山。”</p>
<p>“我认识一个人，他要是搞探察的话，就很可能是个不好的探察员。”小王 子说。</p>
<p>“这是可能的。因此，如果探察家的品德不错，就对他的发现进行调查。”</p>
<p>“去看一看吗？”</p>
<p>“不。那太复杂了。但是要求探察家提出证据来。例如，假使他发现了一座 大山，就要求他带来一些大石头。”</p>
<p>地理学家忽然忙乱起来。</p>
<p>“正好，你是从老远来的么！你是个探察家！你来给我介绍一下你的星球吧！”</p>
<p>于是，已经打开登记簿的地理学家，削起他的铅笔来。他首先是用铅笔记下 探察家的叙述，等到探察家提出了证据以后再用墨水笔记下来。</p>
<p>“怎么样？”地理学家询问道。</p>
<p>“啊！我那里，”小王子说道，“没有多大意思，那儿很小。我有三座火山， 两座是活的，一座是熄灭了的。但是也很难说。”</p>
<p>“很难说。”地理学家说道。</p>
<p>“我还有一朵花。”</p>
<p>“我们是不记载花卉的。”地理学家说。</p>
<p>“这是为什么？花是最美丽的东西。”</p>
<p>“因为花卉是短暂的。”</p>
<p>“什么叫短暂？”</p>
<p>“地理学书籍是所有书中最严肃的书。”地理学家说道，“这类书是从不会 过时的。很少会发生一座山变换了位置，很少会出现一个海洋干涸的现象。我们 要写永恒的东西。”</p>
<p>“但是熄灭的火山也可能会再复苏的。”小王子打断了地理学家。“什么叫 短暂？”</p>
<p>“火山是熄灭了的也好，苏醒的也好，这对我们这些人来讲都是一回事。” 地理学家说，“对我们来说，重要的是山。山是不会变换位置的。”</p>
<p>“但是，‘短暂’是什么意思？”小王子再三地问道。他一旦提出一个问题 是从不放过的。</p>
<p>“意思就是：有很快就会消失的危险。”</p>
<p>“我的花是很快就会消失的吗？”</p>
<p>“那当然。”</p>
<p>小王子自言自语地说：“我的花是短暂的，而且她只有四根刺来防御外侮！ 可我还把她独自留在家里！”</p>
<p>这是他第一次产生了后悔，但他又重新振作起来：</p>
<p>“您是否能建议我去看些什么？”小王子问道。</p>
<p>“地球这颗行星，”地理学家回答他说，“它的名望很高……”</p>
<p>于是小王子就走了，他一边走一边想着他的花。</p>
]]></content>
      <categories>
        <category>小王子</category>
      </categories>
  </entry>
  <entry>
    <title>《小王子》—XVI</title>
    <url>/2021/02/26/%E3%80%8A%E5%B0%8F%E7%8E%8B%E5%AD%90%E3%80%8B%E2%80%94XVI/</url>
    <content><![CDATA[<h1 id="《小王子》—XVI"><a href="#《小王子》—XVI" class="headerlink" title="《小王子》—XVI"></a>《小王子》—XVI</h1><h5 id="英文音频：https-www-bilibili-com-video-BV1H64y1f73s-p-17"><a href="#英文音频：https-www-bilibili-com-video-BV1H64y1f73s-p-17" class="headerlink" title="英文音频：https://www.bilibili.com/video/BV1H64y1f73s?p=17"></a>英文音频：<a href="https://www.bilibili.com/video/BV1H64y1f73s?p=17">https://www.bilibili.com/video/BV1H64y1f73s?p=17</a></h5><hr>
<p>第七个行星，于是就是地球了。</p>
<p>地球可不是一颗普通的行星！它上面有一百一十一个国王（当然，没有漏掉 黑人国王），七千个地理学家，九十万个实业家，七百五十万个酒鬼，三亿一千 一百万个爱虚荣的人，也就是说，大约有二十亿的大人。</p>
<p>为了使你们对地球的大小有一个概念，我想要告诉你们：在发明电之前，在 六的大洲上，为了点路灯，需要维持一支为数四十六万二千五百一十一人的真正 大军。</p>
<p>从稍远的地方看过去，它给人以一种壮丽辉煌的印象。这支军队的行动就象 歌剧院的芭蕾舞动作一样，那么有条不紊。首先出现的是新西兰和澳大利亚的点 灯人。点着了灯，随后他们就去睡觉了。于是就轮到中国和西伯利亚的点灯人走 上舞台。随后，他们也藏到幕布后面去了。于是就又轮到俄罗斯和印度的点灯人 了。然后就是非洲和欧洲的。接着是南美的，再就是北美的。他们从来也不会搞 错他们上场的次序。真了不起。</p>
<p>北极仅有一盏路灯，南极也只有一盏；唯独北极的点灯人和他南极的同行， 过着闲逸、懒散的生活：他们每年只工作两次。</p>
]]></content>
      <categories>
        <category>小王子</category>
      </categories>
  </entry>
  <entry>
    <title>《小王子》—XVII</title>
    <url>/2021/02/26/%E3%80%8A%E5%B0%8F%E7%8E%8B%E5%AD%90%E3%80%8B%E2%80%94XVII/</url>
    <content><![CDATA[<h1 id="《小王子》—XVII"><a href="#《小王子》—XVII" class="headerlink" title="《小王子》—XVII"></a>《小王子》—XVII</h1><h5 id="英文音频：https-www-bilibili-com-video-BV1H64y1f73s-p-18"><a href="#英文音频：https-www-bilibili-com-video-BV1H64y1f73s-p-18" class="headerlink" title="英文音频：https://www.bilibili.com/video/BV1H64y1f73s?p=18"></a>英文音频：<a href="https://www.bilibili.com/video/BV1H64y1f73s?p=18">https://www.bilibili.com/video/BV1H64y1f73s?p=18</a></h5><hr>
<p>当人们想要说得俏皮些的时候，说话就可能会不大实在。在给你们讲点灯人 的时候，我就不那么忠实，很可能给不了解我们这个星球的人们造成一个错误的 概念。在地球上，人们所占的位置非常小。如果住在地球上的二十亿居民全站着， 并且象开大会一样靠得紧些，那么就可以从容地站在一个二十海里见方的广场上。 也就是说可以把整个人类集中在太平洋中一个最小的岛屿上。</p>
<p>当然，大人们是不会相信你们的。他们自以为要占很大地方，他们把自己看 得象猴面包树那样大得了不起。你们可以建议他们计算一下。这样会使他们很高 兴，因为他们非常喜欢数目字。可是你们无须浪费时间去做这种乏味的连篇累牍 的演算。这没有必要。你们可以完全相信我。</p>
<p><img src="/2021/02/26/%E3%80%8A%E5%B0%8F%E7%8E%8B%E5%AD%90%E3%80%8B%E2%80%94XVII/xwz31.jpg" alt="《小王子》"></p>
<p>小王子到了地球上感到非常奇怪，他一个人也没有看到，他正担心自己跑错 了星球。这时，在沙地上有一个月光色的圆环在蠕动。</p>
<p><img src="/2021/02/26/%E3%80%8A%E5%B0%8F%E7%8E%8B%E5%AD%90%E3%80%8B%E2%80%94XVII/xwz32.jpg" alt="小王子和蛇"></p>
<p>小王子毫无把握地随便说了声：“晚安。”</p>
<p>“晚安。”蛇说道。</p>
<p>“我落在什么行星上？”小王子问道。</p>
<p>“在地球上，在非洲。”蛇回答道。</p>
<p>“啊！……怎么，难道说地球上没有人吗？”<br>“这里是沙漠，沙漠中没有人。地球是很大的。”蛇说。</p>
<p>小王子坐在一块石头上，抬眼望着天空，说道：</p>
<p>“我捉摸这些星星闪闪发亮是否为了让每个人将来有一天都能重新找到自己 的星球。看，我那颗行星。它恰好在我们头顶上……可是，它离我们好远哟！”</p>
<p>“它很美。”蛇说，“你到这里来干什么呢？”</p>
<p>“我和一朵花闹了别扭。”小王子说。</p>
<p>“啊！”蛇说道。</p>
<p>于是他们都沉默下来。</p>
<p>“人在什么地方？”小王子终于又开了腔。“在沙漠上，真有点孤独……”</p>
<p>“到了有人的地方，也一样孤独。”蛇说。</p>
<p>小王子长时间地看着蛇。</p>
<p>“你是个奇怪的动物，细得象个手指头……。”小王子终于说道。</p>
<p>“但我比一个国王的手指更有威力。”蛇说道。</p>
<p>小王子微笑着说：</p>
<p>“你并不那么有威力……你连脚都没有……你甚至都不能旅行……”</p>
<p>“我可以把你带到很远的地方去，比一只船能去的地方还要远。”蛇说道。</p>
<p>蛇就盘结在小王子的脚腕子上，象一只金镯子。</p>
<p>“被我碰触的人，我就把他送回老家去。”蛇还说，“可是你是纯洁的，而 且是从另一个星球上来的……”</p>
<p>小王子什么也没有回答。</p>
<p>“在这个花岗石的地球上，你这么弱小，我很可怜你。如果你非常怀念你的 星球，那时我可以帮助你。我可以……”</p>
<p>“啊！我很明白你的意思。”小王子说，“但是你为什么说话总是象让人猜 谜语似的？”</p>
<p>“这些谜语我都能解开的。”蛇说。</p>
<p>于是他们又都沉默起来。</p>
]]></content>
      <categories>
        <category>小王子</category>
      </categories>
  </entry>
  <entry>
    <title>《小王子》—XVIII</title>
    <url>/2021/02/26/%E3%80%8A%E5%B0%8F%E7%8E%8B%E5%AD%90%E3%80%8B%E2%80%94XVIII/</url>
    <content><![CDATA[<h1 id="《小王子》—XVIII"><a href="#《小王子》—XVIII" class="headerlink" title="《小王子》—XVIII"></a>《小王子》—XVIII</h1><h5 id="英文音频：https-www-bilibili-com-video-BV1H64y1f73s-p-19"><a href="#英文音频：https-www-bilibili-com-video-BV1H64y1f73s-p-19" class="headerlink" title="英文音频：https://www.bilibili.com/video/BV1H64y1f73s?p=19"></a>英文音频：<a href="https://www.bilibili.com/video/BV1H64y1f73s?p=19">https://www.bilibili.com/video/BV1H64y1f73s?p=19</a></h5><hr>
<p>小王子穿过沙漠。他只见过一朵花，一个有着三枚花瓣的花朵，一朵很不起 眼的小花……</p>
<p><img src="/2021/02/26/%E3%80%8A%E5%B0%8F%E7%8E%8B%E5%AD%90%E3%80%8B%E2%80%94XVIII/xwz33.jpg" alt="一个有着三枚花瓣的花朵"></p>
<p>“你好。”小王子说。</p>
<p>“你好。”花说。</p>
<p>“人在什么地方？”小王子有礼貌地问道。</p>
<p>有一天，花曾看见一支骆驼商队走过：</p>
<p>“人吗？我想大约有六七个人，几年前，我瞅见过他们。可是，从来不知道 到什么地方去找他们。风吹着他们到处跑。他们没有根，这对他们来说是很不方 便的。”</p>
<p>“再见了。”小王子说。</p>
<p>“再见。”花说。</p>
]]></content>
      <categories>
        <category>小王子</category>
      </categories>
  </entry>
  <entry>
    <title>《小王子》—XX</title>
    <url>/2021/02/26/%E3%80%8A%E5%B0%8F%E7%8E%8B%E5%AD%90%E3%80%8B%E2%80%94XX/</url>
    <content><![CDATA[<h1 id="《小王子》—XX"><a href="#《小王子》—XX" class="headerlink" title="《小王子》—XX"></a>《小王子》—XX</h1><h5 id="英文音频：https-www-bilibili-com-video-BV1H64y1f73s-p-21"><a href="#英文音频：https-www-bilibili-com-video-BV1H64y1f73s-p-21" class="headerlink" title="英文音频：https://www.bilibili.com/video/BV1H64y1f73s?p=21"></a>英文音频：<a href="https://www.bilibili.com/video/BV1H64y1f73s?p=21">https://www.bilibili.com/video/BV1H64y1f73s?p=21</a></h5><hr>
<p>在沙漠、岩石、雪地上行走了很长的时间以后，小王子终于发现了一条大路。 所有的大路都是通往人住的地方的。<br>“你们好。”小王子说。<br>这是一个玫瑰盛开的花园。</p>
<p><img src="/2021/02/26/%E3%80%8A%E5%B0%8F%E7%8E%8B%E5%AD%90%E3%80%8B%E2%80%94XX/xwz35.jpg" alt="小王子和玫瑰花"></p>
<p>“你好。”玫瑰花说道。</p>
<p>小王子瞅着这些花，它们全都和他的那朵花一样。</p>
<p>“你们是什么花？”小王子惊奇地问。</p>
<p>“我们是玫瑰花。”花儿们说道。</p>
<p>“啊！”小王子说……。</p>
<p>他感到自己非常不幸。他的那朵花曾对他说她是整个宇宙中独一无二的一种 花。可是，仅在这一座花园里就有五千朵完全一样的这种花朵！</p>
<p>小王子自言自语地说：“如果她看到这些，她是一定会很恼火……她会咳嗽得 更厉害，并且为避免让人耻笑，她会佯装死去。那么，我还得装着去护理她，因 为如果不这样的话，她为了使我难堪，她可能会真的死去……”</p>
<p><img src="/2021/02/26/%E3%80%8A%E5%B0%8F%E7%8E%8B%E5%AD%90%E3%80%8B%E2%80%94XX/xwz36.jpg" alt="《小王子》童话小说图片"></p>
<p>接着他又说道：“我还以为我有一朵独一无二的花呢，我有的仅是一朵普通 的花。这朵花，再加上三座只有我膝盖那么高的火山，而且其中一座还可能是永 远熄灭了的，这一切不会使我成为一个了不起的王子……”于是，他躺在草丛中哭 泣起来。</p>
]]></content>
      <categories>
        <category>小王子</category>
      </categories>
  </entry>
  <entry>
    <title>《小王子》—XXI</title>
    <url>/2021/02/26/%E3%80%8A%E5%B0%8F%E7%8E%8B%E5%AD%90%E3%80%8B%E2%80%94XXI/</url>
    <content><![CDATA[<h1 id="《小王子》—XXI"><a href="#《小王子》—XXI" class="headerlink" title="《小王子》—XXI"></a>《小王子》—XXI</h1><h5 id="英文音频：https-www-bilibili-com-video-BV1H64y1f73s-p-22"><a href="#英文音频：https-www-bilibili-com-video-BV1H64y1f73s-p-22" class="headerlink" title="英文音频：https://www.bilibili.com/video/BV1H64y1f73s?p=22"></a>英文音频：<a href="https://www.bilibili.com/video/BV1H64y1f73s?p=22">https://www.bilibili.com/video/BV1H64y1f73s?p=22</a></h5><hr>
<p>就在这当儿，跑来了一只狐狸。</p>
<p>“你好。”狐狸说。</p>
<p><img src="/2021/02/26/%E3%80%8A%E5%B0%8F%E7%8E%8B%E5%AD%90%E3%80%8B%E2%80%94XXI/xwz37.jpg" alt="小王子和狐狸"></p>
<p>“你好。”小王子很有礼貌地回答道。他转过身来，但什么也没有看到。</p>
<p>“我在这儿，在苹果树下。”那声音说。</p>
<p>“你是谁？”小王子说，“你很漂亮。”</p>
<p>“我是一只狐狸。”狐狸说。</p>
<p>“来和我一起玩吧，”小王子建议道，“我很苦恼……”</p>
<p>“我不能和你一起玩，”狐狸说，“我还没有被驯服呢。”</p>
<p>“啊！真对不起。”小王子说。</p>
<p>思索了一会儿，他又说道：</p>
<p>“什么叫‘驯服’呀？”</p>
<p>“你不是此地人。”狐狸说，“你来寻找什么？”</p>
<p>“我来找人。”小王子说，“什么叫‘驯服’呢？”</p>
<p>“人，”狐狸说，“他们有枪，他们还打猎，这真碍事！他们唯一的可取之 处就是他们也养鸡，你是来寻找鸡的吗？”</p>
<p>“不，”小王子说，“我是来找朋友的。什么叫‘驯服’呢？”</p>
<p>“这是已经早就被人遗忘了的事情，”狐狸说，“它的意思就是‘建立联系’。”</p>
<p>“建立联系？”</p>
<p>“一点不错，”狐狸说。“对我来说，你还只是一个小男孩，就像其他千万 个小男孩一样。我不需要你。你也同样用不着我。对你来说，我也不过是一只狐 狸，和其他千万只狐狸一样。但是，如果你驯服了我，我们就互相不可缺少了。 对我来说，你就是世界上唯一的了；我对你来说，也是世界上唯一的了。”</p>
<p>“我有点明白了。”小王子说，“有一朵花……，我想，她把我驯服了……”</p>
<p>“这是可能的。”狐狸说，“世界上什么样的事都可能看到……”</p>
<p>“啊，这不是在地球上的事。”小王子说。</p>
<p>狐狸感到十分蹊跷。</p>
<p>“在另一个星球上？”</p>
<p>“是的。”</p>
<p>“在那个星球上，有猎人吗？”</p>
<p>“没有。”</p>
<p>“这很有意思。那么，有鸡吗？”</p>
<p>“没有。”</p>
<p>“没有十全十美的。”狐狸叹息地说道。</p>
<p><img src="/2021/02/26/%E3%80%8A%E5%B0%8F%E7%8E%8B%E5%AD%90%E3%80%8B%E2%80%94XXI/xwz38.jpg" alt="《小王子》中的猎人"></p>
<p>可是，狐狸又把话题拉回来：</p>
<p>“我的生活很单调。我捕捉鸡，而人又捕捉我。所有的鸡全都一样，所有的 人也全都一样。因此，我感到有些厌烦了。但是，如果你要是驯服了我，我的生 活就一定会是欢快的。我会辨认出一种与众不同的脚步声。其他的脚步声会使我 躲到地下去，而你的脚步声就会象音乐一样让我从洞里走出来。再说，你看！你 看到那边的麦田没有？我不吃面包，麦子对我来说，一点用也没有。我对麦田无 动于衷。而这，真使人扫兴。但是，你有着金黄色的头发。那么，一旦你驯服了 我，这就会十分美妙。麦子，是金黄色的，它就会使我想起你。而且，我甚至会 喜欢那风吹麦浪的声音……”</p>
<p>狐狸沉默不语，久久地看着小王子。</p>
<p>“请你驯服我吧！”他说。</p>
<p>“我是很愿意的。”小王子回答道，“可我的时间不多了。我还要去寻找朋 友，还有许多事物要了解。”</p>
<p>“只有被驯服了的事物，才会被了解。”狐狸说，“人不会再有时间去了解 任何东西的。他们总是到商人那里去购买现成的东西。因为世界上还没有购买朋 友的商店，所以人也就没有朋友。如果你想要一个朋友，那就驯服我吧！”</p>
<p>“那么应当做些什么呢？”小王子说。</p>
<p>“应当非常耐心。”狐狸回答道，“开始你就这样坐在草丛中，坐得离我稍 微远些。我用眼角瞅着你，你什么也不要说。话语是误会的根源。但是，每天， 你坐得靠我更近些……”</p>
<p>第二天，小王子又来了。</p>
<p>“最好还是在原来的那个时间来。”狐狸说道，“比如说，你下午四点钟来， 那么从三点钟起，我就开始感到幸福。时间越临近，我就越感到幸福。到了四点 钟的时候，我就会坐立不安；我就会发现幸福的代价。但是，如果你随便什么时 候来，我就不知道在什么时候该准备好我的心情……应当有一定的仪式。”</p>
<p>“仪式是什么？”小王子问道。</p>
<p>“这也是一种早已被人忘却了的事。”狐狸说，“它就是使某一天与其他日 子不同，使某一时刻与其他时刻不同。比如说，我的那些猎人就有一种仪式。他 们每星期四都和村子里的姑娘们跳舞。于是，星期四就是一个美好的日子！我可 以一直散步到葡萄园去。如果猎人们什么时候都跳舞，天天又全都一样，那么我 也就没有假日了。”</p>
<p>就这样，小王子驯服了狐狸。当出发的时刻就快要来到时：</p>
<p>“啊！”狐狸说，“我一定会哭的。”</p>
<p>“这是你的过错，”小王子说，“我本来并不想给你任何痛苦，可你却要我驯 服你……”</p>
<p>“是这样的。”狐狸说。</p>
<p>“你可就要哭了！”小王子说。</p>
<p>“当然罗。”狐狸说。</p>
<p>“那么你什么好处也没得到。”</p>
<p>“由于麦子颜色的缘故，我还是得到了好处。”狐狸说。</p>
<p>然后，他又接着说。</p>
<p>“再去看看那些玫瑰花吧。你一定会明白，你的那朵是世界上独一无二的玫 瑰。你回来和我告别时，我再赠送给你一个秘密。”</p>
<p>于是小王子又去看那些玫瑰。</p>
<p>“你们一点也不象我的那朵玫瑰，你们还什么都不是呢！”小王子对她们说。 “没有人驯服过你们，你们也没有驯服过任何人。你们就象我的狐狸过去那样， 它那时只是和千万只别的狐狸一样的一只狐狸。但是，我现在已经把它当成了我 的朋友，于是它现在就是世界上独一无二的了。”</p>
<p>这时，那些玫瑰花显得十分难堪。</p>
<p>“你们很美，但你们是空虚的。”小王子仍然在对她们说，“没有人能为你 们去死。当然罗，我的那朵玫瑰花，一个普通的过路人以为她和你们一样。可是， 她单独一朵就比你们全体更重要，因为她是我浇灌的。因为她是我放在花罩中的。 因为她是我用屏风保护起来的。因为她身上的毛虫（除了留下两三只为了变蝴蝶 而外）是我除灭的。因为我倾听过她的怨艾和自诩，甚至有时我聆听着她的沉默。 因为她是我的玫瑰。”</p>
<p><img src="/2021/02/26/%E3%80%8A%E5%B0%8F%E7%8E%8B%E5%AD%90%E3%80%8B%E2%80%94XXI/xwz39.jpg" alt="小说《小王子》的狐狸"></p>
<p>他又回到了狐狸身边。</p>
<p>“再见了。”小王子说道。</p>
<p>“再见。”狐狸说。“喏，这就是我的秘密。很简单：只有用心才能看得清。 实质性的东西，用眼睛是看不见的。”</p>
<p>“实质性的东西，用眼睛是看不见的。”小王子重复着这句话，以便能把它 记在心间。</p>
<p>“正因为你为你的玫瑰花费了时间，这才使你的玫瑰变得如此重要。”</p>
<p>“正因为你为你的玫瑰花费了时间……”小王子又重复着，要使自己记住这些。</p>
<p>“人们已经忘记了这个道理，”狐狸说，“可是，你不应该忘记它。你现在 要对你驯服过的一切负责到底。你要对你的玫瑰负责……”</p>
<p>“我要对我的玫瑰负责……”小王子又重复着……</p>
]]></content>
      <categories>
        <category>小王子</category>
      </categories>
  </entry>
  <entry>
    <title>《小王子》—XXII</title>
    <url>/2021/02/26/%E3%80%8A%E5%B0%8F%E7%8E%8B%E5%AD%90%E3%80%8B%E2%80%94XXII/</url>
    <content><![CDATA[<h1 id="《小王子》—XXII"><a href="#《小王子》—XXII" class="headerlink" title="《小王子》—XXII"></a>《小王子》—XXII</h1><h5 id="英文音频：https-www-bilibili-com-video-BV1H64y1f73s-p-23"><a href="#英文音频：https-www-bilibili-com-video-BV1H64y1f73s-p-23" class="headerlink" title="英文音频：https://www.bilibili.com/video/BV1H64y1f73s?p=23"></a>英文音频：<a href="https://www.bilibili.com/video/BV1H64y1f73s?p=23">https://www.bilibili.com/video/BV1H64y1f73s?p=23</a></h5><hr>
<p>“你好。”小王子说道。</p>
<p>“你好。”扳道工说道。</p>
<p>“你在这里做什么？”小王子问。</p>
<p>“我一包包地分选旅客，按每千人一包。”扳道工说，“我打发这些运载旅 客的列车，一会儿发往右方，一会儿发往左方。”</p>
<p>这时，一列灯火明亮的快车，雷鸣般地响着，把扳道房震得颤颤悠悠。</p>
<p>“他们真匆忙呀，”小王子说，“他们要寻找什么？”</p>
<p>“开机车的人自己也不知道。”扳道工说道。</p>
<p>于是，第二列灯火通明的快车又朝着相反的方向轰隆轰隆地开过去。</p>
<p>“他们怎么又回来了呢？”小王子问道。</p>
<p>“他们不是原来那些人了。”扳道工说，“这是一次对开列车。”</p>
<p>“他们不满意他们原来所住的地方吗？”</p>
<p>“人们是从来也不会满意自己所在的地方的。”扳道工说。</p>
<p>此时，第三趟灯火明亮的快车又隆隆而过。</p>
<p>“他们是在追随第一批旅客吗？”小王子问道。</p>
<p>“他们什么也不追随。”扳道工说，“他们在里面睡觉，或是在打哈欠。只 有孩子们把鼻子贴在玻璃窗上往外看。”</p>
<p>“只有孩子知道他们自己在寻找什么。”小王子说，“他们为一个布娃娃花 费不少时间，这个布娃娃就成了很重要的东西，如果有人夺走的他们的布娃娃， 他们就哭泣……”</p>
<p>“他们真幸运。”扳道工说。</p>
]]></content>
      <categories>
        <category>小王子</category>
      </categories>
  </entry>
  <entry>
    <title>《小王子》—XXIII</title>
    <url>/2021/02/26/%E3%80%8A%E5%B0%8F%E7%8E%8B%E5%AD%90%E3%80%8B%E2%80%94XXIII/</url>
    <content><![CDATA[<h1 id="《小王子》—XXIII"><a href="#《小王子》—XXIII" class="headerlink" title="《小王子》—XXIII"></a>《小王子》—XXIII</h1><h5 id="英文音频：https-www-bilibili-com-video-BV1H64y1f73s-p-24"><a href="#英文音频：https-www-bilibili-com-video-BV1H64y1f73s-p-24" class="headerlink" title="英文音频：https://www.bilibili.com/video/BV1H64y1f73s?p=24"></a>英文音频：<a href="https://www.bilibili.com/video/BV1H64y1f73s?p=24">https://www.bilibili.com/video/BV1H64y1f73s?p=24</a></h5><hr>
<p>“你好。”小王子说。</p>
<p>“你好。”商人说道。</p>
<p>这是一位贩卖能够止渴的精制药丸的商人。每周吞服一丸就不会感觉口渴。</p>
<p>“你为什么卖这玩艺儿？”小王子说。<br>“这就大大地节约了时间。”商人说，“专家们计算过，这样，每周可以节 约五十三分钟。”</p>
<p>“那么，用这五十三分钟做什么用？”</p>
<p>“随便怎么用都行。……”</p>
<p>小王子自言自语地说：“我如果有五十三分钟可支配，我就悠哉游哉地向水 泉走去……”</p>
]]></content>
      <categories>
        <category>小王子</category>
      </categories>
  </entry>
  <entry>
    <title>《小王子》—XXIV</title>
    <url>/2021/02/26/%E3%80%8A%E5%B0%8F%E7%8E%8B%E5%AD%90%E3%80%8B%E2%80%94XXIV/</url>
    <content><![CDATA[<h1 id="《小王子》—XXIV"><a href="#《小王子》—XXIV" class="headerlink" title="《小王子》—XXIV"></a>《小王子》—XXIV</h1><h5 id="英文音频：https-www-bilibili-com-video-BV1H64y1f73s-p-25"><a href="#英文音频：https-www-bilibili-com-video-BV1H64y1f73s-p-25" class="headerlink" title="英文音频：https://www.bilibili.com/video/BV1H64y1f73s?p=25"></a>英文音频：<a href="https://www.bilibili.com/video/BV1H64y1f73s?p=25">https://www.bilibili.com/video/BV1H64y1f73s?p=25</a></h5><hr>
<p>这是我在沙漠上出了事故的第八天。我听着有关这个商人的故事，喝完了我 所备用的最后一滴水。</p>
<p>“啊！”我对小王子说，“你回忆的这些故事真美。可是，我还没有修好我 的飞机。我没有喝的了，假如我能悠哉游哉地走到水泉边去，我一定也会很高兴 的！”</p>
<p>小王子对我说：“我的朋友狐狸……”</p>
<p>“我的小家伙，现在还说什么狐狸！”</p>
<p>“为什么？”</p>
<p>“因为这就要渴死人了。”</p>
<p>他不理解我的思路，他回答我道：</p>
<p>“即使快要死了，有过一个朋友也好么！我就为我有过一个狐狸朋友而感到 很高兴……”</p>
<p>“他不顾危险。”我自己思量着，“他从来不知道饥渴。只要有点阳光，他 就满足了……”</p>
<p>他看着我，答复着我的思想：</p>
<p>“我也渴了……我们去找一口井吧……”</p>
<p>我显出厌烦的样子：在茫茫的大沙漠上盲目地去找水井，真荒唐。然而我们 还是开始去寻找了。</p>
<p>当我们默默地走了好几个小时以后，天黑了下来，星星开始发出光亮。由于 渴我有点发烧，我看着这些星星，象是在做梦一样。小王子的话在我的脑海中跳 来跳去。</p>
<p>“你也渴吗？”我问他。<br>他却不回答我的问题，只是对我说：</p>
<p>“水对心也是有益处的……”</p>
<p>我不懂他的话是什么意思，可我也不做声……我知道不应该去问他。</p>
<p>他累了，他坐下来。我在他身旁坐下。沉默了一会，他又说道：</p>
<p>“星星是很美的，因为有一朵人们看不到的花……”</p>
<p>我回答道：“当然。”而我默默地看着月光下沙漠的褶皱。</p>
<p>“沙漠是美的。”他又说道。</p>
<p>确实如此。我一直很喜欢沙漠。坐在一个沙丘上，什么也看不见、听不见。 但是，却有一种说不出的东西在默默地放着光芒……</p>
<p>“使沙漠更加美丽的，就是在某个角落里，藏着一口井……”</p>
<p><img src="/2021/02/26/%E3%80%8A%E5%B0%8F%E7%8E%8B%E5%AD%90%E3%80%8B%E2%80%94XXIV/xwz40.jpg" alt="使沙漠更加美丽的，就是在某个角落里，藏着一口井"></p>
<p>我很惊讶，突然明白了为什么沙漠放着光芒。当我还是一个小孩子的时候， 我住在一座古老的房子里，而且传说，这个房子里埋藏着一个宝贝。当然，从来 没有任何人能发现这个宝贝，可能，甚至也没有人去寻找过。但是，这个宝贝使 整个房子着了魔似的。我家的房子在它的心灵深处隐藏着一个秘密……</p>
<p>我对小王子说道：“是的，无论是房子，星星，或是沙漠，使它们美丽的东 西是看不见的！”<br>“我真高兴，你和我的狐狸的看法一样。”小王子说。</p>
<p>小王子睡觉了，我就把他抱在怀里，又重新上路了。我很激动。就好象抱着 一个脆弱的宝贝。就好象在地球上没有比这更脆弱的了。我借着月光看着这惨白 的面额，这双紧闭的眼睛，这随风飘动的绺绺头发，这时我对自己说道：“我所 看到的仅仅是外表。最重要的是看不见的……”</p>
<p>由于看到他稍稍张开的嘴唇露出一丝微笑，我又自言自语地说：“在这个熟 睡了的小王子身上，使我非常感动的，是他对他那朵花的忠诚，是在他心中闪烁 的那朵玫瑰花的形象。这朵玫瑰花，即使在小王子睡着了的时候，也象一盏灯的 火焰一样在他身上闪耀着光辉……”这时，我就感觉到他更加脆弱。应该保护灯焰： 一阵风就可能把它吹灭……</p>
<p>于是，就这样走着，我在黎明时发现了水井。</p>
]]></content>
      <categories>
        <category>小王子</category>
      </categories>
  </entry>
  <entry>
    <title>《小王子》—XXV</title>
    <url>/2021/02/26/%E3%80%8A%E5%B0%8F%E7%8E%8B%E5%AD%90%E3%80%8B%E2%80%94XXV/</url>
    <content><![CDATA[<h1 id="《小王子》—XXV"><a href="#《小王子》—XXV" class="headerlink" title="《小王子》—XXV"></a>《小王子》—XXV</h1><h5 id="英文音频：https-www-bilibili-com-video-BV1H64y1f73s-p-26"><a href="#英文音频：https-www-bilibili-com-video-BV1H64y1f73s-p-26" class="headerlink" title="英文音频：https://www.bilibili.com/video/BV1H64y1f73s?p=26"></a>英文音频：<a href="https://www.bilibili.com/video/BV1H64y1f73s?p=26">https://www.bilibili.com/video/BV1H64y1f73s?p=26</a></h5><hr>
<p>“那些人们，他们往快车里拥挤，但是他们却不知道要寻找什么。于是，他 们就忙忙碌碌，来回转圈子……”小王子说道。</p>
<p>他接着又说：</p>
<p>“这没有必要……”</p>
<p>我们终于找到的这口井，不同于撒哈拉的那些井。撒哈拉的井只是沙漠中挖 的洞。这口井则很象村子中的井。可是，那里又没有任何村庄，我还以为是在做 梦呢。</p>
<p>“真怪，”我对小王子说：“一切都是现成的：辘轳、水桶、绳子……”</p>
<p>他笑了，拿着绳子，转动着辘轳。辘轳就象是一个长期没有风来吹动的旧风 标一样，吱吱作响。</p>
<p><img src="/2021/02/26/%E3%80%8A%E5%B0%8F%E7%8E%8B%E5%AD%90%E3%80%8B%E2%80%94XXV/xwz41.jpg" alt="《小王子》的水井"></p>
<p>“你听，”小王子说：“我们唤醒了这口井，它现在唱起歌来了……”我不愿 让他费劲。我对他说：</p>
<p>“让我来干吧。这活对你太重了。”</p>
<p>我慢慢地把水桶提到井栏上。我把它稳稳地放在那里。我的耳朵里还响着辘 轳的歌声。依然还在晃荡的水面上，我看见太阳的影子在跳动。</p>
<p>“我正需要喝这种水。”小王子说：“给我喝点……”</p>
<p>这时我才明白了他所要寻找的是什么！</p>
<p>我把水桶提到他的嘴边。他闭着眼睛喝水。就象节日一般舒适愉快。这水远 不只是一种饮料，它是披星戴月走了许多路才找到的，是在辘轳的歌声中，经过 我双臂的努力得来的。它象是一件礼品慰藉着心田。在我小的时候，圣诞树的灯 光，午夜的弥撒的音乐，甜蜜的微笑，这一切都使圣诞节时我收到的礼品辉映着 幸福的光彩。</p>
<p>“你这里的人在同一个花园中种植着五千朵玫瑰。”小王子说：“可是，他 们却不能从中找到自己所要寻找的东西……”</p>
<p>“他们是找不到的。”我回答道。</p>
<p>“然而，他们所寻找的东西却是可以从一朵玫瑰花或一点儿水中找到的……”</p>
<p>“一点不错。”我回答道。</p>
<p>小王子又加了一句：</p>
<p>“眼睛是什么也看不见的。应该用心去寻找。”</p>
<p>我喝了水。我痛快地呼吸着空气。沙漠在晨曦中泛出蜂蜜的光泽。这蜂蜜般 的光泽也使我感到幸福。为什么我要难过……</p>
<p>小王子又重新在我的身边坐下。他温柔地对我说：“你应该实践你的诺言。”</p>
<p>“什么诺言？”</p>
<p>“你知道……给我的小羊一个嘴套子……我要对我的花负责的呀！”</p>
<p>我从口袋中拿出我的画稿。小王子瞅见了，笑着说：</p>
<p>“你画的猴面包树，有点象白菜……”</p>
<p>“啊！”</p>
<p>我还为我画的猴面包树感到骄傲呢！</p>
<p>“你画的狐狸……它那双耳朵……有点象犄角……而且又太长了！”</p>
<p>这时，他又笑了。</p>
<p>“小家伙，你太不公正了。我过去只会画开着肚皮和闭着肚皮的巨蟒。”</p>
<p>“啊！这就行了。”他说：“孩子们认得出来。”</p>
<p>我就用铅笔勾画了一个嘴套。当我把它递给小王子时，我心里很难受：</p>
<p>“你的打算，我一点也不知道……”</p>
<p>但是，他不回答我，他对我说：</p>
<p>“你知道，我落在地球上……到明天就一周年了……”</p>
<p>接着，沉默了一会儿，他又说道：</p>
<p>“我就落在这附近……”</p>
<p>此时，他的面颊绯红。</p>
<p>我不知为什么，又感到一阵莫名其妙的心酸。这时，我产生了一个问题：</p>
<p>“一星期以前，我认识你的那天早上，你单独一个人在这旷无人烟的地方走 着；这么说，这并不是偶然的了？你是要回到你降落的地方去是吗！”</p>
<p>小王子的脸又红了。</p>
<p>我犹豫不定地又说了一句：</p>
<p>“可能是因为周年纪念吧？……”</p>
<p>小王子脸又红了。他从来也不回答这些问题，但是，脸红，就等于说“是的”， 是吧？</p>
<p>“啊！”我对他说：“我有点怕……”</p>
<p>但他却回答我说：</p>
<p>“你现在该工作了。你应该回到你的机器那里去。我在这里等你。你明天晚 上再来……”</p>
<p>但是，我放心不下。我想起了狐狸的话。如果被人驯服了，就可能会要哭的……</p>
]]></content>
      <categories>
        <category>小王子</category>
      </categories>
  </entry>
  <entry>
    <title>《小王子》—XXVI</title>
    <url>/2021/02/26/%E3%80%8A%E5%B0%8F%E7%8E%8B%E5%AD%90%E3%80%8B%E2%80%94XXVI/</url>
    <content><![CDATA[<h1 id="《小王子》—XXVI"><a href="#《小王子》—XXVI" class="headerlink" title="《小王子》—XXVI"></a>《小王子》—XXVI</h1><h5 id="英文音频：https-www-bilibili-com-video-BV1H64y1f73s-p-27"><a href="#英文音频：https-www-bilibili-com-video-BV1H64y1f73s-p-27" class="headerlink" title="英文音频：https://www.bilibili.com/video/BV1H64y1f73s?p=27"></a>英文音频：<a href="https://www.bilibili.com/video/BV1H64y1f73s?p=27">https://www.bilibili.com/video/BV1H64y1f73s?p=27</a></h5><hr>
<p>在井旁边有一堵残缺的石墙。第二天晚上我工作回来的时候，我远远地看见 了小王子耷拉着双腿坐在墙上。我听见他在说话：</p>
<p>“你怎么不记得了呢？”他说，“绝不是在这儿。”</p>
<p>大概还有另一个声音在回答他，因为他答着腔说道：</p>
<p>“没错，没错，日子是对的；但地点不是这里……”</p>
<p>我继续朝墙走去。我还是看不到，也听不见任何别人。可是小王子又回答道：</p>
<p>“……那当然。你会在沙上看到我的脚印是从什么地方开始的。你在那里等着 我就行了。今天夜里我去那里。”</p>
<p><img src="/2021/02/26/%E3%80%8A%E5%B0%8F%E7%8E%8B%E5%AD%90%E3%80%8B%E2%80%94XXVI/xwz42.jpg" alt="小王子和蛇"></p>
<p>我离墙约有二十米远，可我依然什么也没有看见。</p>
<p>小王子沉默了一会又说：</p>
<p>“你的毒液管用吗？你保证不会使我长时间地痛苦吗？”</p>
<p>我焦虑地赶上前去，但我仍然不明白是怎么回事。</p>
<p>“现在你去吧，我要下来了！……”小王子说。</p>
<p>于是，我也朝墙脚下看去，我吓了一跳。就在那里，一条黄蛇直起身子冲着 小王子。这种黄蛇半分钟就能结果你的性命。我一面赶紧掏口袋，拔出手枪，一 面跑过去。可是一听到我的脚步声，蛇却象一股干涸了的水柱一样，慢慢钻进沙 里去。它不慌不忙地在石头的缝隙中钻动着，发出轻轻的金属般的响声。</p>
<p>我到达墙边的时候，正好把我的这位小王子接在我的怀抱中。他的脸色雪一 样惨白。</p>
<p>“这是搞的什么名堂！你怎么竟然和蛇也谈起心来了！”我解开了他一直带 着的金黄色的围脖。我用水渍湿了他的太阳穴，让他喝了点水。这时，我什么也 不敢再问他。他严肃地看着我，用双臂搂着我的脖子。我感到他的心就象一只被 枪弹击中而濒于死亡的鸟的心脏一样在跳动着。他对我说：</p>
<p>“我很高兴，你找到了你的机器所缺少的东西。你不久就可以回家去了……”</p>
<p>“你怎么知道的？”</p>
<p>我正是来告诉他，在没有任何希望的情况下，我成功地完成了修理工作。</p>
<p>他不回答我的问题，却接着说道：</p>
<p>“我也一样，今天，要回家去了……”</p>
<p>然后，他忧伤地说：</p>
<p>“我回家要远得多……要难得多……”</p>
<p>我清楚地感到发生了某种不寻常的事。我把他当作小孩一样紧紧抱在怀里， 可是我感觉到他径直地向着一个无底深渊沉陷下去，我想法拉住他，却怎么也办 不到……</p>
<p>他的眼神很严肃，望着遥远的地方。</p>
<p>“我有你画的羊，羊的箱子和羊的嘴套子……”</p>
<p>他带着忧伤的神情微笑了。</p>
<p>我等了很长时间，才觉得他身子渐渐暖和起来。</p>
<p>“小家伙，你受惊了……”</p>
<p>他害怕了，这是无疑的！他却温柔地笑着说：</p>
<p>“今天晚上，我会怕得更厉害……”</p>
<p>我再度意识到要发生一件不可弥补的事。我觉得我的心一下子就凉了。这时 我才明白：一想到再也不能听到这笑声，我就不能忍受。这笑声对我来说，就好 象是沙漠中的甘泉一样。</p>
<p>“小家伙，我还想听你笑……”</p>
<p>但他对我说：</p>
<p>“到今天夜里，正好是一年了。我的星球将正好处于我去年降落的那个地方 的上空……”</p>
<p>“小家伙，这蛇的事，约会的事，还有星星，这全是一场噩梦吧？”</p>
<p>但他并不回答我的问题。他对我说：</p>
<p>“重要的事，是看不见的……”</p>
<p>“当然……”</p>
<p>“这就象花一样。如果你爱上了一朵生长在一颗星星上的花，那么夜间，你 看着天空就感到甜蜜愉快。所有的星星上都好象开着花。”</p>
<p>“当然……”</p>
<p>“这也就象水一样，由于那辘轳和绳子的缘故，你给我喝的井水好象音乐一 样……你记得吗？……这水非常好喝……”</p>
<p>“当然……”</p>
<p>“夜晚，你抬头望着星星，我的那颗太小了，我无法给你指出我的那颗星星 是在哪里。这样倒更好。你可以认为我的那颗星星就在这些星星之中。那么，所 有的星星，你都会喜欢看的……这些星星都将成为你的朋友。而且，我还要给你一 件礼物……”</p>
<p>他又笑了。</p>
<p>“啊！小家伙，小家伙，我喜欢听你这笑声！”</p>
<p>“这正好是我给你的礼物，……这就好象水那样。”</p>
<p>“你说的是什么？”</p>
<p>“人们眼里的星星并不都一样。对旅行的人来说，星星是向导。对别的人来 说，星星只是些小亮光。对另外一些学者来说，星星就是他们探讨的学问。对我 所遇见的那个实业家来说，星星是金钱。但是，所有这些星星都不会说话。你呢， 你的那些星星将是任何人都不曾有过的……”</p>
<p>“你说的是什么？”</p>
<p>“夜晚，当你望着天空的时候，既然我就住在其中一颗星星上，既然我在其 中一颗星星上笑着，那么对你来说，就好象所有的星星都在笑，那么你将看到的<br>星星就是会笑的星星！”</p>
<p>这时，他又笑了。</p>
<p>“那么，在你得到了安慰之后（人们总是会自我安慰的）你就会因为认识了 我而感到高兴。你将永远是我的朋友。你就会想要同我一起笑。有时，你会为了 快乐而不知不觉地打开窗户。你的朋友们会奇怪地看着你笑着仰望天空。那时， 你就可以对他们说：‘是的，星星总是引我欢笑！’他们会以为你发疯了。我的 恶作剧将使你难堪……”</p>
<p>这时，他又笑了。</p>
<p>“这就好象我并没有给你星星，而是给你一大堆会笑出声来的小铃铛……”</p>
<p>他仍然笑着。随后他变得严肃起来：</p>
<p>“今天夜里……你知道……不要来了。”</p>
<p>“我不离开你。”</p>
<p>“我将会象是很痛苦的样子……我有点象要死去似的。就是这么回事，你就别 来看这些了，没有必要。”</p>
<p>“我不离开你。”</p>
<p>可是他担心起来。</p>
<p>“我对你说这些……这也是因为蛇的缘故。别让它咬了你……蛇是很坏的，它随 意咬人……”</p>
<p>“我不离开你。”</p>
<p>这时，他似乎有点放心了：</p>
<p>“对了，它咬第二口的时候就没有毒液了……”</p>
<p><img src="/2021/02/26/%E3%80%8A%E5%B0%8F%E7%8E%8B%E5%AD%90%E3%80%8B%E2%80%94XXVI/xwz43.jpg" alt="《小王子》童话小说图片"></p>
<p>这天夜里，我没有看到他起程。他不声不响地跑了。当我终于赶上他的时候， 他坚定地快步走着。他只是对我说道：</p>
<p>“啊，你在这儿……”</p>
<p>于是他拉着我的手。但是他仍然很担心：</p>
<p>“你不该这样。你会难受的。我会象是死去的样子，但这不会是真的……”</p>
<p>我默默无言。</p>
<p>“你明白，路很远。我不能带着这付身躯走。它太重了。”</p>
<p>我依然沉默不语。</p>
<p>“但是，这就好象剥落的旧树皮一样。旧树皮，并没有什么可悲的。”</p>
<p>我还是沉默不语。</p>
<p>他有些泄气了。但是他又振作起来：</p>
<p>“这将是蛮好的，你知道。我也一定会看星星的。所有的星星都将是带有生 了锈的辘轳的井。所有的星星都会倒水给我喝……”</p>
<p>我还是沉默不语。</p>
<p>“这将是多么好玩啊！你将有五亿个铃铛，我将有五亿口水井……”</p>
<p>这时，他也沉默了，因为他在哭。</p>
<p>“就是这儿。让我自个儿走一步吧。”</p>
<p><img src="/2021/02/26/%E3%80%8A%E5%B0%8F%E7%8E%8B%E5%AD%90%E3%80%8B%E2%80%94XXVI/xwz44.jpg" alt="他这时坐下来，因为他害怕了。"></p>
<p>他这时坐下来，因为他害怕了。他却仍然说道：</p>
<p>“你知道……我的花……我是要对她负责的！而她又是那么弱小！她又是那么天 真。她只有四根微不足道的刺，保护自己，抵抗外敌……”</p>
<p>我也坐了下来，因为我再也站立不住了。他说道：</p>
<p>“就是这些……全都说啦……”</p>
<p>他犹豫了一下，然后站起来。他迈出了一步。而我却动弹不得。</p>
<p><img src="/2021/02/26/%E3%80%8A%E5%B0%8F%E7%8E%8B%E5%AD%90%E3%80%8B%E2%80%94XXVI/xwz45.jpg" alt="他轻轻地象一棵树一样倒在地上，大概由于沙地的缘故，连一点响声都没有。"></p>
<p>在他的脚踝子骨附近，一道黄光闪了一下。刹那间他一动也不动了。他没有 叫喊。他轻轻地象一棵树一样倒在地上，大概由于沙地的缘故，连一点响声都没有。</p>
]]></content>
      <categories>
        <category>小王子</category>
      </categories>
  </entry>
  <entry>
    <title>《小王子》—XXVII</title>
    <url>/2021/02/26/%E3%80%8A%E5%B0%8F%E7%8E%8B%E5%AD%90%E3%80%8B%E2%80%94XXVII/</url>
    <content><![CDATA[<h1 id="《小王子》—XXVII"><a href="#《小王子》—XXVII" class="headerlink" title="《小王子》—XXVII"></a>《小王子》—XXVII</h1><h5 id="英文音频：https-www-bilibili-com-video-BV1H64y1f73s-p-28"><a href="#英文音频：https-www-bilibili-com-video-BV1H64y1f73s-p-28" class="headerlink" title="英文音频：https://www.bilibili.com/video/BV1H64y1f73s?p=28"></a>英文音频：<a href="https://www.bilibili.com/video/BV1H64y1f73s?p=28">https://www.bilibili.com/video/BV1H64y1f73s?p=28</a></h5><hr>
<p>到现在，一点不错，已经有六年了……我还从未讲过这个故事。同伴们重新见 到了我，都为能看见我活着回来而高兴。我却很悲伤。我告诉他们：“这是因为 疲劳的缘故……”</p>
<p>现在，我稍微得到了些安慰。就是说……还没有完全平静下来。可我知道他已 经回到了他的星球上。因为那天黎明，我没有再见到他的身躯。他的身躯并不那 么重……从此，我就喜欢在夜间倾听着星星，好象是倾听着五亿个铃铛……</p>
<p>可是，现在却又发生了不寻常的事。我给小王子画的羊嘴套上，忘了画皮带！ 他再也不可能把它套在羊嘴上。于是，我思忖着：“他的星球上发生了什么事呢？ 大概小羊把花吃掉了吧……”</p>
<p>有时我又对自己说，“绝对不会的！小王子每天夜里都用玻璃罩子罩住他的 花，而且他会把羊看管好的……”想到这里，我就非常高兴。这时，所有的星星都 在柔情地轻声笑着。</p>
<p>忽而我又对自己说：“人们有时总免不了会疏忽的，那就够戗！某一天晚上 他忘了玻璃罩子，或者小羊夜里不声不响地跑出来……”想到这里，小铃铛都变成 泪珠了！</p>
<p>这真是一个很大的奥秘。对你们这些喜欢小王子的人来说，就象对于我来说 一样，无论什么地方，凡是某处，如果一只羊（尽管我们并不认识它），吃了一 朵玫瑰花，或是没有吃掉一朵玫瑰花，那么宇宙的面貌就全然不同。</p>
<p>你们望着天空。你们想一想：羊究竟是吃了还是没有吃掉花？那么你们就会 看到一切都变了样……</p>
<p>任何一个大人将永远不会明白这个问题竟如此重要！</p>
<p><img src="/2021/02/26/%E3%80%8A%E5%B0%8F%E7%8E%8B%E5%AD%90%E3%80%8B%E2%80%94XXVII/xwz46.jpg" alt="在我看来，这是世界上最美、也最凄凉的景色。"></p>
<p>在我看来，这是世界上最美、也最凄凉的景色。上一页跟它前一页的景色是一样的。我再画上一遍，是为了引起你们注意。这里，就是小王子在地球上出现，然后又消失的地方。有一天，你们若去非洲沙漠旅行，请仔细认一认这个景色，免得当面错过了。你们若有机会经过那里，我恳求你们，不要匆匆离去，在这颗星下守候片刻。倘若有个孩子走到你们跟前，倘若他在笑，有一头金发，不回答别人的提问，你们就可猜到他是谁了。那时，劳驾你们！不要让我老是这么忧伤，赶快写信告诉我；他回来了……</p>
<p>全文完</p>
]]></content>
      <categories>
        <category>小王子</category>
      </categories>
  </entry>
  <entry>
    <title>高斯混合模型GMM</title>
    <url>/2021/02/27/%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<h1 id="高斯混合模型GMM（Gaussian-Mixture-Model）"><a href="#高斯混合模型GMM（Gaussian-Mixture-Model）" class="headerlink" title="高斯混合模型GMM（Gaussian Mixture Model）"></a>高斯混合模型GMM（Gaussian Mixture Model）</h1><h5 id="参考视频：https-www-bilibili-com-video-BV13b411w7Xj"><a href="#参考视频：https-www-bilibili-com-video-BV13b411w7Xj" class="headerlink" title="参考视频：https://www.bilibili.com/video/BV13b411w7Xj"></a>参考视频：<a href="https://www.bilibili.com/video/BV13b411w7Xj">https://www.bilibili.com/video/BV13b411w7Xj</a></h5><h5 id="PDF笔记来源：https-github-com-ws13685555932-machine-learning-derivation"><a href="#PDF笔记来源：https-github-com-ws13685555932-machine-learning-derivation" class="headerlink" title="PDF笔记来源：https://github.com/ws13685555932/machine_learning_derivation"></a>PDF笔记来源：<a href="https://github.com/ws13685555932/machine_learning_derivation">https://github.com/ws13685555932/machine_learning_derivation</a></h5>

	<div class="row">
    <embed src="../../../../file/11高斯混合模型.pdf" width="100%" height="550" type="application/pdf">
	</div>


]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title>模式识别</title>
    <url>/2021/03/03/%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB/</url>
    <content><![CDATA[<h1 id="模式识别"><a href="#模式识别" class="headerlink" title="模式识别"></a>模式识别</h1><hr>
<h2 id="阅读资料"><a href="#阅读资料" class="headerlink" title="阅读资料"></a>阅读资料</h2><p><a href="https://blog.csdn.net/zhinengxuexi/article/details/88702830">什么是模式识别，模式识别主要识别什么？</a></p>
<p><a href="http://www.duozhishidai.com/article-15389-1.html">模式识别应用于哪些领域，模式识别技术的发展趋势</a></p>
<p><a href="https://blog.csdn.net/scyscyao/article/details/5987581?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.control&amp;dist_request_id=&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.control">几种常见模式识别算法整理和总结</a></p>
]]></content>
      <categories>
        <category>课程</category>
      </categories>
      <tags>
        <tag>模式识别</tag>
      </tags>
  </entry>
  <entry>
    <title>线性回归</title>
    <url>/2021/03/03/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</url>
    <content><![CDATA[<h1 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h1><h5 id="参考视频：https-www-bilibili-com-video-BV1hW41167iL"><a href="#参考视频：https-www-bilibili-com-video-BV1hW41167iL" class="headerlink" title="参考视频：https://www.bilibili.com/video/BV1hW41167iL"></a>参考视频：<a href="https://www.bilibili.com/video/BV1hW41167iL">https://www.bilibili.com/video/BV1hW41167iL</a></h5><h5 id="PDF笔记来源：https-github-com-ws13685555932-machine-learning-derivation"><a href="#PDF笔记来源：https-github-com-ws13685555932-machine-learning-derivation" class="headerlink" title="PDF笔记来源：https://github.com/ws13685555932/machine_learning_derivation"></a>PDF笔记来源：<a href="https://github.com/ws13685555932/machine_learning_derivation">https://github.com/ws13685555932/machine_learning_derivation</a></h5>

	<div class="row">
    <embed src="../../../../file/03线性回归.pdf" width="100%" height="550" type="application/pdf">
	</div>


]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title>DEEP-METRIC-LEARNING-USING-TRIPLET-NETWORK</title>
    <url>/2021/03/04/DEEP-METRIC-LEARNING-USING-TRIPLET-NETWORK/</url>
    <content><![CDATA[<h1 id="DEEP-METRIC-LEARNING-USING-TRIPLET-NETWORK"><a href="#DEEP-METRIC-LEARNING-USING-TRIPLET-NETWORK" class="headerlink" title="DEEP METRIC LEARNING USING TRIPLET NETWORK"></a>DEEP METRIC LEARNING USING TRIPLET NETWORK</h1><h5 id="论文来源：ICLR-2015"><a href="#论文来源：ICLR-2015" class="headerlink" title="论文来源：ICLR 2015"></a>论文来源：ICLR 2015</h5><h5 id="论文链接：https-arxiv-org-pdf-1412-6622-pdf"><a href="#论文链接：https-arxiv-org-pdf-1412-6622-pdf" class="headerlink" title="论文链接：https://arxiv.org/pdf/1412.6622.pdf"></a>论文链接：<a href="https://arxiv.org/pdf/1412.6622.pdf">https://arxiv.org/pdf/1412.6622.pdf</a></h5><h5 id="代码链接：https-github-com-eladhoffer-TripletNet"><a href="#代码链接：https-github-com-eladhoffer-TripletNet" class="headerlink" title="代码链接：https://github.com/eladhoffer/TripletNet"></a>代码链接：<a href="https://github.com/eladhoffer/TripletNet">https://github.com/eladhoffer/TripletNet</a></h5><hr>
<h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p><img src="/2021/03/04/DEEP-METRIC-LEARNING-USING-TRIPLET-NETWORK/net_struct.png" width="80%"></p>
<p>网络每次输入三个样本$x,x^+,x^-$其中$x$为主样本，$x^+$为正样本，其类型和$x$相同，$x^-$为负样本，其类型和$x$不同。</p>
<p>网络的目标就是训练网络Net，使得经过网络后相同类的embedding尽可能相同，不同类的embedding尽可能不同。</p>
<script type="math/tex; mode=display">
\text { Triplet } N e t\left(x, x^{-}, x^{+}\right)=\left[\begin{array}{l}
\left\|N e t(x)-N e t\left(x^{-}\right)\right\|_{2} \\
\left\|N e t(x)-N e t\left(x^{+}\right)\right\|_{2}
\end{array}\right] \in \mathbb{R}_{+}^{2}</script><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><script type="math/tex; mode=display">Loss(d_+,d_-) = ||(d_+,d_--1)||^2_2 = const \cdot d_+^2</script><script type="math/tex; mode=display">
d_+= \frac{e^{||Net(x)-Net(x^+)||_2}}{e^{||Net(x)-Net(x^+)||_2} + e^{||Net(x)-Net(x^-)||_2}}</script><script type="math/tex; mode=display">
d_-= \frac{e^{||Net(x)-Net(x^-)||_2}}{e^{||Net(x)-Net(x^+)||_2} + e^{||Net(x)-Net(x^-)||_2}}</script><h2 id="样本生成策略"><a href="#样本生成策略" class="headerlink" title="样本生成策略"></a>样本生成策略</h2><p>在该论文中作者是采用随机选取的策略生成640000个三元组$(x,x^+,x^-)$</p>
]]></content>
      <categories>
        <category>paper</category>
      </categories>
  </entry>
  <entry>
    <title>对比学习（ContrastiveLearning）</title>
    <url>/2021/03/04/%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%EF%BC%88ContrastiveLearning%EF%BC%89/</url>
    <content><![CDATA[<h1 id="对比学习（Contrastive-Learning）"><a href="#对比学习（Contrastive-Learning）" class="headerlink" title="对比学习（Contrastive Learning）"></a>对比学习（Contrastive Learning）</h1><h2 id="学习链接："><a href="#学习链接：" class="headerlink" title="学习链接："></a>学习链接：</h2><p><a href="https://zhuanlan.zhihu.com/p/141141365">https://zhuanlan.zhihu.com/p/141141365</a></p>
<p><a href="https://ankeshanand.com/blog/2020/01/26/contrative-self-supervised-learning.html">https://ankeshanand.com/blog/2020/01/26/contrative-self-supervised-learning.html</a></p>
]]></content>
      <categories>
        <category>basic_concept</category>
      </categories>
      <tags>
        <tag>ContrastiveLearning</tag>
      </tags>
  </entry>
  <entry>
    <title>激活函数</title>
    <url>/2021/03/04/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/</url>
    <content><![CDATA[<p><img src="/2021/03/04/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/激活函数.png" alt></p>
<h2 id="什么事激活函数？"><a href="#什么事激活函数？" class="headerlink" title="什么事激活函数？"></a>什么事激活函数？</h2><p>神经网络中的每个神经元节点接受上一层神经元的输出值作为本神经元的输入值，并将输入值传递给下一层，输入层神经元节点会将输入属性值直接传递给下一层（隐层或输出层）。在多层神经网络中，上层节点的输出和下层节点的输入之间具有一个函数关系，这个函数称为激活函数（又称激励函数）。</p>
<h2 id="激活函数有什么用？"><a href="#激活函数有什么用？" class="headerlink" title="激活函数有什么用？"></a>激活函数有什么用？</h2><p>如果不用激励函数（其实相当于激励函数是f(x) = x），在这种情况下你每一层节点的输入都是上层输出的线性函数，很容易验证，无论你神经网络有多少层，输出都是输入的线性组合，与没有隐藏层效果相当，这种情况就是最原始的感知机（Perceptron）了，那么网络的逼近能力就相当有限。正因为上面的原因，我们决定引入非线性函数作为激励函数，这样深层神经网络表达能力就更加强大（不再是输入的线性组合，而是几乎可以逼近任意函数）。</p>
<h2 id="常见激活函数"><a href="#常见激活函数" class="headerlink" title="常见激活函数"></a>常见激活函数</h2><p>常见的激活函数有：</p>
<p>Sigmoid激活函数</p>
<p>Tanh激活函数</p>
<p>Relu激活函数</p>
<p>Leaky Relu激活函数</p>
<p>P-Relu激活函数</p>
<p>ELU激活函数</p>
<p>R-Relu激活函数</p>
<p>Gelu激活函数</p>
<p>swich激活函数</p>
<p>Selu激活函数</p>
<p>激活函数可以分为两大类 ：</p>
<p>饱和激活函数：sigmoid、tanh</p>
<p>非饱和激活函数: ReLU、Leaky Relu、ELU【指数线性单元】、PReLU【参数化的ReLU 】、RReLU【随机ReLU】</p>
<h3 id="Sigmoid"><a href="#Sigmoid" class="headerlink" title="Sigmoid"></a>Sigmoid</h3><p>sigmoid函数也叫Logistic函数，用于隐藏层的输出，输出在(0,1)之间，它可以将一个实数映射到(0,1)的范围内，可以用来做二分类。常用于:在特征相差比较复杂或是相差不是特别大的时候效果比较好。该函数将大的负数转换成0，将大的正数转换为1。公式描述如下：</p>
<script type="math/tex; mode=display">\sigma(x) = \frac{1}{1+e^{-x}}</script><p>函数图像：</p>
<p><img src="/2021/03/04/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/sigmiod_f.jpg" width="80%"></p>
<p>导数图像：</p>
<p><img src="/2021/03/04/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/sigmiod_g.jpg" width="80%"></p>
<p>缺点：</p>
<ol>
<li><p>梯度消失：Sigmoid 函数趋近 0 和 1 的时候变化率会变得平坦，也就是说，Sigmoid 的梯度趋近于 0。神经网络使用 Sigmoid 激活函数进行反向传播时，输出接近 0 或 1 的神经元其梯度趋近于 0。这些神经元叫作饱和神经元。因此，这些神经元的权重不会更新。此外，与此类神经元相连的神经元的权重也更新得很慢。该问题叫作梯度消失。因此，想象一下，如果一个大型神经网络包含 Sigmoid 神经元，而其中很多个都处于饱和状态，那么该网络无法执行反向传播。</p>
</li>
<li><p>Sigmoid 的 output 不是0均值（即zero-centered）。这会导致后一层的神经元将得到上一层输出的非0均值的信号作为输入。</p>
</li>
<li><p>计算成本高昂：exp() 函数与其他非线性激活函数相比，计算成本高昂。</p>
</li>
</ol>
<h3 id="Tanh"><a href="#Tanh" class="headerlink" title="Tanh"></a>Tanh</h3><p>Tanh 激活函数又叫作双曲正切激活函数（hyperbolic tangent activation function）。</p>
<script type="math/tex; mode=display">f(z) = tanh(z)=\frac{e^z-e^{-z}}{e^z+e^{-z}}</script><p>函数及导数图像：</p>
<p><img src="/2021/03/04/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/tanh_fg.png" width="80%"></p>
<p>与 Sigmoid 函数类似，Tanh 函数也使用真值，但 Tanh 函数将其压缩至-1 到 1 的区间内。与 Sigmoid 不同，Tanh 函数的输出以零为中心，因为区间在-1 到 1 之间。你可以将 Tanh 函数想象成两个 Sigmoid 函数放在一起。在实践中，Tanh 函数的使用优先性高于 Sigmoid 函数。负数输入被当作负值，零输入值的映射接近零，正数输入被当作正值。</p>
<p>优点：它解决了Sigmoid函数的不是zero-centered输出问题。</p>
<p>缺点：梯度消失（gradient vanishing）的问题和幂运算的问题仍然存在。</p>
<h3 id="Relu"><a href="#Relu" class="headerlink" title="Relu"></a>Relu</h3><script type="math/tex; mode=display">Relu(x) = max(0,x)</script><p>函数及导数图像：</p>
<p><img src="/2021/03/04/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/relu_fg.png" width="80%"></p>
<p>Relu是个分段线性函数，显然其导数在正半轴为1，负半轴为0，这样它在整个实数域上有一半的空间是不饱和的。相比之下，sigmoid函数几乎全部区域都是饱和的.</p>
<p>ReLU虽然简单，但却是近几年的重要成果，有以下几大优点：</p>
<ol>
<li>解决了gradient vanishing问题 (在正区间)</li>
<li>计算速度非常快，只需要判断输入是否大于0</li>
<li>收敛速度远快于sigmoid和tanh</li>
</ol>
<p>ReLu是分段线性函数，它的非线性性很弱，因此网络一般要做得很深。但这正好迎合了我们的需求，因为在同样效果的前提下，往往深度比宽度更重要，更深的模型泛化能力更好。所以自从有了Relu激活函数，各种很深的模型都被提出来了，一个标志性的事件是应该是VGG模型和它在ImageNet上取得的成功.</p>
<p>ReLU也有几个需要特别注意的问题：</p>
<ol>
<li>ReLU的输出不是zero-centered</li>
<li>某些神经元可能永远不会被激活(Dead ReLU Problem)，导致相应的参数永远不能被更新。</li>
</ol>
<p>有两个主要原因可能导致这种情况产生: </p>
<ul>
<li><p>非常不幸的参数初始化，这种情况比较少见 </p>
</li>
<li><p>learning rate太高导致在训练过程中参数更新太大，不幸使网络进入这种状态。解决方法是可以采用Xavier初始化方法，以及避免将learning rate设置太大或使用adagrad等自动调节learning rate的算法。</p>
</li>
</ul>
<h3 id="Leaky-Relu"><a href="#Leaky-Relu" class="headerlink" title="Leaky Relu"></a>Leaky Relu</h3><script type="math/tex; mode=display">LeakyRelu(x) = max(0.01x,x)</script><p><img src="/2021/03/04/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/LeakyRelu_fg.jpg" width="80%"></p>
<p>Leaky ReLU 的概念是：当 x &lt; 0 时，它得到 0.01 的正梯度。</p>
<p>优点：</p>
<p>该函数一定程度上缓解了 dead ReLU 问题。</p>
<p>缺点：</p>
<p>使用该函数的结果并不连贯。尽管它具备 ReLU 激活函数的所有特征，如计算高效、快速收敛、在正区域内不会饱和。</p>
<h3 id="P-Relu-Parametric-ReLU"><a href="#P-Relu-Parametric-ReLU" class="headerlink" title="P-Relu(Parametric ReLU)"></a>P-Relu(Parametric ReLU)</h3><script type="math/tex; mode=display">PReLU(x) = (\alpha x,x)</script><p>其中$\alpha$为可学习参数。</p>
<p><img src="/2021/03/04/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/P-ReLU_fg.jpg" width="80%"></p>
<p>这里引入了一个随机的超参数 $\alpha$ ，它可以被学习，因为你可以对它进行反向传播。这使神经元能够选择负区域最好的梯度，有了这种能力，它们可以变成 ReLU 或 Leaky ReLU。</p>
<p>总之，最好使用 ReLU，但是你可以使用 Leaky ReLU 或 Parametric ReLU 实验一下，看看它们是否更适合你的问题。</p>
<h3 id="Elu"><a href="#Elu" class="headerlink" title="Elu"></a>Elu</h3><script type="math/tex; mode=display">
f(x) = 
\begin{cases}
    x &,& if x>0 \\\\
    \alpha(e^x -1) &,& otherwise
\end{cases}</script><p>函数及导数图像：</p>
<p><img src="/2021/03/04/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/elu_fg.jpg" width="80%"></p>
<p>ELU也是为解决ReLU存在的问题而提出。</p>
<p>Elu激活函数有优点：ReLU的基本所有优点、不会有Dead ReLU问题，输出的均值接近0、零中心点问题。</p>
<p>Elu激活函数有缺点：计算量稍大，原点不可导。</p>
<p>类似于Leaky ReLU，理论上虽然好于ReLU，但在实际使用中目前并没有好的证据ELU总是优于ReLU。</p>
<h3 id="Gelu"><a href="#Gelu" class="headerlink" title="Gelu"></a>Gelu</h3><script type="math/tex; mode=display">\sigma(x) = \frac{x}{1+e^{-1.702x}}</script><p>函数及导数图像：</p>
<p><img src="/2021/03/04/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/gelu_fg.jpg" width="80%"></p>
<p>bert中使用的激活函数，作者经过实验证明比relu等要好。原点可导，不会有Dead ReLU问题。值得注意的是最近席卷NLP领域的BERT等预训练模型几乎都是用的这个激活函数。</p>
<h3 id="Swich"><a href="#Swich" class="headerlink" title="Swich"></a>Swich</h3><script type="math/tex; mode=display">\sigma(x) = \frac{x}{1+e^{-x\beta}}</script><p>函数及导数图像：</p>
<p><img src="/2021/03/04/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/swich_fg.jpg" width="80%"></p>
<p>根据上图，从图像上来看，Swish函数跟ReLu差不多，唯一区别较大的是接近于0的负半轴区域，因此，Swish 激活函数的输出可能下降，即使在输入值增大的情况下。大多数激活函数是单调的，即输入值增大的情况下，输出值不可能下降。而 Swish 函数为 0 时具备单侧有界（one-sided boundedness）的特性，它是平滑、非单调的。</p>
<p>缺点： - 只有实验证明，没有理论支持。 - 在浅层网络上，性能与relu差别不大。</p>
<h3 id="Selu"><a href="#Selu" class="headerlink" title="Selu"></a>Selu</h3><script type="math/tex; mode=display">Selu(x) = \lambda \begin{cases}
    x && if \quad x>0 \\\\
    \alpha e^x -\alpha && if \quad x\leq 0
\end{cases}</script><p>其实就是ELU乘了个lambda，关键在于这个lambda是大于1的。以前relu，prelu，elu这些激活函数，都是在负半轴坡度平缓，这样在activation的方差过大的时候可以让它减小，防止了梯度爆炸，但是正半轴坡度简单的设成了1。而selu的正半轴大于1，在方差过小的的时候可以让它增大，同时防止了梯度消失。这样激活函数就有一个不动点，网络深了以后每一层的输出都是均值为0方差为1。</p>
<p>当其中参数取为$\lambda \approx 1.0506, \alpha \approx 1.6733$时，在网络权重服从标准正态分布的条件下，各层输出的分布会向标准正态分布靠拢。这种「自我标准化」的特性可以避免梯度消失和爆炸的问题，让结构简单的前馈神经网络获得甚至超越 state-of-the-art 的性能。</p>
<p>selu的证明部分前提是权重服从正态分布，但是这个假设在实际中并不能一定成立，众多实验发现效果并不比relu好。</p>
]]></content>
      <categories>
        <category>basic_concept</category>
      </categories>
      <tags>
        <tag>激活函数</tag>
      </tags>
  </entry>
  <entry>
    <title>贝叶斯线性回归</title>
    <url>/2021/03/04/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</url>
    <content><![CDATA[<h1 id="贝叶斯线性回归"><a href="#贝叶斯线性回归" class="headerlink" title="贝叶斯线性回归"></a>贝叶斯线性回归</h1><h5 id="参考视频：https-www-bilibili-com-video-BV1St411m7XJ-p-1"><a href="#参考视频：https-www-bilibili-com-video-BV1St411m7XJ-p-1" class="headerlink" title="参考视频：https://www.bilibili.com/video/BV1St411m7XJ?p=1"></a>参考视频：<a href="https://www.bilibili.com/video/BV1St411m7XJ?p=1">https://www.bilibili.com/video/BV1St411m7XJ?p=1</a></h5><h5 id="PDF笔记来源：https-github-com-ws13685555932-machine-learning-derivation"><a href="#PDF笔记来源：https-github-com-ws13685555932-machine-learning-derivation" class="headerlink" title="PDF笔记来源：https://github.com/ws13685555932/machine_learning_derivation"></a>PDF笔记来源：<a href="https://github.com/ws13685555932/machine_learning_derivation">https://github.com/ws13685555932/machine_learning_derivation</a></h5>

	<div class="row">
    <embed src="../../../../file/19贝叶斯线性回归.pdf" width="100%" height="550" type="application/pdf">
	</div>


]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title>NLP算法岗-面经汇总</title>
    <url>/2021/03/05/NLP%E7%AE%97%E6%B3%95%E5%B2%97-%E9%9D%A2%E7%BB%8F%E6%B1%87%E6%80%BB/</url>
    <content><![CDATA[<p>链接：<a href="https://blog.csdn.net/abcdefg90876/article/details/105283341">https://blog.csdn.net/abcdefg90876/article/details/105283341</a></p>
]]></content>
      <categories>
        <category>nlp</category>
      </categories>
      <tags>
        <tag>面经</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch-Batch_first的理解</title>
    <url>/2021/03/15/Pytorch-Batch-first%E7%9A%84%E7%90%86%E8%A7%A3/</url>
    <content><![CDATA[<h1 id="Pytorch-Batch-first的理解"><a href="#Pytorch-Batch-first的理解" class="headerlink" title="Pytorch-Batch_first的理解"></a>Pytorch-Batch_first的理解</h1><p>用过PyTorch的朋友大概都知道，对于不同的网络层，输入数据的维度虽然不同，但是通常第一维都是batch_size。</p>
<p>比如torch.nn.Linear的输入$(batch_size, *, in_features)$，torch.nn.Conv2d的输入$（batch_size, C_{in}, H_{in} , W_{in} ）$。</p>
<p>而RNN的输入却是$(seq_len, batch_size, input_size)$，batch_size位于第二维度！虽然你可以将batch_size和序列长度seq_len对换位置，此时只需要把参数batch_first设置为True。但是默认情况下RNN输入为啥不是batch first？</p>
<p>原因同上，因为cuDNN中RNN的API就是batch_size在第二维度！进一步，为啥cuDNN要这么做呢？</p>
<p>举个例子，假设输入序列的长度(seq_len)是3，batch_size是2，一个batch的数据是[[“A”, “B”, “C”], [“D”, “E”, “F”]]，如下图所示。</p>
<p><img src="/2021/03/15/Pytorch-Batch-first%E7%9A%84%E7%90%86%E8%A7%A3/img1.png" width="80%"></p>
<p>由于RNN是序列模型，只有 $t_1$ 时刻计算完成，才能进入 $t_2$ 时刻，而”batch”就体现在每个时刻 $t_i$ 的计算过程中，上图中 $t_i$ 时刻将[“A”, “D”]作为当前时刻的batch数据，$t_2$ 时刻将[“B”, “E”]作为当前时刻的batch数据，可想而知，”A”与”D”在内存中相邻比”A”与”B”相邻更合理，这样取数据时才更高效。而不论Tensor的维度是多少，在内存中都以一维数组的形式存储，batch first意味着Tensor在内存中存储时，先存储第一个sequence，再存储第二个… 而如果是seq_len first，模型的输入在内存中，先存储所有sequence的第一个元素，然后是第二个元素… 两种区别如图2所示，seq_len first意味着不同sequence中同一个时刻对应的输入元素(比如”A”, “D” )在内存中是毗邻的，这样可以快速读取数据。</p>
<p><img src="/2021/03/15/Pytorch-Batch-first%E7%9A%84%E7%90%86%E8%A7%A3/img2.png" width="80%"></p>
]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
  </entry>
  <entry>
    <title>Supervised Contrastive Learning</title>
    <url>/2021/03/15/Supervised-Contrastive-Learning/</url>
    <content><![CDATA[<h1 id="Supervised-Contrastive-Learning"><a href="#Supervised-Contrastive-Learning" class="headerlink" title="Supervised Contrastive Learning"></a>Supervised Contrastive Learning</h1><h5 id="论文来源：NeurIPS-2020"><a href="#论文来源：NeurIPS-2020" class="headerlink" title="论文来源：NeurIPS 2020"></a>论文来源：NeurIPS 2020</h5><h5 id="论文链接：https-arxiv-org-pdf-2004-11362-pdf"><a href="#论文链接：https-arxiv-org-pdf-2004-11362-pdf" class="headerlink" title="论文链接：https://arxiv.org/pdf/2004.11362.pdf"></a>论文链接：<a href="https://arxiv.org/pdf/2004.11362.pdf">https://arxiv.org/pdf/2004.11362.pdf</a></h5><h5 id="代码链接：https-github-com-HobbitLong-SupContrast"><a href="#代码链接：https-github-com-HobbitLong-SupContrast" class="headerlink" title="代码链接：https://github.com/HobbitLong/SupContrast"></a>代码链接：<a href="https://github.com/HobbitLong/SupContrast">https://github.com/HobbitLong/SupContrast</a></h5><hr>
<h2 id="自监督对比学习（Self-Supervised-Contrastive-Learning）"><a href="#自监督对比学习（Self-Supervised-Contrastive-Learning）" class="headerlink" title="自监督对比学习（Self Supervised Contrastive Learning）"></a>自监督对比学习（Self Supervised Contrastive Learning）</h2><p>假设你有如下资源：</p>
<ol>
<li><p>一个BackBone Network，用来提取图片feature，例如：ResNet-50, ResNet-101或者ResNet-200。</p>
</li>
<li><p>一个训练数据集，例如ImageNet</p>
</li>
</ol>
<p>并且要求训练过程中不使用图片包含的Ground Truth类别信息（即不能用分类任务来训练），如何训练BackBone Network，使其能够为每张图片提取出好的feature？</p>
<p>这个问题的答案有很多，这里主要介绍一种“自监督对比学习”。对于自监督学习，核心是如何给数据自动产生一种标签，然后使用该标签来进行某种“监督学习”。例如：对于无标签的图片，可以把图片随机旋转一个角度 $\alpha$ （例如： $90^。,180^。，270^。$ ），然后用旋转后的图片作为输入，训练网络来预测图片到底旋转了哪个角度。作为本文涉及到的“自监督对比学习”，它的设定是这样的：</p>
<ol>
<li><p>假设一个MiniBatch包含 $N$ 张图片，分别随机对每张图片进行两次Data Augmentation（裁剪、翻转等）处理，每张图片会得出两张新的图片，总共会得出 $2N$ 张新图片，作为后续网络输入进行训练。</p>
</li>
<li><p>经过BackBone Network计算后， $2N$ 张图片，会产生 $2N$ 个feature ，对每个feature进行normalization处理，使其变为单位向量。这样每个feature就落在了一个半径为1的超球面上(hypersphere)。得到的feature为： $\{z_1,z_2,…,z_i,…,z_j,…,z_{2N} \}$ 。</p>
</li>
<li><p>对于任意一张图片 $i$ ：</p>
</li>
</ol>
<ul>
<li><p>在其余的 $2N-1$ 张图片中，都存在一张图片 $j$ ，图片 $i$ 和 $j$ 来源于同一张图片（同一张图片随机Augmentation两次，得出的图片 $i$ 和 $j$ ）。因为它们来源于同一张图片，所以让图片 $i$ 和 $j$ 的feature越接近越好。</p>
</li>
<li><p>除了图片 $i$ 和 $j$ ，对于其余的 $2N-2$ 张图片，因为它们与图片 $i$ 来源于不同的图片，所以让它们的feature与图片 $i$ 的feature越远越好<br>可以看出，这种方法的本质是：分别用与图片 $i$ 来源相同的图片的feature、与图片 $i$ 来源不同的图片的feature，跟图片 [$i$ 的feature进行对比，然后让来源相同的图片feature越接近，来源不同的图片feature越远。按照这个要求，训练使用如下loss函数来训练模型：</p>
</li>
</ul>
<script type="math/tex; mode=display">L^{self} = \sum\limits_{i \in I}L_i^{self} = - \sum\limits_{i\in I} \log \frac{exp(z_i\cdot z_{j(i)}/ \tau)}{\sum\limits_{a \in A(i)} \cdot exp(z_i \cdot z_k / \tau)}</script><p>其中，$z_l = Proj(Enc(x_l))$为表征学习得到的特征， $\tau$ 是一个大于0的常数（论文中称为：a scalar temperature parameter），$\cdot$表示内积(inner product) $a \cdot b=|a||b| \cos \theta$，内积越大表示$\theta$越小，即两向量的夹角越小，即两向量更相似。</p>
<p>该Loss表示：</p>
<p>对于任意图片 $i$ ：</p>
<ol>
<li><p>图片 $i$ 和 $j$ 的feature内积越小越好，$\cos\theta$越大，loss越小。</p>
</li>
<li><p>图片 $i$ 与来源不同的其它图片的feature内积的总和，越小越好。</p>
</li>
</ol>
<p>这种方式虽然能学到不错的feature，但有一个不足是：没有考虑到属于同一个类的不同图片之间的feature的相关性。例如下图所示的情况：</p>
<p><img src="/2021/03/15/Supervised-Contrastive-Learning/img1.png" width="80%"></p>
<p>对于一张图片（左上角），来源相同的图片（左侧下方两张augmentation后的图片）的feature，在超球面上的距离很接近，来源不同的图片之间的feature的距离会比较远。但有一张与左上角图片属于同一类的图片（上图里面的红框图片），他的feature与左上角图片的feature的距离也会很远。</p>
<p>直觉上，同类图片的feature，应该也是越接近越好。但由于“自监督对比学习”的设定里面不使用图片所属的类别信息，所以无法知道哪些图片属于同一类，因此也无法让同类图片的feature彼此距离接近。</p>
<p>如果能使用图片的类别 label信息，是否能提高以上“自监督对比学习“的feature的质量？</p>
<h2 id="监督对比学习（Supervised-Contrastive-Learning）"><a href="#监督对比学习（Supervised-Contrastive-Learning）" class="headerlink" title="监督对比学习（Supervised Contrastive Learning）"></a>监督对比学习（Supervised Contrastive Learning）</h2><p>为了让同类图片的feature彼此接近，需要使用类别信息来判断哪些图片属于同一个类，因此，方法的名字从“自监督”变成了“监督”。对比学习的依据，从“是否来源于同一张图片“，变成了”是否属于同一个类“。训练使用的loss函数变为：</p>
<script type="math/tex; mode=display">
\begin{array}{c}
\mathcal{L}_{\text {out }}^{\text {sup }}=\sum\limits_{i \in I} \mathcal{L}_{\text {out }, i}^{\text {sup }}=\sum\limits_{i \in I} \frac{-1}{|P(i)|} \sum\limits_{p \in P(i)} \log \frac{\exp \left(\boldsymbol{z}_{i} \cdot \boldsymbol{z}_{p} / \tau\right)}{\sum_{a \in A(i)} \exp \left(\boldsymbol{z}_{i} \cdot \boldsymbol{z}_{a} / \tau\right)} \\
\mathcal{L}_{i n}^{s u p}=\sum\limits_{i \in I} \mathcal{L}_{i n, i}^{s u p}=\sum_{i \in I}-\log \left\{\frac{1}{|P(i)|} \sum\limits_{p \in P(i)} \frac{\exp \left(\boldsymbol{z}_{i} \cdot \boldsymbol{z}_{p} / \tau\right)}{\sum\limits_{a \in A(i)} \exp \left(\boldsymbol{z}_{i} \cdot \boldsymbol{z}_{a} / \tau\right)}\right\}
\end{array}</script><p>$P(i) \equiv \{ p \in A(i):\tilde{y_p} = \tilde{y_i}\}$，代表所有与下标为i的样本label相同的样本集合（正样本集合），$|P(i)|$为该集合的大小。</p>
<p>该loss表达的含义是：</p>
<p>对于任意图片 $i$</p>
<ol>
<li><p>与图片 $i$ 属于同类的所有其它图片的feature，与图片 $i$ 的feature的内积的总和，越大越好</p>
</li>
<li><p>与图片 $i$ 不属于同类的所有其它图片的feature，与图片 $i$ 的feature的内积的总和，越小越好</p>
</li>
</ol>
<p>前者的求和操作在$\log$外部，而后者在内部。作者在论文中论证了这两种损失函数的优劣，最终得出$L_{out}^{sup}$更好。</p>
<p>相比自监督对比损失，两种方式都有如下特性：</p>
<ol>
<li>使用了大量正样本</li>
</ol>
<p>自监督学习仅仅将data augmentation得到的样本作为正样本，而在有监督的设置中，通过data augmentation得到的正样本以及与anchor标签一样的正样本都对公式中的分子有贡献。</p>
<ol>
<li>负样本越多，对比性越强</li>
</ol>
<p>保留了对于负样本的求和，噪声样本越多，对比的效果越好。</p>
<ol>
<li>具有发掘难正/负样本的内在能力</li>
</ol>
<p>这两个损失函数的梯度鼓励从hard positive和hard negative中学习。</p>
<p><img src="/2021/03/15/Supervised-Contrastive-Learning/img2.png" width="80%"></p>
]]></content>
      <categories>
        <category>paper</category>
      </categories>
      <tags>
        <tag>Contrastive Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>SVD（奇异值分解）</title>
    <url>/2021/03/16/SVD%EF%BC%88%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3%EF%BC%89/</url>
    <content><![CDATA[<h1 id="SVD-奇异值分解"><a href="#SVD-奇异值分解" class="headerlink" title="SVD(奇异值分解)"></a>SVD(奇异值分解)</h1><h2 id="1-特征值分解（EVD）"><a href="#1-特征值分解（EVD）" class="headerlink" title="1.特征值分解（EVD）"></a>1.特征值分解（EVD）</h2><h3 id="实对称矩阵"><a href="#实对称矩阵" class="headerlink" title="实对称矩阵"></a>实对称矩阵</h3><p>在理角奇异值分解之前，需要先回顾一下特征值分解，如果矩阵$A$是一个$m \times m$的<strong>实对称矩阵</strong>（即$A = A^T$），那么它可以被分解成如下的形式</p>
<script type="math/tex; mode=display">A=Q\Sigma Q^T = Q \begin{bmatrix}
    \lambda_1& \cdots&\cdots&\cdots \\
    \cdots &\lambda_2&\cdots&\cdots \\
    \cdots &\cdots&\ddots&\cdots \\
    \cdots & \cdots &\cdots&\lambda_m
\end{bmatrix} Q^T \tag{1-1}</script><p>其中$Q$为标准正交阵，即 Q^T有$QQ^T=I$，$\Sigma$为对角矩阵，且上面的矩阵的维度均为$m\times m$。$\lambda_i$称为<strong>特征值</strong>，$q_i$是$Q$（特征矩阵）中的列向量，称为<strong>特征向量</strong>。</p>
<blockquote>
<p>$I$ 在这里表示单位阵，有时候也用$E$表示单位阵。</p>
</blockquote>
<h3 id="一般矩阵"><a href="#一般矩阵" class="headerlink" title="一般矩阵"></a>一般矩阵</h3><p>上面的特征值分解，对矩阵有着较高的要求，它需要被分解的矩阵𝐴为实对称矩阵，但是现实中，我们所遇到的问题一般不是实对称矩阵。那么当我们碰到一般性的矩阵，即有一个$m\times n$的矩阵$A$，它是否能被分解成上面的式（1-1）的形式呢？当然是可以的，这就是我们下面要讨论的内容。</p>
<h2 id="2-奇异值分解（SVD）"><a href="#2-奇异值分解（SVD）" class="headerlink" title="2.奇异值分解（SVD）"></a>2.奇异值分解（SVD）</h2><h3 id="2-1-奇异值分解定义"><a href="#2-1-奇异值分解定义" class="headerlink" title="2.1 奇异值分解定义"></a>2.1 奇异值分解定义</h3><p>有一个$m \times n$的实数矩阵$A$，我们想要把它分解成如下的形式</p>
<script type="math/tex; mode=display">A=U \Sigma V^T \tag{2-1}</script><p>其中$U$和$V$均为单位正交阵，即有$UU^T=I$和 $VV^T=I$ ，$U$称为左奇异矩阵，$V$称为右奇异矩阵，$\Sigma$仅在主对角线上有值，我们称它为奇异值，其它元素均为0。上面矩阵的维度分别为$U\in R^{m\times m}$, $\Sigma \in R^{m\times n}$, $V\in R^{n \times m}$。</p>
<p>一般地$\Sigma$有如下形式</p>
<script type="math/tex; mode=display">\Sigma=\begin{bmatrix}
    \sigma_1&0&0&0&0 \\
    0&\sigma_2&0&0&0 \\
    0&0&\ddots&0&0 \\
    0&0&0&\ddots&0
\end{bmatrix}_{m\times n}</script><p><img src="/2021/03/16/SVD%EF%BC%88%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3%EF%BC%89/svd.svg" width="80%"></p>
<center style="font-size:14px;color:#C0C0C0;text-decoration:underline">图1-1 奇异值分解</center>

<p>对于奇异值分解，我们可以利用上面的图形象表示，图中方块的颜色表示值的大小，颜色越浅，值越大。对于奇异值矩阵$\Sigma$，只有其主对角线有奇异值，其余均为0。</p>
<h3 id="2-2-奇异值求解"><a href="#2-2-奇异值求解" class="headerlink" title="2.2 奇异值求解"></a>2.2 奇异值求解</h3><p>正常求上面的$U,V,\Sigma$不便于求，我们可以利用如下性质</p>
<script type="math/tex; mode=display">AA^T=U\Sigma V^T V\Sigma^T U^T = U\Sigma\Sigma^T U^T \tag{2-2}</script><script type="math/tex; mode=display">A^TA=V\Sigma^T U^T U\Sigma V^T = V\Sigma^T \Sigma V^T \tag{2-3}</script><blockquote>
<p>需要指出的是，这里 $\Sigma\Sigma^T$ 与$\Sigma^T \Sigma$在矩阵的角度上来讲，它们是不相等的，因为它们的维数不同$\Sigma \Sigma^T\in R^{m\times m}$，而$\Sigma^T \Sigma \in R^{n\times n}$，但是它们在主对角线的奇异值是相等的，即有</p>
<script type="math/tex; mode=display">\Sigma\Sigma^T=\begin{bmatrix}
    \sigma_1^2&0&0&0 \\
    0&\sigma_2^2&0&0\\
    0&0&\ddots&0\\
    0&0&0&\ddots
\end{bmatrix}_{m \times m} \qquad \Sigma^T\Sigma=\begin{bmatrix}
    \sigma_1^2&0&0&0 \\
    0&\sigma_2^2&0&0\\
    0&0&\ddots&0\\
    0&0&0&\ddots
\end{bmatrix}_{n \times n}</script></blockquote>
<p>可以看到式（2-2）与式（1-1）的形式非常相同，进一步分析，我们可以发现$AA^T$和$A^TA$也是对称矩阵，那么可以利用式（1-1），做特征值分解。利用式（2-2）特征值分解，得到的特征矩阵即为$U$；利用式（2-3）特征值分解，得到的特征矩阵即为$V$；对$\Sigma\Sigma^T$或$\Sigma^T\Sigma$中的特征值开方，可以得到所有的奇异值。</p>
<h2 id="3-奇异值分解的应用"><a href="#3-奇异值分解的应用" class="headerlink" title="3.奇异值分解的应用"></a>3.奇异值分解的应用</h2><h3 id="3-1-纯数学例子"><a href="#3-1-纯数学例子" class="headerlink" title="3.1 纯数学例子"></a>3.1 纯数学例子</h3><p>假设我们现在有矩阵$A$，需要对其做奇异值分解，已知</p>
<script type="math/tex; mode=display">A=\begin{bmatrix}
    1&5&7&6&1\\
    2&1&10&4&4\\
    3&6&7&5&2
\end{bmatrix}</script><p>那么可以求出$AA^T$和$A^TA$，如下</p>
<script type="math/tex; mode=display">AA^T=\begin{bmatrix}
    112&105&114\\
    105&137&110\\
    114&110&123
\end{bmatrix} \qquad A^TA=\begin{bmatrix}
    14&25&48&29&15\\
    25&62&87&64&21\\
    48&87&198&117&61\\
    29&64&117&77&32\\
    15&21&61&32&21
\end{bmatrix}</script><p>分别对上面做特征值分解，得到如下结果</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">U = [[-<span class="number">0.55572489</span>,  <span class="number">0.40548161</span>, -<span class="number">0.72577856</span>],</span><br><span class="line">    [-<span class="number">0.59283199</span>, -<span class="number">0.80531618</span>,  <span class="number">0.00401031</span>],</span><br><span class="line">    [-<span class="number">0.58285511</span>,  <span class="number">0.43249337</span>,  <span class="number">0.68791671</span>]]</span><br><span class="line"></span><br><span class="line">VT =  [[-<span class="number">0.18828164</span>, -<span class="number">0.37055755</span>, -<span class="number">0.74981208</span>, -<span class="number">0.46504304</span>, -<span class="number">0.22080294</span>],</span><br><span class="line">       [ <span class="number">0.01844501</span>,  <span class="number">0.76254787</span>, -<span class="number">0.4369731</span> ,  <span class="number">0.27450785</span>, -<span class="number">0.38971845</span>],</span><br><span class="line">       [ <span class="number">0.73354812</span>,  <span class="number">0.27392013</span>, -<span class="number">0.12258381</span>, -<span class="number">0.48996859</span>,  <span class="number">0.36301365</span>],</span><br><span class="line">       [ <span class="number">0.36052404</span>, -<span class="number">0.34595041</span>, -<span class="number">0.43411102</span>,  <span class="number">0.6833004</span> ,  <span class="number">0.30820273</span>],</span><br><span class="line">       [-<span class="number">0.5441869</span> ,  <span class="number">0.2940985</span> , -<span class="number">0.20822387</span>, -<span class="number">0.0375734</span> ,  <span class="number">0.7567019</span> ]]</span><br></pre></td></tr></table></figure>
<p>奇异值$\Sigma$=Diag(18.53581747,  5.0056557 ,  1.83490648)</p>
<h3 id="3-2-SVD在图片压缩中的应用"><a href="#3-2-SVD在图片压缩中的应用" class="headerlink" title="3.2 SVD在图片压缩中的应用"></a>3.2 SVD在图片压缩中的应用</h3><ol>
<li><p>加载相关库</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.image <span class="keyword">as</span>  mpimg</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>
</li>
<li><p>读取图片</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img_eg = mpimg.imread(<span class="string">&quot;view.jpeg&quot;</span>)</span><br><span class="line">img_eg.shape</span><br></pre></td></tr></table></figure>
<blockquote>
<p>(1200, 1920, 3)</p>
</blockquote>
</li>
<li><p>进行奇异值分解</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img_temp = img_eg.reshape(<span class="number">1200</span>, <span class="number">1920</span>*<span class="number">3</span>)</span><br><span class="line">U, S, VT = np.linalg.svd(img_temp)</span><br></pre></td></tr></table></figure>
</li>
<li><p>重构图像</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sval_num = <span class="number">60</span></span><br><span class="line">img_condensed1 = (U[:,<span class="number">0</span>:sval_num]).dot(np.diag(S[<span class="number">0</span>:sval_num])).dot(VT[<span class="number">0</span>:sval_num,:])</span><br><span class="line">img_condensed1 = img_condensed1.reshape(<span class="number">1200</span>, <span class="number">1920</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">sval_num = <span class="number">200</span></span><br><span class="line">img_condensed2 = (U[:,<span class="number">0</span>:sval_num]).dot(np.diag(S[<span class="number">0</span>:sval_num])).dot(VT[<span class="number">0</span>:sval_num,:])</span><br><span class="line">img_condensed2 = img_condensed2.reshape(<span class="number">1200</span>, <span class="number">1920</span>, <span class="number">3</span>)</span><br></pre></td></tr></table></figure></li>
<li>对比效果<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fig, ax = plt.subplots(<span class="number">1</span>, <span class="number">3</span>, figsize=(<span class="number">24</span>, <span class="number">32</span>))</span><br><span class="line">ax[<span class="number">0</span>].imshow(img_eg)</span><br><span class="line">ax[<span class="number">0</span>].<span class="built_in">set</span>(title=<span class="string">&#x27;src&#x27;</span>)</span><br><span class="line">ax[<span class="number">1</span>].imshow(img_condensed1.astype(np.uint8))</span><br><span class="line">ax[<span class="number">1</span>].<span class="built_in">set</span>(title=<span class="string">&#x27;nums of sigma = 60&#x27;</span>)</span><br><span class="line">ax[<span class="number">2</span>].imshow(img_condensed2.astype(np.uint8))</span><br><span class="line">ax[<span class="number">2</span>].<span class="built_in">set</span>(title=<span class="string">&#x27;nums of sigma = 120&#x27;</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p><img src="/2021/03/16/SVD%EF%BC%88%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3%EF%BC%89/img_compare.svg" width="80%"><center style="font-size:14px;color:#C0C0C0;text-decoration:underline">图3-1 奇异值分解重构图片</center></p>
</blockquote>
</li>
</ol>
<p>可以看到，当我们取到前面200个奇异值来重构图片时，基本上已经看不出与原图片有多大的差别。</p>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>从上面的图片的压缩结果中可以看出来，奇异值可以被看作成一个矩阵的代表值，或者说，奇异值能够代表这个矩阵的信息。<strong>当奇异值越大时，它代表的信息越多。</strong>因此，我们取前面若干个最大的奇异值，就可以基本上还原出数据本身。</p>
<p>如下，可以作出奇异值数值变化和前部分奇异值和的曲线图，如下图所示</p>
<p><img src="/2021/03/16/SVD%EF%BC%88%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3%EF%BC%89/cloud.tsinghua.edu.svg" width="80%"></p>
<center style="font-size:14px;color:#C0C0C0;text-decoration:underline">图3-1 奇异值变化图</center>

<p>从上面的第1个图，可以看出，奇异值下降是非常快的，因此可以只取前面几个奇异值，便可基本表达出原矩阵的信息。从第2个图，可以看出，当取到前100个奇异值时，这100个奇异值的和已经占总和的95%左右。</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>降维</tag>
      </tags>
  </entry>
  <entry>
    <title>深入理解PCA与SVD的关系</title>
    <url>/2021/03/16/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3PCA%E4%B8%8ESVD%E7%9A%84%E5%85%B3%E7%B3%BB/</url>
    <content><![CDATA[<h1 id="一、主成分析PCA"><a href="#一、主成分析PCA" class="headerlink" title="一、主成分析PCA"></a>一、主成分析PCA</h1><h2 id="1、所解决问题"><a href="#1、所解决问题" class="headerlink" title="1、所解决问题"></a>1、所解决问题</h2><p>给定 $m$ 个 $n$ 维样本$X=\{x_0,x_1,…,x_m\}$ ，通过变换 $y=Px$ (其中 $P_{k\times n}$ 为变换矩阵)，将样本 $(x_i)_{i=0,…,m}$ 从 $n$ 维降到 $k$ 维 $(y_i)_{i=0,…,m}$ ，计$Y=\{y_0,y_1,…,y_m\}$ ，同时最大程度的减少降维带来的信息损失。</p>
<h2 id="2、所依赖的原则"><a href="#2、所依赖的原则" class="headerlink" title="2、所依赖的原则"></a>2、所依赖的原则</h2><p>根据降维并减小信息损失的目标，可以得出以下两个原则</p>
<p>降维后的各个维度之间相互独立，即去除降维之前样本 $x$ 中各个维度之间的相关性。<br>最大程度保持降维后的每个维度数据的多样性，即最大化每个维度内的方差</p>
<p>记降维后样本集 $Y$ 的协方差矩阵为</p>
<p>$B_{k\times k} = \frac{1}{m}YY^T$</p>
<p>上述第一个条件要求协方差矩阵$B$除了对角线上元素外，其他均为0，也即 $B$ 为对角矩阵。</p>
<p>将变换关系 $y=Px$ 代入Y的协方差矩阵B中，</p>
<script type="math/tex; mode=display">B_{k\times k}=\frac{1}{m}YY^T = \frac{1}{m}PX(PX)^T=P\frac{1}{m}XX^TP=P_{k\times n}C_{n \times n}P_{n\times k}^T \tag{1}</script><p>其中， $C_{n\times n} = \frac{1}{m}XX^T$ 是变换前数据 $X$ 的协方差矩阵。</p>
<p>$C_{n\times n}$ 的特征值分解形式如下：</p>
<script type="math/tex; mode=display">D_{n\times n} = Q_{n\times n}C_{n\times n} Q_{n\times n}^T \tag{2}</script><p>其中， $D_{n\times n}$ 为对角矩阵。</p>
<p>明显的式(1)和式(2)除了维度不同，其他均一样。</p>
<p>结合上述第二条原则，变换矩阵 $P_{k\times n}$ 即是矩阵$C$的前$k$大的特征向量按行组成的矩阵。</p>
<h2 id="3、问题求解方法"><a href="#3、问题求解方法" class="headerlink" title="3、问题求解方法"></a>3、问题求解方法</h2><p>式2就是协方差矩阵 $C$ 的特征值分解，变换矩阵 $P_{k\times n}$ 即是矩阵$C$的前$k$大的特征向量按行组成的矩阵。所以，PCA的求解步骤为：</p>
<ul>
<li>求 $X$ 均值</li>
<li>将 $X$ 减去均值</li>
<li>计算协方差矩阵 $C=\frac{1}{m}XX^T$</li>
<li>对协方差矩阵 $C$ 特征值分解</li>
<li>从大到小排列 $C$ 的特征值</li>
<li>取前 $k$ 个特征值对应的特征向量按行组成矩阵即为变换矩阵 $P_{k\times n}$</li>
</ul>
<p><strong>这里的核心问题是协方差矩阵 $C=\frac{1}{m}XX^T$ 的特征值分解。</strong></p>
<h1 id="二、奇异值分解SVD"><a href="#二、奇异值分解SVD" class="headerlink" title="二、奇异值分解SVD"></a>二、奇异值分解SVD</h1><h2 id="1、所解决问题-1"><a href="#1、所解决问题-1" class="headerlink" title="1、所解决问题"></a>1、所解决问题</h2><script type="math/tex; mode=display">A_{m\times n} = U_{m\times m} \Sigma_{m\times n}V_{n \times n}^T</script><p>其中 $U_{m\times m}$ 和 $V_{n \times n}$ 均为正交矩阵， $\Sigma_{m\times n}$ 为对角矩阵</p>
<p>奇异值分解要解决的问题是将 $A_{m\times n}$ 矩阵分解为对角矩阵 $\Sigma_{m\times n}$ ，$\Sigma_{m\times n}$中对角元素 $\sigma_i$ 称为矩阵 $A_{m\times n}$ 的奇异值</p>
<h2 id="2、问题求解方法"><a href="#2、问题求解方法" class="headerlink" title="2、问题求解方法"></a>2、问题求解方法</h2><script type="math/tex; mode=display">AA^T=U\Sigma V^T V\Sigma^T U^T = U\Sigma\Sigma^T U^T</script><script type="math/tex; mode=display">A^TA=V\Sigma^T U^T U\Sigma V^T = V\Sigma^T \Sigma V^T</script><blockquote>
<p>需要指出的是，这里 $\Sigma\Sigma^T$ 与$\Sigma^T \Sigma$在矩阵的角度上来讲，它们是不相等的，因为它们的维数不同$\Sigma \Sigma^T\in R^{m\times m}$，而$\Sigma^T \Sigma \in R^{n\times n}$，但是它们在主对角线的奇异值是相等的，即有</p>
<script type="math/tex; mode=display">\Sigma\Sigma^T=\begin{bmatrix}
    \sigma_1^2&0&0&0 \\
    0&\sigma_2^2&0&0\\
    0&0&\ddots&0\\
    0&0&0&\ddots
\end{bmatrix}_{m \times m} \qquad \Sigma^T\Sigma=\begin{bmatrix}
    \sigma_1^2&0&0&0 \\
    0&\sigma_2^2&0&0\\
    0&0&\ddots&0\\
    0&0&0&\ddots
\end{bmatrix}_{n \times n}</script></blockquote>
<p>所以 $U$ 是 $AA^T$ 特征值分解的特征向量按列组成的正交矩阵,$V$ 是 $A^TA$ 特征值分解的特征向量按列组成的正交矩阵， $\Sigma\Sigma^T,\Sigma^T\Sigma$ 是$AA^T,A^TA$ 特征值组成的对角矩阵，也可以看出 $A_{m\times n}$ 的奇异值 $\sigma_i$ 是 $A^TA,A^TA$ 特征值 $\lambda_i$ 的平方根。</p>
<p><strong>奇异值分解的关键在于对 $A^TA,AA^T$ 进行特征值分解。</strong></p>
<h1 id="三、PCA与SVD的关系"><a href="#三、PCA与SVD的关系" class="headerlink" title="三、PCA与SVD的关系"></a>三、PCA与SVD的关系</h1><p>由上述分析可知，</p>
<ol>
<li>PCA求解关键在于求解协方差矩阵$C=\frac{1}{m}XX^T$的特征值分解</li>
<li>SVD关键在于 $A^TA,A^TA$ 的特征值分解。</li>
<li>很明显二者所解决的问题非常相似，都是对一个实对称矩阵进行特征值分解，</li>
</ol>
<p>如果取：</p>
<p>$A=\frac{X^T}{\sqrt{m}}$</p>
<p>则有：</p>
<p>$A^TA=(\frac{X^T}{\sqrt{m}})^T \frac{X^T}{\sqrt{m}}=\frac{1}{m}XX^T$</p>
<p>其实，PCA只与SVD的右奇异向量的压缩效果相同。</p>
<p>如果取 $V$的前 $k$ 行作为变换矩阵 $P_{k\times n}$ ，则 $Y_{k\times m} = P_{k\times n }X_{n\times m}$ ，起到压缩行即降维的效果</p>
<p>如果取 $U$的前 $d$ 行作为变换矩阵 $P_{d\times m}$ ，则 $Y_{n\times d} = X_{n\times m}P_{m\times d}$ ，起到压缩列即去除冗余样本的效果。</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>PCA</tag>
        <tag>SVD</tag>
      </tags>
  </entry>
  <entry>
    <title>比特位计数</title>
    <url>/2021/03/17/%E6%AF%94%E7%89%B9%E4%BD%8D%E8%AE%A1%E6%95%B0/</url>
    <content><![CDATA[<h1 id="比特位计数"><a href="#比特位计数" class="headerlink" title="比特位计数"></a>比特位计数</h1><p>给定一个非负整数 num。对于 0 ≤ i ≤ num 范围中的每个数字 i ，计算其二进制数中的 1 的数目并将它们作为数组返回。</p>
<p>示例 1:</p>
<blockquote>
<p>输入: 2</p>
<p>输出: [0,1,1]</p>
</blockquote>
<p>示例 2:</p>
<blockquote>
<p>输入: 5</p>
<p>输出: [0,1,1,2,1,2]</p>
</blockquote>
<p>进阶:</p>
<p>给出时间复杂度为O(n*sizeof(integer))的解答非常容易。但你可以在线性时间O(n)内用一趟扫描做到吗？<br>要求算法的空间复杂度为O(n)。<br>你能进一步完善解法吗？要求在C++或任何其他语言中不使用任何内置函数（如 C++ 中的 __builtin_popcount）来执行此操作。</p>
<h2 id="思路："><a href="#思路：" class="headerlink" title="思路："></a>思路：</h2><p>对于数$a$而言，一定有唯一的数$b=a\&amp;(a-1)$,有$b&lt;a$且，$bitcount(a)=bitcount(b)+1$。 由此对于任意一个数$x$,便可以得到一条$x \rightarrow a$的单调序列。</p>
<h2 id="代码："><a href="#代码：" class="headerlink" title="代码："></a>代码：</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">countBits</span>(<span class="params">self, num: <span class="built_in">int</span></span>) -&gt; <span class="type">List</span>[<span class="built_in">int</span>]:</span><br><span class="line">        ans = [<span class="number">0</span>]*(num+<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, num+<span class="number">1</span>):</span><br><span class="line">            ans[i] = ans[i&amp;(i-<span class="number">1</span>)]+<span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ans</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>acm</category>
      </categories>
      <tags>
        <tag>dp</tag>
      </tags>
  </entry>
  <entry>
    <title>What-makes-for-Good-Views-for-Contrastive-Learning</title>
    <url>/2021/03/22/What-makes-for-Good-Views-for-Contrastive-Learning/</url>
    <content><![CDATA[<h1 id="What-makes-for-Good-Views-for-Contrastive-Learning"><a href="#What-makes-for-Good-Views-for-Contrastive-Learning" class="headerlink" title="What makes for Good Views for Contrastive Learning?"></a>What makes for Good Views for Contrastive Learning?</h1><h5 id="论文来源：NeurIPS-2020"><a href="#论文来源：NeurIPS-2020" class="headerlink" title="论文来源：NeurIPS 2020"></a>论文来源：NeurIPS 2020</h5><h5 id="论文链接：https-arxiv-org-pdf-2005-10243v3-pdf"><a href="#论文链接：https-arxiv-org-pdf-2005-10243v3-pdf" class="headerlink" title="论文链接：https://arxiv.org/pdf/2005.10243v3.pdf"></a>论文链接：<a href="https://arxiv.org/pdf/2005.10243v3.pdf">https://arxiv.org/pdf/2005.10243v3.pdf</a></h5><h5 id="代码链接：https-github-com-HobbitLong-PyContrast"><a href="#代码链接：https-github-com-HobbitLong-PyContrast" class="headerlink" title="代码链接：https://github.com/HobbitLong/PyContrast"></a>代码链接：<a href="https://github.com/HobbitLong/PyContrast">https://github.com/HobbitLong/PyContrast</a></h5><hr>
<p>这篇文章主要提出了， 在对比学习中，我们在追求特征表示能够学到更多的相同类之间的共同特征时(InfoMax Principle)，也应该针对下游任务，去掉会影响下游任务的冗余特征(InfoMin Principle)。</p>
<h1 id="三个定义"><a href="#三个定义" class="headerlink" title="三个定义"></a>三个定义</h1><h2 id="Sufficient-Encoder"><a href="#Sufficient-Encoder" class="headerlink" title="Sufficient Encoder"></a>Sufficient Encoder</h2><p>在两个 view $v_i,v_2$ 之间进行 contrastive learning. 假设 $f_1$ 是 $v_1$ 的特征提取器，最终的特征表示为$z_1=f_1(v_1),z_2=f_2(v_2)$如果 $I(v_1,v_2)=I(f_1(v_1),v_2)$ ，则表示在编码过程中$v_1$关于$v_2$的信息没有损失。 换一句话说就是$z_1$中保留了对比学习所需要的所有信息。这就表明$f_1$是充分的。同理当$I(v_1,v_2)=I(v_1,f_2(v_2))$时，$f_2$是充分的。</p>
<h2 id="Minimal-Sufficient-Encoder"><a href="#Minimal-Sufficient-Encoder" class="headerlink" title="Minimal Sufficient Encoder"></a>Minimal Sufficient Encoder</h2><p>在上面 InfoMax 的基础上，加入 InfoMin 的含义。 $v_1$ 的编码器 $f_{min}$ 是最小的 sufficient encoder 当且仅当对于所有的 sufficient encoder $f$ , 有 $I(f_{min}(v_1),v_2) \leq I(f(v_1),v_2)$ 。因此 $f_{min}$ 是所有的 sufficient encoder 中含有信息量最小的一个。从理论上看，如果一个 encoder 能够保留足够的信息且含有最小的冗余，则该编码器具有更好的泛化能力。</p>
<h2 id="Optimal-Representation-of-a-Task"><a href="#Optimal-Representation-of-a-Task" class="headerlink" title="Optimal Representation of a Task"></a>Optimal Representation of a Task</h2><p>将最大化两个 view 互信息的任务扩展到一个预测任务 $\tau$ . 根据输入数据 $x$ 来预测 $y$ . 则关于 $x$ 的最优表示 $z^{<em>}$ 是关于 $y$ 的一个 minimal sufficient encoder. 意思是 $z^{</em>}$ 中包含了 $x$ 中所有关于 $y$ 的信息，同时尽可能少的包含其他冗余信息。</p>
<h2 id="Colorful-Moving-Mnist"><a href="#Colorful-Moving-Mnist" class="headerlink" title="Colorful-Moving-Mnist"></a>Colorful-Moving-Mnist</h2><p>构造了一组数据集。构造两个 view 之前共享的信息分别为 (1) ”position“（ $x_t$ 和 $v_2^+$ 中数字不同，背景不同，但是数字的位置相同），(2)”digit” （ $x_t$ 和 $v_2^+$ 中数字一致，其余信息都不同），(3)”background”(同理，仅background相同)。同时构造负样本 $v_2^-$ 。</p>
<p><img src="/2021/03/22/What-makes-for-Good-Views-for-Contrastive-Learning/img1.png" width="80%"></p>
<p>通过人为构造的数据集，可以控制两个 view 之间共享的变量。如共享 position，共享 digit，或者同时共享其中两者等。如下表，在每一对人为设置的共享变量的 view 中进行 contrastive learning，在不同的下游任务中进行评价。</p>
<p><img src="/2021/03/22/What-makes-for-Good-Views-for-Contrastive-Learning/img2.png" width="80%"></p>
<p>从该表的结果中可以得出以下结论：</p>
<ul>
<li><p>不同 view 之间共享的信息对下游任务的影响很大。例如，如果只共享 digit 的部分，那么特征表示会忽略 background 和 location 的信息，因此在这两个任务中无法取得效果。如果仅共享 background 的部分，那么关于 digit 分类和 location 无法成功。</p>
</li>
<li><p>当共享多个信息时，往往其中一个信息会占据主导地位。如共享 digit 和 position 时，digit 占据了主导地位，原因可能是因为 convolution 本来对 position 不敏感。当有 background 作为两个 view 之间共享因素时，background 往往会占据主导，其他任务都无法取得好的效果。</p>
</li>
</ul>
<p>假设 $v_1,v_2$ 是原始数据 $x$ 的两个 view, 最大化 $v_1,v_2$ 的 mutual-information $I(v_1,v_2)$ 目的是提取原始数据 $x$ 中与任务 $y$ 有关的信息 $I(x,y)$ 。其中，$I(x,y)$ 在给定数据和学习目标下是一个固定的值。下图给出了在最大化 $I(v_1,v_2)$ 的过程中和 $I(x,y)$ 之间的关系图。</p>
<p><img src="/2021/03/22/What-makes-for-Good-Views-for-Contrastive-Learning/img3.png" width="80%"></p>
<p>学习分为三个阶段：</p>
<ol>
<li><p>开始时 $I(v_1,v_2)&lt;I(x,y)$ ，特征表示中没有包含足够的信息来预测 $y$ 。</p>
</li>
<li><p>在学习过程中到达一个 sweet spot，其中 $I(v_1,v_2)=I(x,y)$ , 表明 $v_1,v_2$ 中 share 的信息量等价于数据和 label 之间的信息量。此时特征表示包含了所有的预测有关的信息，同时没有包含冗余的信息。</p>
</li>
<li><p>随后 $I(v_1,v_2)&gt;I(x,y)$ , 特征表示开始包含冗余的信息。</p>
</li>
</ol>
<p>在InfoMax的过程中，仅当到达中间的 sweet spot 时：</p>
<ul>
<li>在当前任务中具有能够预测目标 $y$ 的信息；</li>
<li>是一个 minimal sufficient encoder，具有最好的泛化能力。</li>
</ul>
<p>在分类任务中验证上面的想法。原始数据是图像，其中 $v_1,v-2$ 是相隔 $d$ 个像素位置提取的 patch，通过最大化 $v_1,v_2$ 的互信息来提取特征。任务 $y$ 是分类任务，在提取的特征基础上用 linear 层进行分类。可以看到，随着 pitch distance 的增大（图中从右到左），mutual-information 不断减小，在中间达到了一个 sweet spot，对下游任务具有最大的泛化能力。</p>
<p><img src="/2021/03/22/What-makes-for-Good-Views-for-Contrastive-Learning/img4.png" width="80%"></p>
<h2 id="Data-Augmentation"><a href="#Data-Augmentation" class="headerlink" title="Data Augmentation"></a>Data Augmentation</h2><p>在产生 view 的 augmentation 层面可以显示 $v_1,v_2$ 的互信息。具体的，施加的augmentation越大，会导致 $v_1,v_2$ 的互信息越小，但可在后续的分类任务中产生更好的性能。</p>
<p><img src="/2021/03/22/What-makes-for-Good-Views-for-Contrastive-Learning/img5.png" width="80%"></p>
<h2 id="Synthesizing-Views-with-Invertible-Generators"><a href="#Synthesizing-Views-with-Invertible-Generators" class="headerlink" title="Synthesizing Views with Invertible Generators"></a>Synthesizing Views with Invertible Generators</h2><p>本文提出了一种 adversarial training 的方法来获得 minimal sufficient encoder. 在原来对两个 view 的两个编码器 $f_1,f_2$ 的基础上增加一个编码器 $g$ ，先使用 $g$ 分别对两个 view $X_1,X_2$ 进行编码得到 $\hat{X_1},\hat{X_2}$ , 随后再使用 $f_1,f_2$ 两个 encoder 来最大化互信息，学习目标为：</p>
<p><strong>Unsupervised View Learning</strong> </p>
<p>Minimize $I(v_1, v_2)$</p>
<script type="math/tex; mode=display">\underset{g}{\min}\underset{f_1,f_2}{\max}I_{NCE}^{f_1,f_2}(g(X)_1;G(X)_{2:3}</script><p><strong>Semi-supervised View Learning</strong></p>
<p>Find Views that Share the Label Information</p>
<script type="math/tex; mode=display">\underset{g,c_1,c_2}{\min}\underset{f_1,f_2}{\max} \underset{unsupervised:\ reduce \ I(v_1,v_2)}{\underbrace{I_{NCE}^{f_1,f_2}(g(X)_1;g(X)_{2:3})}}+\underset{supervised:\ keep\ I(v_1;y)\ and \ I(v_2,y)}{\underbrace{L_{ce}(c_1(g(X)_1),y)+L_{ce}(c_2(g(X)_{2:3}),y)}}</script><p>其中 $g$ 的学习目标是 adversarial 的，最小化互信息。在对抗训练中希望得到 minimal sufficient 表示。</p>
]]></content>
      <categories>
        <category>paper</category>
      </categories>
      <tags>
        <tag>Contrastive Learning</tag>
        <tag>cv</tag>
      </tags>
  </entry>
  <entry>
    <title>矩阵置零</title>
    <url>/2021/03/23/%E7%9F%A9%E9%98%B5%E7%BD%AE%E9%9B%B6/</url>
    <content><![CDATA[<h1 id="矩阵置零"><a href="#矩阵置零" class="headerlink" title="矩阵置零"></a>矩阵置零</h1><p>给定一个 $m x n$ 的矩阵，如果一个元素为 0 ，则将其所在行和列的所有元素都设为 0 。请使用 原地 算法。</p>
<p>进阶：</p>
<ul>
<li>一个直观的解决方案是使用  $O(mn)$ 的额外空间，但这并不是一个好的解决方案。</li>
<li>一个简单的改进方案是使用 $O(m + n)$ 的额外空间，但这仍然不是最好的解决方案。</li>
<li>你能想出一个仅使用常量空间的解决方案吗？</li>
</ul>
<h2 id="思路："><a href="#思路：" class="headerlink" title="思路："></a>思路：</h2><p>先用第一行，第一列来标记对应的行，列中是否出现了0。 因为第一行，第一列会被修改，所以先用两个flag来记录第一行，第一列中是否有0。最后在处理第一行，第一列。</p>
<h2 id="代码："><a href="#代码：" class="headerlink" title="代码："></a>代码：</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">setZeroes</span>(<span class="params">self, matrix: <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">int</span>]]</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Do not return anything, modify matrix in-place instead.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        n, m = <span class="built_in">len</span>(matrix), <span class="built_in">len</span>(matrix[<span class="number">0</span>])</span><br><span class="line">        row_flag, col_flag = <span class="literal">False</span>, <span class="literal">False</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">            <span class="keyword">if</span> matrix[<span class="number">0</span>][j] == <span class="number">0</span>:</span><br><span class="line">                row_flag = <span class="literal">True</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            <span class="keyword">if</span> matrix[i][<span class="number">0</span>] == <span class="number">0</span>:</span><br><span class="line">                col_flag = <span class="literal">True</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">                <span class="keyword">if</span> matrix[i][j] == <span class="number">0</span>:</span><br><span class="line">                    matrix[i][<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">                    matrix[<span class="number">0</span>][j] = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n):</span><br><span class="line">            <span class="keyword">if</span> matrix[i][<span class="number">0</span>] == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, m):</span><br><span class="line">                    matrix[i][j] = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, m):</span><br><span class="line">            <span class="keyword">if</span> matrix[<span class="number">0</span>][j] == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n):</span><br><span class="line">                    matrix[i][j] = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> row_flag == <span class="literal">True</span>:</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">                matrix[<span class="number">0</span>][j] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">if</span> col_flag == <span class="literal">True</span>:</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">                matrix[i][<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>acm</category>
      </categories>
  </entry>
  <entry>
    <title>KD树</title>
    <url>/2021/03/24/KD%E6%A0%91/</url>
    <content><![CDATA[<h5 id="搬运链接：https-www-cnblogs-com-eyeszjwang-articles-2429382-html"><a href="#搬运链接：https-www-cnblogs-com-eyeszjwang-articles-2429382-html" class="headerlink" title="搬运链接：https://www.cnblogs.com/eyeszjwang/articles/2429382.html"></a>搬运链接：<a href="https://www.cnblogs.com/eyeszjwang/articles/2429382.html">https://www.cnblogs.com/eyeszjwang/articles/2429382.html</a></h5><hr>
<h1 id="KD-tree算法"><a href="#KD-tree算法" class="headerlink" title="KD-tree算法"></a>KD-tree算法</h1><p>KD树（k-dimensional树的简称），是一种分割k维数据空间的数据结构。主要应用于多维空间关键数据的搜索（如：范围搜索和最近邻搜索）。</p>
<h2 id="应用背景"><a href="#应用背景" class="headerlink" title="应用背景"></a>应用背景</h2><p>　　SIFT算法中做特征点匹配的时候就会利用到k-d树。而特征点匹配实际上就是一个通过距离函数在高维矢量之间进行相似性检索的问题。针对如何快速而准确地找到查询点的近邻，现在提出了很多高维空间索引结构和近似查询的算法，k-d树就是其中一种。</p>
<p>　　索引结构中相似性查询有两种基本的方式：一种是范围查询（range searches），另一种是K近邻查询（K-neighbor searches）。范围查询就是给定查询点和查询距离的阈值，从数据集中找出所有与查询点距离小于阈值的数据；K近邻查询是给定查询点及正整数K，从数据集中找到距离查询点最近的K个数据，当K=1时，就是最近邻查询（nearest neighbor searches）。</p>
<p>　　特征匹配算子大致可以分为两类。一类是线性扫描法，即将数据集中的点与查询点逐一进行距离比较，也就是穷举，缺点很明显，就是没有利用数据集本身蕴含的任何结构信息，搜索效率较低，第二类是建立数据索引，然后再进行快速匹配。因为实际数据一般都会呈现出簇状的聚类形态，通过设计有效的索引结构可以大大加快检索的速度。索引树属于第二类，其基本思想就是对搜索空间进行层次划分。根据划分的空间是否有混叠可以分为Clipping和Overlapping两种。前者划分空间没有重叠，其代表就是k-d树；后者划分空间相互有交叠，其代表为R树。（这里只介绍k-d树）</p>
<h2 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h2><p>　　先以一个简单直观的实例来介绍k-d树算法。假设有6个二维数据点{（2,3），（5,4），（9,6），（4,7），（8,1），（7,2）}，数据点位于二维空间内（如图1中黑点所示）。k-d树算法就是要确定图1中这些分割空间的分割线（多维空间即为分割平面，一般为超平面）。下面就要通过一步步展示k-d树是如何确定这些分割线的。</p>
<p><img src="/2021/03/24/KD%E6%A0%91/img1.png"></p>
<center style="font-size:14px;color:#C0C0C0;text-decoration:underline">图1  二维数据k-d树空间划分示意图</center>



<p> k-d树算法可以分为两大部分，一部分是有关k-d树本身这种数据结构建立的算法，另一部分是在建立的k-d树上如何进行最邻近查找的算法。</p>
<h3 id="k-d树构建算法"><a href="#k-d树构建算法" class="headerlink" title="k-d树构建算法"></a>k-d树构建算法</h3><p>　　k-d树是一个二叉树，每个节点表示一个空间范围。表1给出的是k-d树每个节点中主要包含的数据结构。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>域名</th>
<th>数据类型</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>Node-data</td>
<td>数据矢量</td>
<td>数据集中某个数据点，是n维矢量（这里也就是k维）</td>
</tr>
<tr>
<td>Range</td>
<td>空间矢量</td>
<td>该节点所代表的空间范围</td>
</tr>
<tr>
<td>split</td>
<td>整数</td>
<td>垂直于分割超平面的方向轴序号</td>
</tr>
<tr>
<td>Left</td>
<td>k-d树</td>
<td>由位于该节点分割超平面左子空间内所有数据点所构成的k-d树</td>
</tr>
<tr>
<td>Right</td>
<td>k-d树</td>
<td>由位于该节点分割超平面右子空间内所有数据点所构成的k-d树</td>
</tr>
<tr>
<td>parent</td>
<td>k-d树</td>
<td>父节点</td>
</tr>
</tbody>
</table>
</div>
<center style="font-size:14px;color:#C0C0C0;text-decoration:underline">表1  k-d树中每个节点的数据类型</center>

<p>　　从上面对k-d树节点的数据类型的描述可以看出构建k-d树是一个逐级展开的递归过程。表2给出的是构建k-d树的伪码。</p>
<h4 id="算法：构建k-d树（createKDTree）"><a href="#算法：构建k-d树（createKDTree）" class="headerlink" title="算法：构建k-d树（createKDTree）"></a>算法：构建k-d树（createKDTree）</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：数据点集Data-set和其所在的空间Range</span><br><span class="line">输出：Kd，类型为k-d tree</span><br><span class="line">1. If Data-set为空，则返回空的k-d tree</span><br><span class="line">2. 调用节点生成程序：</span><br><span class="line">   </span><br><span class="line">(1). 确定split域：对于所有描述子数据（特征矢量），统计它们在每个维上的数据方差。以SURF特征为例，描述子为64维，可计算64个方差。挑选出最大值，对应的维就是split域的值。数据方差大表明沿该坐标轴方向上的数据分散得比较开，在这个方向上进行数据分割有较好的分辨率；</span><br><span class="line"></span><br><span class="line">　　（2）确定Node-data域：数据点集Data-set按其第split域的值排序。位于正中间的那个数据点被选为Node-data。此时新的Data-set&#x27; = Data-set\Node-data（除去其中Node-data这一点）。</span><br><span class="line"></span><br><span class="line">3. dataleft = &#123;d属于Data-set&#x27; &amp;&amp; d[split] ≤ Node-data[split]&#125;</span><br><span class="line"></span><br><span class="line">   Left_Range = &#123;Range &amp;&amp; dataleft&#125;</span><br><span class="line">   dataright = &#123;d属于Data-set&#x27; &amp;&amp; d[split] &gt; Node-data[split]&#125;</span><br><span class="line"></span><br><span class="line">   Right_Range = &#123;Range &amp;&amp; dataright&#125;</span><br><span class="line"></span><br><span class="line">4. left = 由（dataleft，Left_Range）建立的k-d tree，即递归调用createKDTree（dataleft，Left_</span><br><span class="line"></span><br><span class="line">   Range）。并设置left的parent域为Kd；</span><br><span class="line"></span><br><span class="line">   right = 由（dataright，Right_Range）建立的k-d tree，即调用createKDTree（dataleft，Left_</span><br><span class="line"></span><br><span class="line">   Range）。并设置right的parent域为Kd。</span><br></pre></td></tr></table></figure>
<p><strong>以上述举的实例来看，过程如下：</strong></p>
<p>　　由于此例简单，数据维度只有2维，所以可以简单地给x，y两个方向轴编号为0,1，也即split={0,1}。</p>
<p>　　（1）确定split域的首先该取的值。分别计算x，y方向上数据的方差得知x方向上的方差最大，所以split域值首先取0，也就是x轴方向；</p>
<p>　　（2）确定Node-data的域值。根据x轴方向的值2,5,9,4,8,7排序选出中值为7，所以Node-data = （7,2）。这样，该节点的分割超平面就是通过（7,2）并垂直于split = 0（x轴）的直线x = 7；</p>
<p>　　（3）确定左子空间和右子空间。分割超平面x = 7将整个空间分为两部分，如图2所示。x &lt; =  7的部分为左子空间，包含3个节点{（2,3），（5,4），（4,7）}；另一部分为右子空间，包含2个节点{（9,6），（8,1）}。</p>
<p><img src="/2021/03/24/KD%E6%A0%91/img2.png"></p>
<center style="font-size:14px;color:#C0C0C0;text-decoration:underline">图2  x=7将整个空间分为两部分</center>



<p>　　如算法所述，k-d树的构建是一个递归的过程。然后对左子空间和右子空间内的数据重复根节点的过程就可以得到下一级子节点（5,4）和（9,6）（也就是左右子空间的’根’节点），同时将空间和数据集进一步细分。如此反复直到空间中只包含一个数据点，如图1所示。最后生成的k-d树如图3所示。</p>
<p><img src="/2021/03/24/KD%E6%A0%91/img3.png"></p>
<center style="font-size:14px;color:#C0C0C0;text-decoration:underline">图3  上述实例生成的k-d树</center>


<blockquote>
<p>注意：每一级节点旁边的’x’和’y’表示以该节点分割左右子空间时split所取的值。</p>
</blockquote>
<h3 id="k-d树上的最邻近查找算法"><a href="#k-d树上的最邻近查找算法" class="headerlink" title="k-d树上的最邻近查找算法"></a>k-d树上的最邻近查找算法</h3><p>　　在k-d树中进行数据的查找也是特征匹配的重要环节，其目的是检索在k-d树中与查询点距离最近的数据点。这里先以一个简单的实例来描述最邻近查找的基本思路。</p>
<p>　　星号表示要查询的点（2.1,3.1）。通过二叉搜索，顺着搜索路径很快就能找到最邻近的近似点，也就是叶子节点（2,3）。而找到的叶子节点并不一定就是最邻近的，最邻近肯定距离查询点更近，应该位于以查询点为圆心且通过叶子节点的圆域内。为了找到真正的最近邻，还需要进行’回溯’操作：算法沿搜索路径反向查找是否有距离查询点更近的数据点。此例中先从（7,2）点开始进行二叉查找，然后到达（5,4），最后到达（2,3），此时搜索路径中的节点为&lt;（7,2），（5,4），（2,3）&gt;，首先以（2,3）作为当前最近邻点，计算其到查询点（2.1,3.1）的距离为0.1414，然后回溯到其父节点（5,4），并判断在该父节点的其他子节点空间中是否有距离查询点更近的数据点。以（2.1,3.1）为圆心，以0.1414为半径画圆，如图4所示。发现该圆并不和超平面y = 4交割，因此不用进入（5,4）节点右子空间中去搜索。</p>
<p><img src="/2021/03/24/KD%E6%A0%91/img4.png"></p>
<center style="font-size:14px;color:#C0C0C0;
text-decoration:underline">图4  查找（2.1，3.1）点的两次回溯判断</center>


<p>　　再回溯到（7,2），以（2.1,3.1）为圆心，以0.1414为半径的圆更不会与x = 7超平面交割，因此不用进入（7,2）右子空间进行查找。至此，搜索路径中的节点已经全部回溯完，结束整个搜索，返回最近邻点（2,3），最近距离为0.1414。</p>
<p>　　一个复杂点了例子如查找点为（2，4.5）。同样先进行二叉查找，先从（7,2）查找到（5,4）节点，在进行查找时是由y = 4为分割超平面的，由于查找点为y值为4.5，因此进入右子空间查找到（4,7），形成搜索路径&lt;（7,2），（5,4），（4,7）&gt;，取（4,7）为当前最近邻点，计算其与目标查找点的距离为3.202。然后回溯到（5,4），计算其与查找点之间的距离为3.041。以（2，4.5）为圆心，以3.041为半径作圆，如图5所示。可见该圆和y = 4超平面交割，所以需要进入（5,4）左子空间进行查找。此时需将（2,3）节点加入搜索路径中得&lt;（7,2），（2,3）&gt;。回溯至（2,3）叶子节点，（2,3）距离（2,4.5）比（5,4）要近，所以最近邻点更新为（2，3），最近距离更新为1.5。回溯至（7,2），以（2,4.5）为圆心1.5为半径作圆，并不和x = 7分割超平面交割，如图6所示。至此，搜索路径回溯完。返回最近邻点（2,3），最近距离1.5。k-d树查询算法的伪代码如表3所示。</p>
<p><img src="/2021/03/24/KD%E6%A0%91/img5.png"></p>
<center style="font-size:14px;color:#C0C0C0;text-decoration:underline">图5  查找（2，4.5）点的第一次回溯判断</center>



<p><img src="/2021/03/24/KD%E6%A0%91/img6.png"></p>
<center style="font-size:14px;color:#C0C0C0;text-decoration:underline">图6  查找（2，4.5）点的第二次回溯判断</center>





<h4 id="算法：k-d树最邻近查找"><a href="#算法：k-d树最邻近查找" class="headerlink" title="算法：k-d树最邻近查找"></a>算法：k-d树最邻近查找</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：</span><br><span class="line">    </span><br><span class="line">     Kd，    //k-d tree类型</span><br><span class="line"></span><br><span class="line">     target  //查询数据点</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">     </span><br><span class="line">     nearest， //最邻近数据点</span><br><span class="line"></span><br><span class="line">     dist      //最邻近数据点和查询点间的距离</span><br><span class="line"></span><br><span class="line">1. If Kd为NULL，则设dist为infinite并返回</span><br><span class="line">2. //进行二叉查找，生成搜索路径</span><br><span class="line"></span><br><span class="line">   Kd_point = &amp;Kd；                   //Kd-point中保存k-d tree根节点地址</span><br><span class="line"></span><br><span class="line">   nearest = Kd_point -&gt; Node-data；  //初始化最近邻点</span><br><span class="line"></span><br><span class="line">   while（Kd_point）</span><br><span class="line"></span><br><span class="line">   　　push（Kd_point）到search_path中； //search_path是一个堆栈结构，存储着搜索路径节点指针</span><br><span class="line"></span><br><span class="line"> /*** If Dist（nearest，target） &gt; Dist（Kd_point -&gt; Node-data，target）</span><br><span class="line"></span><br><span class="line">   　　　　nearest  = Kd_point -&gt; Node-data；    //更新最近邻点</span><br><span class="line"></span><br><span class="line">   　　　　Max_dist = Dist(Kd_point，target）；  //更新最近邻点与查询点间的距离  ***/</span><br><span class="line"></span><br><span class="line">   　　s = Kd_point -&gt; split；                       //确定待分割的方向</span><br><span class="line"></span><br><span class="line">   　　If target[s] &lt;= Kd_point -&gt; Node-data[s]     //进行二叉查找</span><br><span class="line"></span><br><span class="line">   　　　　Kd_point = Kd_point -&gt; left；</span><br><span class="line"></span><br><span class="line">   　　else</span><br><span class="line"></span><br><span class="line">   　　　　Kd_point = Kd_point -&gt;right；</span><br><span class="line"></span><br><span class="line">   nearest = search_path中最后一个叶子节点； //注意：二叉搜索时不比计算选择搜索路径中的最邻近点，这部分已被注释</span><br><span class="line"></span><br><span class="line">   Max_dist = Dist（nearest，target）；    //直接取最后叶子节点作为回溯前的初始最近邻点</span><br><span class="line"></span><br><span class="line">3. //回溯查找</span><br><span class="line"></span><br><span class="line">   while（search_path != NULL）</span><br><span class="line"></span><br><span class="line">   　　back_point = 从search_path取出一个节点指针；   //从search_path堆栈弹栈</span><br><span class="line"></span><br><span class="line">   　　s = back_point -&gt; split；                   //确定分割方向</span><br><span class="line"></span><br><span class="line">   　　If Dist（target[s]，back_point -&gt; Node-data[s]） &lt; Max_dist   //判断还需进入的子空间</span><br><span class="line"></span><br><span class="line">   　　　　If target[s] &lt;= back_point -&gt; Node-data[s]</span><br><span class="line"></span><br><span class="line">   　　　　　　Kd_point = back_point -&gt; right；  //如果target位于左子空间，就应进入右子空间</span><br><span class="line"></span><br><span class="line">   　　　　else</span><br><span class="line"></span><br><span class="line">   　　　　　　Kd_point = back_point -&gt; left;    //如果target位于右子空间，就应进入左子空间</span><br><span class="line"></span><br><span class="line">   　　　　将Kd_point压入search_path堆栈；</span><br><span class="line"></span><br><span class="line">   　　If Dist（nearest，target） &gt; Dist（Kd_Point -&gt; Node-data，target）</span><br><span class="line"></span><br><span class="line">   　　　　nearest  = Kd_point -&gt; Node-data；                 //更新最近邻点</span><br><span class="line"></span><br><span class="line">   　　　　Min_dist = Dist（Kd_point -&gt; Node-data,target）；  //更新最近邻点与查询点间的距离</span><br></pre></td></tr></table></figure>
<p>　　上述两次实例表明，当查询点的邻域与分割超平面两侧空间交割时，需要查找另一侧子空间，导致检索过程复杂，效率下降。研究表明N个节点的K维k-d树搜索过程时间复杂度为：$t_{worst}=O（kN^{1-1/k}）$。</p>
<p>后记</p>
<p>　　以上为了介绍方便，讨论的是二维情形。像实际的应用中，如SIFT特征矢量128维，SURF特征矢量64维，维度都比较大，直接利用k-d树快速检索（维数不超过20）的性能急剧下降。假设数据集的维数为D，一般来说要求数据的规模N满足$N \gg 2^D$，才能达到高效的搜索。所以这就引出了一系列对k-d树算法的改进。有待进一步研究学习。</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>KNN</tag>
        <tag>KD-Tree</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习的项目结构和开发规范</title>
    <url>/2021/03/25/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%A1%B9%E7%9B%AE%E7%BB%93%E6%9E%84%E5%92%8C%E5%BC%80%E5%8F%91%E8%A7%84%E8%8C%83/</url>
    <content><![CDATA[<h1 id="深度学习的项目结构和开发规范"><a href="#深度学习的项目结构和开发规范" class="headerlink" title="深度学习的项目结构和开发规范"></a>深度学习的项目结构和开发规范</h1><h2 id="文件组织结构"><a href="#文件组织结构" class="headerlink" title="文件组织结构"></a>文件组织结构</h2><p>推荐采用下列文件组织结构。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">├── checkpoints/</span><br><span class="line">├── data/</span><br><span class="line">│   ├── __init__.py</span><br><span class="line">│   ├── dataset.py</span><br><span class="line">│   └── get_data.sh</span><br><span class="line">├── models/</span><br><span class="line">│   ├──lib/</span><br><span class="line">│   │   ├──__init__.py</span><br><span class="line">│   │   └──graph_conv_unit.py</span><br><span class="line">│   ├── __init__.py</span><br><span class="line">│   ├── AlexNet.py</span><br><span class="line">│   ├── BasicModule.py</span><br><span class="line">│   └── ResNet34.py</span><br><span class="line">└── utils/</span><br><span class="line">│   ├── __init__.py</span><br><span class="line">│   └── visualize.py</span><br><span class="line">├── config.py</span><br><span class="line">├── main.py</span><br><span class="line">├── requirements.txt</span><br><span class="line">├── README.md</span><br></pre></td></tr></table></figure>
<p>其中：</p>
<ul>
<li>checkpoints/： 用于保存训练好的模型，可使程序在异常退出后仍能重新载入模型，恢复训练</li>
<li>data/：数据相关操作，包括数据预处理、dataset实现等</li>
<li>models/：模型定义，可以有多个模型，例如上面的AlexNet和ResNet34，一个模型对应一个文件</li>
<li>models/lib/:构成模型的相关部件</li>
<li>utils/：可能用到的工具函数，在本次实验中主要是封装了可视化工具</li>
<li>config.py：配置文件，所有可配置的变量都集中在此，并提供默认值</li>
<li>main.py：主文件，训练和测试程序的入口，可通过不同的命令来指定不同的操作和参数</li>
<li>requirements.txt：程序依赖的第三方库</li>
<li>README.md：提供程序的必要说明</li>
</ul>
<h3 id="参数传递"><a href="#参数传递" class="headerlink" title="参数传递"></a>参数传递</h3><p>深度学习的模型有很多参数，常规的变量名方式的赋值很占行数，<br>这都是其次的，主要是没有高亮，在长篇的coding中，修改值会很不方便和吃力<br>利用argparse这个包可以完美解决这个问题</p>
<p>建议在config.py中定义此文件, 这样可以直接在运行时设置参数，也方便后面的调用<br>例如：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">parse_opt</span>():</span><br><span class="line">    parser = argparse.ArgumentParser()</span><br><span class="line">    <span class="comment">####### Original hyper-parameters #######</span></span><br><span class="line">    <span class="comment"># Data input settings</span></span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--input_json&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="string">&#x27;data/cocotalk.json&#x27;</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">&#x27;path to the json file containing additional info and vocab&#x27;</span>)</span><br><span class="line">    <span class="comment"># Model settings</span></span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--rnn_size&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">512</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">&#x27;size of the rnn in number of hidden nodes in each layer&#x27;</span>)</span><br><span class="line">    <span class="comment"># feature manipulation</span></span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--norm_att_feat&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">0</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">&#x27;If normalize attention features&#x27;</span>)</span><br><span class="line">    <span class="comment"># Optimization: General</span></span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--max_epochs&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=-<span class="number">1</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">&#x27;number of epochs&#x27;</span>)</span><br><span class="line">    <span class="comment"># Sample related</span></span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--max_length&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">20</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">&#x27;Maximum length during sampling&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#Optimization: for the Language Model</span></span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--learning_rate&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">4e-4</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">&#x27;learning rate&#x27;</span>)</span><br><span class="line">    <span class="comment"># Transformer</span></span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--noamopt&#x27;</span>, action=<span class="string">&#x27;store_true&#x27;</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">    <span class="comment"># Evaluation/Checkpointing</span></span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--save_checkpoint_every&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">2500</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">&#x27;how often to save a model checkpoint (in iterations)?&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    args = parser.parse_args()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Check if args are valid</span></span><br><span class="line">    <span class="keyword">assert</span> args.rnn_size &gt; <span class="number">0</span>, <span class="string">&quot;rnn_size should be greater than 0&quot;</span></span><br><span class="line">    <span class="keyword">assert</span> args.num_layers &gt; <span class="number">0</span>, <span class="string">&quot;num_layers should be greater than 0&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> args</span><br></pre></td></tr></table></figure></p>
<h3 id="init构造函数"><a href="#init构造函数" class="headerlink" title="init构造函数"></a><strong>init</strong>构造函数</h3><p>将所有的之前声明的需要传递的参数整合到当前的类中，供内部的成员函数分享使用。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">AttModel</span>(<span class="title class_ inherited__">CaptionModel</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, opt</span>):</span><br><span class="line">        <span class="built_in">super</span>(AttModel, self).__init__()</span><br><span class="line">        self.vocab_size = opt.vocab_size</span><br><span class="line">        self.input_encoding_size = opt.input_encoding_size</span><br><span class="line">        self.rnn_size = opt.rnn_size </span><br><span class="line">        self.num_layers = opt.num_layers  </span><br><span class="line">        self.drop_prob_lm = opt.drop_prob_lm </span><br><span class="line">        self.seq_length = opt.max_length <span class="keyword">or</span> opt.seq_length </span><br><span class="line">        self.fc_feat_size = opt.fc_feat_size</span><br><span class="line">        self.att_feat_size = opt.att_feat_size </span><br><span class="line">        self.att_hid_size = opt.att_hid_size </span><br><span class="line">        self.use_bn = opt.use_bn </span><br><span class="line">        self.ss_prob = opt.sampling_prob </span><br><span class="line">        </span><br><span class="line">        self.gpn = <span class="literal">True</span> <span class="keyword">if</span> opt.use_gpn == <span class="number">1</span> <span class="keyword">else</span> <span class="literal">False</span> </span><br><span class="line">        self.embed_dim = opt.embed_dim </span><br><span class="line">        self.GCN_dim = opt.gcn_dim  </span><br></pre></td></tr></table></figure></p>
]]></content>
  </entry>
  <entry>
    <title>常用数据集</title>
    <url>/2021/04/04/%E5%B8%B8%E7%94%A8%E6%95%B0%E6%8D%AE%E9%9B%86/</url>
    <content><![CDATA[<h2 id="自然语言处理领域"><a href="#自然语言处理领域" class="headerlink" title="自然语言处理领域"></a>自然语言处理领域</h2><h3 id="1）IMDb-Large-Movie-Review-Dataset"><a href="#1）IMDb-Large-Movie-Review-Dataset" class="headerlink" title="1）IMDb Large Movie Review Dataset"></a>1）IMDb Large Movie Review Dataset</h3><p>用于情感二元分类的数据集，其中包含 25,000 条用于训练的电影评论和 25,000 条用于测试的电影评论，这些电影评论的特点是两极分化特别明显。另外数据集里也包含未标记的数据可供使用。</p>
<p>引文：<a href="http://ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf">http://ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf</a></p>
<p>下载地址：<a href="https://s3.amazonaws.com/fast-ai-nlp/imdb.tgz">https://s3.amazonaws.com/fast-ai-nlp/imdb.tgz</a></p>
<h3 id="2）Wikitext-103"><a href="#2）Wikitext-103" class="headerlink" title="2）Wikitext-103"></a>2）Wikitext-103</h3><p>超过 1 亿个语句的数据合集，全部从维基百科的 Good 与 Featured 文章中提炼出来。广泛用于语言建模，当中包括 fastai 库和 ULMFiT 算法中经常用到的预训练模型。</p>
<p>引文：<a href="https://arxiv.org/abs/1609.07843">https://arxiv.org/abs/1609.07843</a></p>
<p>下载地址：<a href="https://s3.amazonaws.com/fast-ai-nlp/wikitext-103.tgz">https://s3.amazonaws.com/fast-ai-nlp/wikitext-103.tgz</a></p>
<h3 id="3）Wikitext-2"><a href="#3）Wikitext-2" class="headerlink" title="3）Wikitext-2"></a>3）Wikitext-2</h3><p>Wikitext-103 的子集，主要用于测试小型数据集的语言模型训练效果。</p>
<p>引文：<a href="https://arxiv.org/abs/1609.07843">https://arxiv.org/abs/1609.07843</a></p>
<p>下载地址：<a href="https://s3.amazonaws.com/fast-ai-nlp/wikitext-2.tgz">https://s3.amazonaws.com/fast-ai-nlp/wikitext-2.tgz</a></p>
<h3 id="4）WMT-2015-French-English-parallel-texts"><a href="#4）WMT-2015-French-English-parallel-texts" class="headerlink" title="4）WMT 2015 French/English parallel texts"></a>4）WMT 2015 French/English parallel texts</h3><p>用于训练翻译模型的法语/英语平行文本，拥有超过 2000 万句法语与英语句子。本数据集由 Chris Callison-Burch 创建，他抓取了上百万个网页，然后通过一组简单的启发式算法将法语网址转换为英文网址，并默认这些文档之间互为译文。</p>
<p>引文：<a href="https://www.cis.upenn.edu/~ccb/publications/findings-of-the-wmt09-shared-tasks.pdf">https://www.cis.upenn.edu/~ccb/publications/findings-of-the-wmt09-shared-tasks.pdf</a></p>
<p>下载地址：<a href="https://s3.amazonaws.com/fast-ai-nlp/giga-fren.tgz">https://s3.amazonaws.com/fast-ai-nlp/giga-fren.tgz</a></p>
<h3 id="5）AG-News"><a href="#5）AG-News" class="headerlink" title="5）AG News"></a>5）AG News</h3><p>496,835 条来自 AG 新闻语料库 4 大类别超过 2000 个新闻源的新闻文章，数据集仅仅援用了标题和描述字段。每个类别分别拥有 30,000 个训练样本及 1900 个测试样本。</p>
<p>引文：<a href="https://arxiv.org/abs/1509.01626">https://arxiv.org/abs/1509.01626</a></p>
<p>下载地址：<a href="https://s3.amazonaws.com/fast-ai-nlp/ag_news_csv.tgz">https://s3.amazonaws.com/fast-ai-nlp/ag_news_csv.tgz</a></p>
<h3 id="6）Amazon-reviews-Full"><a href="#6）Amazon-reviews-Full" class="headerlink" title="6）Amazon reviews - Full"></a>6）Amazon reviews - Full</h3><p>34,686,770 条来自 6,643,669 名亚马逊用户针对 2,441,053 款产品的评论，数据集主要来源于斯坦福网络分析项目（SNAP）。数据集的每个类别分别包含 600,000 个训练样本和 130,000 个测试样本。</p>
<p>引文：<a href="https://arxiv.org/abs/1509.01626">https://arxiv.org/abs/1509.01626</a></p>
<p>下载地址：<a href="https://s3.amazonaws.com/fast-ai-nlp/amazon_review_full_csv.tgz">https://s3.amazonaws.com/fast-ai-nlp/amazon_review_full_csv.tgz</a></p>
<h3 id="7）Amazon-reviews-Polarity"><a href="#7）Amazon-reviews-Polarity" class="headerlink" title="7）Amazon reviews - Polarity"></a>7）Amazon reviews - Polarity</h3><p>34,686,770 条来自 6,643,669 名亚马逊用户针对 2,441,053 款产品的评论，数据集主要来源于斯坦福网络分析项目（SNAP）。该子集的每个情绪极性数据集分别包含 1,800,000 个训练样本和 200,000 个测试样本。</p>
<p>引文：<a href="https://arxiv.org/abs/1509.01626">https://arxiv.org/abs/1509.01626</a></p>
<p>下载地址：<a href="https://s3.amazonaws.com/fast-ai-nlp/amazon_review_polarity_csv.tgz">https://s3.amazonaws.com/fast-ai-nlp/amazon_review_polarity_csv.tgz</a></p>
<h3 id="8）DBPedia-ontology"><a href="#8）DBPedia-ontology" class="headerlink" title="8）DBPedia ontology"></a>8）DBPedia ontology</h3><p>来自 DBpedia 2014 的 14 个不重叠的分类的 40,000 个训练样本和 5,000 个测试样本。</p>
<p>引文：<a href="https://arxiv.org/abs/1509.01626">https://arxiv.org/abs/1509.01626</a></p>
<p>下载地址：<a href="https://s3.amazonaws.com/fast-ai-nlp/dbpedia_csv.tgz">https://s3.amazonaws.com/fast-ai-nlp/dbpedia_csv.tgz</a></p>
<h3 id="9）Sogou-news"><a href="#9）Sogou-news" class="headerlink" title="9）Sogou news"></a>9）Sogou news</h3><p>2,909,551 篇来自 SogouCA 和 SogouCS 新闻语料库 5 个类别的新闻文章。每个类别分别包含 90,000 个训练样本和 12,000 个测试样本。这些汉字都已经转换成拼音。</p>
<p>引文：<a href="https://arxiv.org/abs/1509.01626">https://arxiv.org/abs/1509.01626</a></p>
<p>下载地址：<a href="https://s3.amazonaws.com/fast-ai-nlp/sogou_news_csv.tgz">https://s3.amazonaws.com/fast-ai-nlp/sogou_news_csv.tgz</a></p>
<h3 id="10）Yahoo-Answers"><a href="#10）Yahoo-Answers" class="headerlink" title="10）Yahoo! Answers"></a>10）Yahoo! Answers</h3><p>来自雅虎 Yahoo! Answers Comprehensive Questions and Answers1.0 数据集的 10 个主要分类数据。每个类别分别包含 140,000 个训练样本和 5,000 个测试样本。</p>
<p>引文：<a href="https://arxiv.org/abs/1509.01626">https://arxiv.org/abs/1509.01626</a></p>
<p>下载地址：<a href="https://s3.amazonaws.com/fast-ai-nlp/yahoo_answers_csv.tgz">https://s3.amazonaws.com/fast-ai-nlp/yahoo_answers_csv.tgz</a></p>
<h3 id="11）Yelp-reviews-Full"><a href="#11）Yelp-reviews-Full" class="headerlink" title="11）Yelp reviews - Full"></a>11）Yelp reviews - Full</h3><p>来自 2015 年 Yelp Dataset Challenge 数据集的 1,569,264 个样本。每个评级分别包含 130,000 个训练样本和 10,000 个 测试样本。</p>
<p>引文：<a href="https://arxiv.org/abs/1509.01626">https://arxiv.org/abs/1509.01626</a></p>
<p>下载地址：<a href="https://s3.amazonaws.com/fast-ai-nlp/yelp_review_full_csv.tgz">https://s3.amazonaws.com/fast-ai-nlp/yelp_review_full_csv.tgz</a></p>
<h3 id="12）Yelp-reviews-Polarity"><a href="#12）Yelp-reviews-Polarity" class="headerlink" title="12）Yelp reviews - Polarity"></a>12）Yelp reviews - Polarity</h3><p>来自 2015 年 Yelp Dataset Challenge 数据集的 1,569,264 个样本。该子集中的不同极性分别包含 280,000 个训练样本和 19,000 个测试样本。</p>
<p>引文：<a href="https://arxiv.org/abs/1509.01626">https://arxiv.org/abs/1509.01626</a></p>
<p>下载地址：<a href="https://s3.amazonaws.com/fast-ai-nlp/yelp_review_polarity_csv.tgz">https://s3.amazonaws.com/fast-ai-nlp/yelp_review_polarity_csv.tgz</a></p>
<h2 id="图像分类领域"><a href="#图像分类领域" class="headerlink" title="图像分类领域"></a>图像分类领域</h2><h3 id="1）MNIST"><a href="#1）MNIST" class="headerlink" title="1）MNIST"></a>1）MNIST</h3><p>经典的小型（28x28 像素）灰度手写数字数据集，开发于 20 世纪 90 年代，主要用于测试当时最复杂的模型；到了今日，MNIST 数据集更多被视作深度学习的基础教材。fast.ai 版本的数据集舍弃了原始的特殊二进制格式，转而采用标准的 PNG 格式，以便在目前大多数代码库中作为正常的工作流使用；如果您只想使用与原始同样的单输入通道，只需在通道轴中选取单个切片即可。</p>
<p>引文：<a href="http://yann.lecun.com/exdb/publis/index.html#lecun-98">http://yann.lecun.com/exdb/publis/index.html#lecun-98</a></p>
<p>下载地址：<a href="https://s3.amazonaws.com/fast-ai-imageclas/mnist_png.tgz">https://s3.amazonaws.com/fast-ai-imageclas/mnist_png.tgz</a></p>
<h3 id="2）CIFAR10"><a href="#2）CIFAR10" class="headerlink" title="2）CIFAR10"></a>2）CIFAR10</h3><p>10 个类别，多达 60000 张的 32x32 像素彩色图像（50000 张训练图像和 10000 张测试图像），平均每种类别拥有 6000 张图像。广泛用于测试新算法的性能。fast.ai 版本的数据集舍弃了原始的特殊二进制格式，转而采用标准的 PNG 格式，以便在目前大多数代码库中作为正常的工作流使用。</p>
<p>引文：<a href="https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf">https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf</a></p>
<p>下载地址：<a href="https://s3.amazonaws.com/fast-ai-imageclas/cifar10.tgz">https://s3.amazonaws.com/fast-ai-imageclas/cifar10.tgz</a></p>
<h3 id="3）CIFAR100"><a href="#3）CIFAR100" class="headerlink" title="3）CIFAR100"></a>3）CIFAR100</h3><p>与 CIFAR-10 类似，区别在于 CIFAR-100 拥有 100 种类别，每个类别包含 600 张图像（500 张训练图像和 100 张测试图像），然后这 100 个类别又被划分为 20 个超类。因此，数据集里的每张图像自带一个「精细」标签（所属的类）和一个「粗略」标签（所属的超类）。</p>
<p>引文：<a href="https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf">https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf</a></p>
<p>下载地址：<a href="https://s3.amazonaws.com/fast-ai-imageclas/cifar100.tgz">https://s3.amazonaws.com/fast-ai-imageclas/cifar100.tgz</a></p>
<h3 id="4）Caltech-UCSD-Birds-200-2011"><a href="#4）Caltech-UCSD-Birds-200-2011" class="headerlink" title="4）Caltech-UCSD Birds-200-2011"></a>4）Caltech-UCSD Birds-200-2011</h3><p>包含 200 种鸟类（主要为北美洲鸟类）照片的图像数据集，可用于图像识别工作。分类数量：200；图片数量：11,788；平均每张图片含有的标注数量：15 个局部位置，312 个二进制属性，1 个边框框。</p>
<p>引文：<a href="http://vis-www.cs.umass.edu/bcnn/">http://vis-www.cs.umass.edu/bcnn/</a></p>
<p>下载地址：<a href="https://s3.amazonaws.com/fast-ai-imageclas/CUB_200_2011.tgz">https://s3.amazonaws.com/fast-ai-imageclas/CUB_200_2011.tgz</a></p>
<h3 id="5）Caltech-101"><a href="#5）Caltech-101" class="headerlink" title="5）Caltech 101"></a>5）Caltech 101</h3><p>包含 101 种物品类别的图像数据集，平均每个类别拥有 40—800 张图像，其中很大一部分类别的图像数量固为 50 张左右。每张图像的大小约为 300 x 200 像素。本数据集也可以用于目标检测定位。</p>
<p>引文：<a href="http://www.vision.caltech.edu/feifeili/Fei-Fei_GMBV04.pdf">http://www.vision.caltech.edu/feifeili/Fei-Fei_GMBV04.pdf</a></p>
<p>下载地址：<a href="https://s3.amazonaws.com/fast-ai-imageclas/caltech_101.tar.gz">https://s3.amazonaws.com/fast-ai-imageclas/caltech_101.tar.gz</a></p>
<h3 id="6）Oxford-IIIT-Pet"><a href="#6）Oxford-IIIT-Pet" class="headerlink" title="6）Oxford-IIIT Pet"></a>6）Oxford-IIIT Pet</h3><p>包含 37 种宠物类别的图像数据集，每个类别约有 200 张图像。这些图像在比例、姿势以及光照方面有着丰富的变化。本数据集也可以用于目标检测定位。</p>
<p>引文：<a href="http://www.robots.ox.ac.uk/~vgg/publications/2012/parkhi12a/parkhi12a.pdf">http://www.robots.ox.ac.uk/~vgg/publications/2012/parkhi12a/parkhi12a.pdf</a></p>
<p>下载地址：<a href="https://s3.amazonaws.com/fast-ai-imageclas/oxford-iiit-pet.tgz">https://s3.amazonaws.com/fast-ai-imageclas/oxford-iiit-pet.tgz</a></p>
<h3 id="7）Oxford-102-Flowers"><a href="#7）Oxford-102-Flowers" class="headerlink" title="7）Oxford 102 Flowers"></a>7）Oxford 102 Flowers</h3><p>包含 102 种花类的图像数据集（主要是一些英国常见的花类），每个类别包含 40—258 张图像。这些图像在比例、姿势以及光照方面有着丰富的变化。</p>
<p>引文：<a href="http://www.robots.ox.ac.uk/~vgg/publications/papers/nilsback08.pdf">http://www.robots.ox.ac.uk/~vgg/publications/papers/nilsback08.pdf</a></p>
<p>下载地址：<a href="https://s3.amazonaws.com/fast-ai-imageclas/oxford-102-flowers.tgz">https://s3.amazonaws.com/fast-ai-imageclas/oxford-102-flowers.tgz</a></p>
<h3 id="8）Food-101"><a href="#8）Food-101" class="headerlink" title="8）Food-101"></a>8）Food-101</h3><p>包含 101 种食品类别的图像数据集，共有 101,000 张图像，平均每个类别拥有 250 张测试图像和 750 张训练图像。训练图像未经过数据清洗。所有图像都已经重新进行了尺寸缩放，最大边长达到了 512 像素。</p>
<p>引文：<a href="https://pdfs.semanticscholar.org/8e3f/12804882b60ad5f59aad92755c5edb34860e.pdf">https://pdfs.semanticscholar.org/8e3f/12804882b60ad5f59aad92755c5edb34860e.pdf</a></p>
<p>下载地址：<a href="https://s3.amazonaws.com/fast-ai-imageclas/food-101.tgz">https://s3.amazonaws.com/fast-ai-imageclas/food-101.tgz</a></p>
<h3 id="9）Stanford-cars"><a href="#9）Stanford-cars" class="headerlink" title="9）Stanford cars"></a>9）Stanford cars</h3><p>包含 196 种汽车类别的图像数据集，共有 16,185 张图像，分别为 8,144 张训练图像和 8,041 张测试图像，每个类别的图像类型比例基本上都是五五开。本数据集的类别主要基于汽车的牌子、车型以及年份进行划分。</p>
<p>引文：<a href="https://ai.stanford.edu/~jkrause/papers/3drr13.pdf">https://ai.stanford.edu/~jkrause/papers/3drr13.pdf</a></p>
<p>下载地址：<a href="https://s3.amazonaws.com/fast-ai-imageclas/stanford-cars.tgz">https://s3.amazonaws.com/fast-ai-imageclas/stanford-cars.tgz</a></p>
<h2 id="目标检测定位"><a href="#目标检测定位" class="headerlink" title="目标检测定位"></a>目标检测定位</h2><h3 id="1）Camvid-Motion-based-Segmentation-and-Recognition-Dataset"><a href="#1）Camvid-Motion-based-Segmentation-and-Recognition-Dataset" class="headerlink" title="1）Camvid: Motion-based Segmentation and Recognition Dataset"></a>1）Camvid: Motion-based Segmentation and Recognition Dataset</h3><p>700 张包含像素级别语义分割的图像分割数据集，每张图像都经过第二个人的检查和确认来确保数据的准确性。</p>
<p>引文：<a href="https://pdfs.semanticscholar.org/08f6/24f7ee5c3b05b1b604357fb1532241e208db.pdf">https://pdfs.semanticscholar.org/08f6/24f7ee5c3b05b1b604357fb1532241e208db.pdf</a></p>
<p>下载地址：<a href="https://s3.amazonaws.com/fast-ai-imagelocal/camvid.tgz">https://s3.amazonaws.com/fast-ai-imagelocal/camvid.tgz</a></p>
<h3 id="2）PASCAL-Visual-Object-Classes-VOC"><a href="#2）PASCAL-Visual-Object-Classes-VOC" class="headerlink" title="2）PASCAL Visual Object Classes (VOC)"></a>2）PASCAL Visual Object Classes (VOC)</h3><p>用于类识别的标准图像数据集——这里同时提供了 2007 与 2012 版本。2012 年的版本拥有 20 个类别。训练数据的 11,530 张图像中包含了 27,450 个 ROI 注释对象和 6,929 个目标分割数据。</p>
<p>引文：<a href="http://host.robots.ox.ac.uk/pascal/VOC/pubs/everingham10.pdf">http://host.robots.ox.ac.uk/pascal/VOC/pubs/everingham10.pdf</a></p>
<p>下载地址：<a href="https://s3.amazonaws.com/fast-ai-imagelocal/pascal-voc.tgz">https://s3.amazonaws.com/fast-ai-imagelocal/pascal-voc.tgz</a></p>
<p>COCO 数据集</p>
<p>目前最常用于图像检测定位的数据集应该要属 COCO 数据集（全称为 Common Objects in Context）。本文提供 2017 版 COCO 数据集的所有文件，另外附带由 fast.ai 创建的子集数据集。我们可以从 COCO 数据集下载页面（<a href="http://cocodataset.org/#download）获取每个">http://cocodataset.org/#download）获取每个</a> COCO 数据集的详情。fast.ai 创建的子集数据集包含五个选定类别的所有图像，这五个选定类别分别为：椅子、沙发、电视遥控、书籍和花瓶。</p>
<p>fast.ai 创建的子集数据集：<a href="https://s3.amazonaws.com/fast-ai-coco/coco_sample.tgz">https://s3.amazonaws.com/fast-ai-coco/coco_sample.tgz</a></p>
<p>训练图像数据集：<a href="https://s3.amazonaws.com/fast-ai-coco/train2017.zip">https://s3.amazonaws.com/fast-ai-coco/train2017.zip</a></p>
<p>验证图像数据集：<a href="https://s3.amazonaws.com/fast-ai-coco/val2017.zip">https://s3.amazonaws.com/fast-ai-coco/val2017.zip</a></p>
<p>测试图像数据集：<a href="https://s3.amazonaws.com/fast-ai-coco/test2017.zip">https://s3.amazonaws.com/fast-ai-coco/test2017.zip</a></p>
<p>未经标注的图像数据集：<a href="https://s3.amazonaws.com/fast-ai-coco/unlabeled2017.zip">https://s3.amazonaws.com/fast-ai-coco/unlabeled2017.zip</a></p>
<p>测试图像数据集详情：<a href="https://s3.amazonaws.com/fast-ai-coco/image_info_test2017.zip">https://s3.amazonaws.com/fast-ai-coco/image_info_test2017.zip</a></p>
<p>未经标注的图像数据集详情：<a href="https://s3.amazonaws.com/fast-ai-coco/image_info_unlabeled2017.zip">https://s3.amazonaws.com/fast-ai-coco/image_info_unlabeled2017.zip</a></p>
<p>训练/验证注释集：<a href="https://s3.amazonaws.com/fast-ai-coco/annotations_trainval2017.zip">https://s3.amazonaws.com/fast-ai-coco/annotations_trainval2017.zip</a></p>
<p>主体训练/验证注释集：<a href="https://s3.amazonaws.com/fast-ai-coco/stuff_annotations_trainval2017.zip">https://s3.amazonaws.com/fast-ai-coco/stuff_annotations_trainval2017.zip</a></p>
<p>全景训练/验证注释集：<a href="https://s3.amazonaws.com/fast-ai-coco/panoptic_annotations_trainval2017.zip">https://s3.amazonaws.com/fast-ai-coco/panoptic_annotations_trainval2017.zip</a></p>
]]></content>
      <categories>
        <category>数据集</category>
      </categories>
      <tags>
        <tag>数据集</tag>
      </tags>
  </entry>
  <entry>
    <title>SceneGraphGeneration</title>
    <url>/2021/04/07/SceneGraphGeneration/</url>
    <content><![CDATA[<h2 id="学习链接"><a href="#学习链接" class="headerlink" title="学习链接"></a>学习链接</h2><p><a href="https://blog.csdn.net/qq_39388410/article/details/105877505">https://blog.csdn.net/qq_39388410/article/details/105877505</a></p>
<p><a href="https://cloud.tencent.com/developer/article/1598413">https://cloud.tencent.com/developer/article/1598413</a></p>
<h2 id="问题-amp-任务"><a href="#问题-amp-任务" class="headerlink" title="问题 &amp; 任务"></a>问题 &amp; 任务</h2><p>图片场景图生成任务（Image scene graph generation）目标是让计算机自动生成一种语义化的图结构（称为 scene graph，场景图），作为图像的表示。图像中的目标对应 graph node，目标间的关系对应 graph edge（目标的各种属性，如颜色，有时会在图中表示）。</p>
<p>这种结构化表示方法相对于向量表示更加直观，可以看做是小型知识图谱，因此可以广泛应用于知识管理、推理、检索、推荐等。此外，该表示方法是模态无关的，自然语言、视频、语音等数据同样可以表示成类似结构，因此对于融合多模态信息很有潜力。</p>
<p>Scene graph 刚开始提出时[1]，被应用到图片检索任务，利用 scene graph 去图片库搜索内容相近的图片。当时使用到的 scene graph 是基于目标检测数据集人工标注的，耗时耗力。随着 Visual Genome 大型数据集的公开，其对超过十万的图片进行了 scene graph 的人工标注，如何自动生成 scene graph 成为了热门的研究任务。</p>
<p><img src="/2021/04/07/SceneGraphGeneration/img1.jpg"></p>
<center style="font-size:14px;color:#C0C0C0;text-decoration:underline">图 1</center>

<p>形式化地，记关系集合为R，目标集为O, 目标位置为B （一般是 Bounding box），图像为 I，则图片场景图为G={ B, O, R}。根据给定条件多少，场景图生成任务可以由简单到复杂细分为以下几种：</p>
<p>关系分类(Predicate classification): 给定图中目标位置及类别，对关系进行归类，记为 P(R | O, B, I)。<br>场景图分类(Scene graph classification): 给定图中目标位置，对关系及目标关系进行归类，记为 P(R, O | B, I)。<br>场景图生成(Scene graph generation): 只给定图片，要求生成 scene graph，记为 P( G={R, O, B} | I)。<br>前一个任务可认为是后一个任务简化版。在评测模型能力时，一般需要考察模型在此三个任务的表现，以评价模型中关系分类模块、目标分类模块及目标定位模块的作用。</p>
<h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><p>Visual Genome 于2016年提出[1]，是这个领域最常用的数据集，包含对超过 10W 张图片的目标、属性、关系、自然语言描述、视觉问答等的标注。与此任务相关的数据总结如下：</p>
<p>目标 or 图节点：用 bounding box 标注位置，并且有对应的类别名称。Visual Genome 包含约 17,000 种目标。<br>关系 or 图的边：可以动作 (jumping over)，空间关系（on），从属关系(belong-to, has)，动词(wear)等。Visual Genome 包含一共 13K 种关系。<br>属性（在图中附着在是节点上）：可以实验颜色(yellow)，状态(standing)等。Visual Genome 包含约 155000 种属性。<br>对于目标、关系、属性对应的字词，使用了 WordNet 进行规范化，目的是为了归并同义词。不过常用的只是 Visual Genome 的一个子集（[3,4,5] 文献都用到这个子集，简称为 VG150），选取了150 种常见目标，50 种常见关系，统计详见下表[2], [3]。</p>
<p><img src="/2021/04/07/SceneGraphGeneration/img2.jpg" width="80%"></p>
<center style="font-size:14px;color:#C0C0C0;text-decoration:underline">图 2</center>

<h2 id="方法分类"><a href="#方法分类" class="headerlink" title="方法分类"></a>方法分类</h2><p>目前的大多数场景图生成模型，根据任务的分解大致分为如下两种：</p>
<ol>
<li>P(O,B,R | I) = P(O,B | I) * P(R| I,O,B)，即先目标检测，再进行关系预测（有一个专门研究该子任务的领域，称为研究视觉关系识别，visual relationship detection）。最简单的方法是下文中基于统计频率的 baseline 方法，另外做视觉关系检测任务的大多数工作都可以应用到这里。</li>
<li>P(O,B,R | I) = P(B | I) * P(R,O| I,O,B)，即先定位目标，然后将一张图片中所有的目标和关系看做一个未标记的图结构，再分别对节点和边进行类别预测。这种做法考虑到了一张图片中的各元素互为上下文，为彼此分类提供辅助信息。接下来的 IMP、GRCNN 及 Neural Motif 中基于 RNN 的方法属于这一类。事实上，自此类方法提出之后 [2]，才正式有了 scene graph generation 这个新任务名称（之前基本都称为 visual relationship detection）。</li>
</ol>
<h2 id="当前的挑战"><a href="#当前的挑战" class="headerlink" title="当前的挑战"></a>当前的挑战</h2><p>最近从数据角度，发现该任务的几个棘手的问题。</p>
<ol>
<li>关系简单。从表1看出，空间关系和从属关系占了所有标注的90%以上，语义关系只占很少部分。</li>
<li>关系类别不互斥。比如 on, sitting on。这使得同一对目标可能存在多个关系标注。视觉关系分类建模成“多选一”的分类问题是否合理，也需要深思。</li>
<li>关系整体分布长尾效应严重。一般而言，可以将视觉关系分为空间关系、从属关系和语义关系等几种。对每一种关系的画柱状图，可以看出长尾分布非常严重。</li>
<li>关系的条件分布 bias 问题严重。比如已知主语宾语是 person 和 head，基本就可以猜测关系是 person has head 或是无关系。这也导致用复杂模型来预测并不比盲猜好多少的现象。</li>
<li>标注稀疏。VG 数据集每个图片大致有10个标注目标，但只有不到5组关系标注。很多目标之间存在关系，但却没有标注。</li>
</ol>
<p>这些问题不能完全归咎于数据集，很大程度上由任务本身决定，换数据集问题照样存在。还有毕竟重建数据集工作量巨大，也不能保证重新标注得多好。更聪明的做法或许是接受现实，从解决问题的方法上努力，比如引入半监督学习处理标注稀疏的问题 [10]，或者引入因果推理处理 bias 问题 [5]。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><blockquote>
<p>[1] J. Johnson et al., “Image retrieval using scene graphs,” in 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Boston, MA, USA, Jun. 2015, pp. 3668–3678, doi: 10.1109/CVPR.2015.7298990.</p>
<p>[2] D. Xu, Y. Zhu, C. B. Choy, and L. Fei-Fei, “Scene Graph Generation by Iterative Message Passing,” p. 10, 2017.</p>
<p>[3] R. Zellers, M. Yatskar, S. Thomson, and Y. Choi, “Neural Motifs: Scene Graph Parsing with Global Context,” in 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, Salt Lake City, UT, Jun. 2018, pp. 5831–5840, doi: 10.1109/CVPR.2018.00611.</p>
<p>[4] S. Ren, K. He, R. Girshick, and J. Sun, “Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 39, no. 6, pp. 1137–1149, 2017, doi: 10.1109/TPAMI.2016.2577031.</p>
<p>[5] K. Tang, Y. Niu, J. Huang, J. Shi, and H. Zhang, “Unbiased Scene Graph Generation from Biased Training,” ArXiv200211949 Cs, Mar. 2020, Accessed: Mar. 10, 2020. [Online]. Available: <a href="http://arxiv.org/abs/2002.11949">http://arxiv.org/abs/2002.11949</a>.</p>
<p>[6] C. Lu, R. Krishna, M. Bernstein, and L. Fei-Fei, “Visual Relationship Detection with Language Priors,” ArXiv160800187 Cs, Jul. 2016, Accessed: Mar. 04, 2020. [Online]. Available: <a href="http://arxiv.org/abs/1608.00187">http://arxiv.org/abs/1608.00187</a>.</p>
<p>[7] J. Yang, J. Lu, S. Lee, D. Batra, and D. Parikh, “Graph R-CNN for Scene Graph Generation,” ArXiv180800191 Cs, Aug. 2018, Accessed: Feb. 21, 2020. [Online]. Available: <a href="http://arxiv.org/abs/1808.00191">http://arxiv.org/abs/1808.00191</a>.</p>
<p>[8] R. Krishna et al., “Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations,” ArXiv160207332 Cs, Feb. 2016, Accessed: Feb. 22, 2020. [Online]. Available: <a href="http://arxiv.org/abs/1602.07332">http://arxiv.org/abs/1602.07332</a>.</p>
<p>[9] A. Zareian, S. Karaman, and S.-F. Chang, “Bridging Knowledge Graphs to Generate Scene Graphs,” ArXiv200102314 Cs, Jan. 2020, Accessed: Feb. 22, 2020. [Online]. Available: <a href="http://arxiv.org/abs/2001.02314">http://arxiv.org/abs/2001.02314</a>.</p>
<p>[10] V. S. Chen, P. Varma, R. Krishna, M. Bernstein, C. Re, and L. Fei-Fei, “Scene graph prediction with limited labels,” in Proceedings of the IEEE International Conference on Computer Vision, 2019, pp. 2580–2590.</p>
</blockquote>
]]></content>
      <categories>
        <category>任务讲解</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>cv</tag>
      </tags>
  </entry>
  <entry>
    <title>Bert</title>
    <url>/2021/04/08/Bert/</url>
    <content><![CDATA[<h2 id="语言模型的定义和BERT解读"><a href="#语言模型的定义和BERT解读" class="headerlink" title="语言模型的定义和BERT解读"></a>语言模型的定义和BERT解读</h2><p>什么是语言模型, 其实用一个公式就可以表示$P(c_{1},\ldots ,c_{m})$, 假设我们有一句话, $c_{1}$到$c_{m}$是这句话里的$m$个字, 而语言模型就是求的是这句话出现的概率是多少.   </p>
<p>比如说在一个语音识别的场景, 机器听到一句话是”wo wang dai san le(我忘带伞了)”, 然后机器解析出两个句子, 一个是”我网袋散了”, 另一个是”我忘带伞了”, 也就是前者的概率大于后者. 然后语言模型就可以判断$P(“我忘带伞了”) &gt; P(“我网袋散了”)$, 从而得出这句语音的正确解析结果是”我忘带伞了”.</p>
<p>BERT的全称是: Bidirectional Encoder Representations from Transformers, 如果翻译过来也就是<strong>双向transformer编码表达</strong>, 我们在上节课解读了transformer的编码器, 编码器输出的隐藏层就是自然语言序列的数学表达, 那么双向是什么意思呢? 我们来看一下下面这张图.</p>
<p><img src="/2021/04/08/Bert/bidirectional.png" alt></p>
<p>上图中$E_i$是指的单个字或词, $T_i$指的是最终计算得出的<strong>隐藏层</strong>, 还记得我们在Transformer(一)中讲到的注意力矩阵和注意力加权, 经过这样的操作之后, 序列里面的每一个字, <strong>都含有这个字前面的信息和后面的信息</strong>, 这就是<strong>双向</strong>的理解, 在这里, 一句话中每一个字, 经过注意力机制和加权之后, <strong>当前这个字等于用这句话中其他所有字重新表达了一遍</strong>, 每个字含有了这句话中所有成分的信息.</p>
<h3 id="在BERT中-主要是以两种预训练的方式来建立语言模型"><a href="#在BERT中-主要是以两种预训练的方式来建立语言模型" class="headerlink" title="在BERT中, 主要是以两种预训练的方式来建立语言模型:"></a>在BERT中, 主要是以两种预训练的方式来建立语言模型:</h3><h4 id="BERT语言模型任务一-MASKED-LM"><a href="#BERT语言模型任务一-MASKED-LM" class="headerlink" title="BERT语言模型任务一: MASKED LM"></a>BERT语言模型任务一: MASKED LM</h4><p>在BERT中, Masked LM(Masked language Model)构建了语言模型, 这也是BERT的预训练中任务之一, 简单来说, 就是<strong>随机遮盖或替换</strong>一句话里面任意字或词, 然后让模型通过上下文的理解预测那一个被遮盖或替换的部分, 之后<strong>做$Loss$的时候只计算被遮盖部分的$Loss$</strong>, 其实是一个很容易理解的任务, 实际操作方式如下:   </p>
<ol>
<li>随机把一句话中$15\%$的$token$替换成以下内容:<br>1) 这些$token$有$80\%$的几率被替换成$[mask]$;<br>2) 有$10 \%$的几率被替换成任意一个其他的$token$;<br>3) 有$10 \%$的几率原封不动.</li>
<li>之后让模型<strong>预测和还原</strong>被遮盖掉或替换掉的部分, 模型最终输出的隐藏层的计算结果的维度是:   </li>
</ol>
<p>$X_{hidden}: [batch_size, \ seq_len, \  embedding_dim]$   </p>
<p>我们初始化一个映射层的权重$W_{vocab}$:   </p>
<p>$W_{vocab}: [embedding_dim, \ vocab_size]$   </p>
<p>我们用$W_{vocab}$完成隐藏维度到字向量数量的映射, 只要求$X_{hidden}$和$W_{vocab}$的矩阵乘(点积):   </p>
<p>$X_{hidden}W_{vocab}: [batch_size, \ seq_len, \ vocab_size]$</p>
<p>之后把上面的计算结果在$vocab_size$(最后一个)维度做$softmax$归一化, 是每个字对应的$vocab_size$的和为$1$, 我们就可以通过$vocab_size$里概率最大的字来得到模型的预测结果, 就可以和我们准备好的$Label$做损失($Loss$)并反传梯度了.<br>注意做损失的时候, 只计算在第1步里当句中<strong>随机遮盖或替换</strong>的部分, 其余部分不做损失, 对于其他部分, 模型输出什么东西, 我们不在意.</p>
<h4 id="BERT语言模型任务二-Next-Sentence-Prediction"><a href="#BERT语言模型任务二-Next-Sentence-Prediction" class="headerlink" title="BERT语言模型任务二: Next Sentence Prediction"></a>BERT语言模型任务二: Next Sentence Prediction</h4><ol>
<li>首先我们拿到属于上下文的一对句子, 也就是两个句子, 之后我们要在这两段连续的句子里面加一些特殊$token$:<br>$[cls]$上一句话,$[sep]$下一句话.$[sep]$<br>也就是在句子开头加一个$[cls]$, 在两句话之中和句末加$[sep]$, 具体地就像下图一样:   </li>
</ol>
<p><img src="/2021/04/08/Bert/embeddings.png" alt></p>
<ol>
<li>我们看到上图中两句话是$[cls]$ my dog is cute $[sep]$ he likes playing $[sep]$, $[cls]$我的狗很可爱$[sep]$他喜欢玩耍$[sep]$, 除此之外, 我们还要准备同样格式的两句话, 但他们不属于上下文关系的情况;<br>$[cls]$我的狗很可爱$[sep]$企鹅不擅长飞行$[sep]$, 可见这属于上下句不属于上下文关系的情况;<br>在实际的训练中, 我们让上面两种情况出现的比例为$1:1$, 也就是一半的时间输出的文本属于上下文关系, 一半时间不是.</li>
<li>我们进行完上述步骤之后, 还要随机初始化一个可训练的$segment \ embeddings$, 见上图中, 作用就是用$embeddings$的信息让模型分开上下句, 我们一把给上句全$0$的$token$, 下句啊全$1$的$token$, 让模型得以判断上下句的起止位置, 例如:<br>$[cls]$我的狗很可爱$[sep]$企鹅不擅长飞行$[sep]$<br>$0 \quad  \  0 \ \ 0 \ \  0 \ \ 0 \ \ 0 \ \ 0 \ \  0 \ \ \ 1 \ \  1 \ \ 1 \ \ 1 \ \ 1 \ \ 1 \ \ 1 \ \ 1$<br>上面$0$和$1$就是$segment \ embeddings$.</li>
<li>还记得我们上节课说过的, 注意力机制就是, 让每句话中的每一个字对应的那一条向量里, 都融入这句话所有字的信息, 那么我们在最终隐藏层的计算结果里, 只要取出$[cls]token$所对应的一条向量, 里面就含有整个句子的信息, 因为我们期望这个句子里面所有信息都会往$[cls]token$所对应的一条向量里汇总:<br>模型最终输出的隐藏层的计算结果的维度是:   </li>
</ol>
<p>我们$X_{hidden}: [batch_size, \ seq_len, \  embedding_dim]$   </p>
<p>我们要取出$[cls]token$所对应的一条向量, $[cls]$对应着$\ seq_len$维度的第$0$条:   </p>
<p>$cls_vector = X_{hidden}[:, \ 0, \ :]$   </p>
<p>$cls_vector \in \mathbb{R}^{batch_size, \  embedding_dim}$   </p>
<p>之后我们再初始化一个权重, 完成从$embedding_dim$维度到$1$的映射, 也就是逻辑回归, 之后用$sigmoid$函数激活, 就得到了而分类问题的推断.<br>我们用$\hat{y}$来表示模型的输出的推断, 他的值介于$(0, \ 1)$之间:   </p>
<p>$\hat{y} = sigmoid(Linear(cls_vector)) \quad \hat{y} \in (0, \ 1)$</p>
]]></content>
      <categories>
        <category>模型</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>预训练</tag>
        <tag>词向量</tag>
      </tags>
  </entry>
  <entry>
    <title>Graph-R-CNN-for-Scene-Graph-Generation</title>
    <url>/2021/04/09/Graph-R-CNN-for-Scene-Graph-Generation/</url>
    <content><![CDATA[<h1 id="Graph-R-CNN-for-Scene-Graph-Generation"><a href="#Graph-R-CNN-for-Scene-Graph-Generation" class="headerlink" title="Graph R-CNN for Scene Graph Generation"></a>Graph R-CNN for Scene Graph Generation</h1><h5 id="论文来源：ECCV-2018"><a href="#论文来源：ECCV-2018" class="headerlink" title="论文来源：ECCV 2018"></a>论文来源：ECCV 2018</h5><h5 id="论文链接：https-openaccess-thecvf-com-content-ECCV-2018-papers-Jianwei-Yang-Graph-R-CNN-for-ECCV-2018-paper-pdf"><a href="#论文链接：https-openaccess-thecvf-com-content-ECCV-2018-papers-Jianwei-Yang-Graph-R-CNN-for-ECCV-2018-paper-pdf" class="headerlink" title="论文链接：https://openaccess.thecvf.com/content_ECCV_2018/papers/Jianwei_Yang_Graph_R-CNN_for_ECCV_2018_paper.pdf"></a>论文链接：<a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Jianwei_Yang_Graph_R-CNN_for_ECCV_2018_paper.pdf">https://openaccess.thecvf.com/content_ECCV_2018/papers/Jianwei_Yang_Graph_R-CNN_for_ECCV_2018_paper.pdf</a></h5><h5 id="代码链接：https-github-com-jwyang-graph-rcnn-pytorch"><a href="#代码链接：https-github-com-jwyang-graph-rcnn-pytorch" class="headerlink" title="代码链接：https://github.com/jwyang/graph-rcnn.pytorch"></a>代码链接：<a href="https://github.com/jwyang/graph-rcnn.pytorch">https://github.com/jwyang/graph-rcnn.pytorch</a></h5><hr>
<p>本文提出的三个主要的创新点就是：</p>
<p>1． Relation Proposal Network（RePN）用来做关系过滤</p>
<p>2． Attentional Graph Convolutional Network 用来做信息融合</p>
<p>3． 提出了一个新指标SGGen+，比原来的SGGen更科学</p>
<p>然后集合这些东西，本文的方法是目前最佳</p>
<h2 id="Approach"><a href="#Approach" class="headerlink" title="Approach"></a>Approach</h2><script type="math/tex; mode=display">P(\mathcal{S} | \boldsymbol{I})=\overbrace{P(\boldsymbol{V} | \boldsymbol{I})}^{\text {Object Region }} \underbrace{P(\boldsymbol{E} | \boldsymbol{V}, \boldsymbol{I})}_{\text {Pelationship }} \overbrace{P(\boldsymbol{R}, \boldsymbol{O} | \boldsymbol{V}, \boldsymbol{E}, \boldsymbol{I})}^{\text {Graph Labeling }}</script><p>本文的大致研究结构和主流操作一致，如下：</p>
<p>基本的想法还是按照先找点，再找边，再找点和边的label。找点采用常见的faster-rcnn，这也是大多数工作的做法。</p>
<h3 id="Relation-Proposal-Network"><a href="#Relation-Proposal-Network" class="headerlink" title="Relation Proposal Network"></a>Relation Proposal Network</h3><p>这里的关系配对，作者采用物体分类概率作为主要的计算依据。一个直观的感觉就是person会和clothes产生关系，而elephant肯定不会和clothes产生关系，本文想利用这种先验知识，首先淘汰掉一批不可能的关系。</p>
<p>具体的做法肯定是构建一种由object分类概率和subject分类概率决定的函数关系</p>
<script type="math/tex; mode=display">s_{ij} = f (p^o_i, p^o_j)</script><p>这个分数的高低决定了相关程度。最直观的想法肯定是用一个全连接去做，但是作者认为这样计算量太大。</p>
<p>所以作者换用如下的方法：</p>
<script type="math/tex; mode=display">f\left(\boldsymbol{p}_{i}^{o}, \boldsymbol{p}_{j}^{o}\right)=\left\langle\Phi\left(\boldsymbol{p}_{i}^{o}\right), \Psi\left(\boldsymbol{p}_{j}^{o}\right)\right\rangle, i \neq j</script><p>$\Phi(\cdot)$ 和 $\Psi(\cdot)$分别代表在关系中主语和宾语对映射函数。这个分解使得，仅使用$X^o$的两个投影过程，然后执行一次矩阵乘法就能获得分数矩阵$S=\left\{s_{i j}\right\}^{n \times n}$。</p>
<p>这样只需要两个 MLP，加一个矩阵乘法，这样的计算复杂度非常的小。</p>
<p>在获得分数矩阵后，我们将其降序排序，然后选择前K对。然后，我们使用非最大抑制(NMS)来过滤出与其他对象有明显重叠的对象对。每个关系都有一对边界框，组合顺序很重要。我们计算两个对象对$\{u, v\}$和$\{p, q\}$ 之间对重叠：</p>
<script type="math/tex; mode=display">\operatorname{IoU}(\{u, v\},\{p, q\})=\frac{I\left(r_{u}^{o}, r_{p}^{o}\right)+I\left(r_{v}^{o}, r_{q}^{o}\right)}{U\left(r_{u}^{o}, r_{p}^{o}\right)+U\left(r_{v}^{o}, r_{q}^{o}\right)}</script><p>$I$计算两个box交集的区域，$U$计算并集区域。剩余的$m$个对象对被认为是具有意义关系$E$的候选对象。利用$E$，我们得到了一个图$G = (V,E)$，它比原来的全连通图稀疏得多。</p>
<h3 id="Attentional-GCN"><a href="#Attentional-GCN" class="headerlink" title="Attentional GCN"></a>Attentional GCN</h3><p>GCN是图卷积网络，其实和传统的卷积网络的思路是一样的，不理解的同学可以看看下图。卷积可以看作九宫格的中心点和其他点都连接在一起，那图卷积的意思就是去掉部分连接，这样就形成了图特有的拓扑结构。图中每一个点都会有其相联的点，这样就可以用相联的点来更新本点。所有点都做一次这样的操作相当于进行了一次图卷积操作。</p>
<script type="math/tex; mode=display">\boldsymbol{z}_{i}^{(l+1)}=\sigma\left(\boldsymbol{z}_{i}^{(l)}+\sum_{j \in \mathcal{N}(i)} \alpha_{i j} W \boldsymbol{z}_{j}^{(l)}\right)</script><script type="math/tex; mode=display">\boldsymbol{z}_{i}^{(l+1)}=\sigma\left(W Z^{(l)} \boldsymbol{\alpha}_{i}\right)</script><p>这个式子用来计算图卷积的结果：</p>
<p>这个式子的含义是，将节点信息用W做一个映射，再加权求和并累加到目标节点，经过激活函数就得出了新的节点信息。</p>
<p>想把这个式子应用到场景图中，存在一个问题。那就是我不知道边与边的权值啊，平均分肯定是不合适，所以本文作者就弄出了一个aGCN，来自动生成边上面的权值。</p>
<script type="math/tex; mode=display">u_{i j}=w_{h}^{T} \sigma\left(W_{a}\left[\boldsymbol{z}_{i}^{(l)}, \boldsymbol{z}_{j}^{(l)}\right]\right)</script><script type="math/tex; mode=display">\boldsymbol{\alpha}_{i}=\operatorname{softmax}\left(\boldsymbol{u}_{i}\right)</script><p>使用之前经过RePN的图，我们就得到了所有的物体边和点。作者受到《Relation networks for object detection.》这篇文章的启发，在所有的点之间也加入了快速连接，这样图中就存在着（点-点）（点-边）的连接。之后使用之前提出的图卷积的做法，作者指出了点与边更新的式子：</p>
<script type="math/tex; mode=display">
\boldsymbol{z}_{i}^{o}=\sigma(\overbrace{W^{\text {skip }} Z^{o} \boldsymbol{\alpha}^{\mathbf{s k i p}}}^{\text {Message from Other Objects}}+\overbrace{W^{s r} Z^{r} \boldsymbol{\alpha}^{s r}+W^{o r} Z^{r} \boldsymbol{\alpha}^{o r}}^{\text {Messages from Neighboring Relationships}})</script><script type="math/tex; mode=display">
\boldsymbol{z}_{i}^{r}=\sigma(\boldsymbol{z}_{i}^{r}+\underbrace{W^{r s} Z^{o} \boldsymbol{\alpha}^{r s}+W^{r o} Z^{o} \boldsymbol{\alpha}^{r o}}_{\text {Messages from Neighboring Objects }})</script><p>这个式子其实也很好理解，点的信息使用点和边信息来更新，边的信息只使用点的信息来更新，因为点之间存在着快速连接，而边之间没有。</p>
<p>接下来作者再次想到一个问题，如何初始化点与边的信息呢？作者的做法是使用了两种aGCN，一种初始化信息使用分类信息，第二种初始化信息使用视觉信息。</p>
]]></content>
      <categories>
        <category>paper</category>
      </categories>
      <tags>
        <tag>cv</tag>
        <tag>scene graph generation</tag>
      </tags>
  </entry>
  <entry>
    <title>VQA相关方法的简单综述</title>
    <url>/2021/04/11/VQA%E7%9B%B8%E5%85%B3%E6%96%B9%E6%B3%95%E7%9A%84%E7%AE%80%E5%8D%95%E7%BB%BC%E8%BF%B0/</url>
    <content><![CDATA[<h1 id="VQA相关方法的简单综述"><a href="#VQA相关方法的简单综述" class="headerlink" title="VQA相关方法的简单综述"></a>VQA相关方法的简单综述</h1><h5 id="原文链接："><a href="#原文链接：" class="headerlink" title="原文链接："></a>原文链接：</h5><p><a href="https://zhuanlan.zhihu.com/p/59530688">https://zhuanlan.zhihu.com/p/59530688</a></p>
<p><a href="https://www.cnblogs.com/limitlessun/p/13532857.html">https://www.cnblogs.com/limitlessun/p/13532857.html</a></p>
<hr>
<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>VQA指的是，给定一张图片和一个与该图片相关的自然语言问题，计算机能产生一个正确的回答。 显然，这是一个典型的多模态问题，融合了CV与NLP的技术，计算机需要同时学会理解图像和文字。正因如此，直到相关技术取得突破式发展的2015年，VQA的概念才被正式提出。</p>
<p>可见，VQA仍然是一个非常新颖的研究方向，但它很容易让人联想到其他两个已经被研究较久的领域：文本QA和Image Captioning。</p>
<p>文本QA即纯文本的回答，计算机根据文本形式的材料回答问题。与之相比，VQA把材料换成了图片形式，从而引入了一系列新的问题：</p>
<ul>
<li>图像是更高维度的数据，比纯文本具有更多的噪声。</li>
<li>文本是结构化的，也具备一定的语法规则，而图像则不然。</li>
<li>文本本身即是对真实世界的高度抽象，而图像的抽象程度较低，可以展现更丰富的信息，同时也更难被计算机“理解”。</li>
</ul>
<p>与Image Captioning这种看图说话的任务相比，VQA的难度也显得更大。因为Image Captioning更像是把图像“翻译”成文本，只需把图像内容映射成文本再加以结构化整理即可，而VQA需要更好地理解图像内容并进行一定的推理，有时甚至还需要借助外部的知识库。 然而，VQA的评估方法更为简单，因为答案往往是客观并简短的，很容易与ground truth对比判断是否准确，不像Image Captioning需要对长句子做评估。</p>
<p>总之，VQA是一个非常具有挑战性的问题，正处于方兴未艾的阶段，有许多“坑”正等着未来的研究者去填。</p>
<h2 id="常用方法"><a href="#常用方法" class="headerlink" title="常用方法"></a>常用方法</h2><p>吴琦等人的综述发表于2016年，所以仅能对2015至2016年的相关工作进行总结。即便如此，我们还是能看到，短短两年的时间，VQA领域就涌现了不少的成果。 吴琦等人把这些方法分为四大类，分别是Joint embedding approaches、Attention mechanisms、Compositional Models和Models using external knowledge base 。</p>
<h3 id="Joint-embedding-approaches"><a href="#Joint-embedding-approaches" class="headerlink" title="Joint embedding approaches"></a>Joint embedding approaches</h3><p>Joint embedding是处理多模态问题时的经典思路，在这里指对图像和问题进行联合编码。该方法的示意图为：<br><img src="/2021/04/11/VQA%E7%9B%B8%E5%85%B3%E6%96%B9%E6%B3%95%E7%9A%84%E7%AE%80%E5%8D%95%E7%BB%BC%E8%BF%B0/img1.png" alt></p>
<p>首先，图像和问题分别由CNN和RNN进行第一次编码得到各自的特征，随后共同输入到另一个编码器中得到joint embedding，最后通过解码器输出答案。 值得注意的是，有的工作把VQA视为序列生成问题，而有的则把VQA简化为一个答案范围可预知的分类问题。在前者的设定下，解码器是一个RNN，输出长度不等的序列；后者的解码器则是一个分类器，从预定义的词汇表中选择答案。</p>
<h3 id="Attention-mechanisms"><a href="#Attention-mechanisms" class="headerlink" title="Attention mechanisms"></a>Attention mechanisms</h3><p>attention机制起源于机器翻译问题，目的是让模型动态地调整对输入项各部分的关注度，从而提升模型的“专注力”。而自从Xu等人将attention机制成功运用到Image Captioning中，attention机制在视觉任务中受到越来越多的关注，应用到VQA中也是再自然不过。下面就是将attention机制应用到上个方法中的示意图。<br><img src="/2021/04/11/VQA%E7%9B%B8%E5%85%B3%E6%96%B9%E6%B3%95%E7%9A%84%E7%AE%80%E5%8D%95%E7%BB%BC%E8%BF%B0/img2.png" alt></p>
<p>相关的工作表明，加入attention机制能获得明显的提升，从直观上也比较容易理解：在attention机制的作用下，模型在根据图像和问题进行推断时不得不强制判断“该往哪看”，比起原本盲目地全局搜索，模型能够更有效地捕捉关键图像部位。</p>
<h3 id="Compositional-Models"><a href="#Compositional-Models" class="headerlink" title="Compositional Models"></a>Compositional Models</h3><p>Compositional Models的核心思想是将设计一种模块化的模型。</p>
<p>这方面的一个典型代表是Andreas等人的Neural Module Networks。其最大的特点是根据问题的类型动态组装模块来产生答案。<br><img src="/2021/04/11/VQA%E7%9B%B8%E5%85%B3%E6%96%B9%E6%B3%95%E7%9A%84%E7%AE%80%E5%8D%95%E7%BB%BC%E8%BF%B0/img3.png" alt></p>
<p>比如，在上面的例子中，当面对 What color is his tie? 这个问题时，模型首先利用parser对问题进行语法解析，接着判断出需要用到attend和classify这两个模块，然后判断出这两个模块的连接方式。最终，模型的推理过程是，先把注意力集中到tie上，然后对其color进行分类，得出答案。<br><img src="/2021/04/11/VQA%E7%9B%B8%E5%85%B3%E6%96%B9%E6%B3%95%E7%9A%84%E7%AE%80%E5%8D%95%E7%BB%BC%E8%BF%B0/img4.png" alt></p>
<p>而在另一个例子中，当面对 Is there a red shape above a circle? 这种更为复杂的问题时，模型选择的模块也自动变得复杂了许多。</p>
<p>吴琦等人列举的另一个典型代表是Xiong等人的Dynamic Memory Networks。该网络由四个主要的模块构成，分别是表征图像的input module、表征问题的question module、作为内存的episodic memory module和产生答案的answer module。 模型运作过程如下图。<br><img src="/2021/04/11/VQA%E7%9B%B8%E5%85%B3%E6%96%B9%E6%B3%95%E7%9A%84%E7%AE%80%E5%8D%95%E7%BB%BC%E8%BF%B0/img5.jpg" alt></p>
<h3 id="Models-using-external-knowledge-base"><a href="#Models-using-external-knowledge-base" class="headerlink" title="Models using external knowledge base"></a>Models using external knowledge base</h3><p>虽然VQA要解决的是看图回答问题的任务，但实际上，很多问题往往需要具备一定的先验知识才能回答。例如，为了回答“图上有多少只哺乳动物”这样的问题，模型必须得知道“哺乳动物”的定义，而不是单纯理解图像内容。因此，把知识库加入VQA模型中就成了一个很有前景的研究方向。 这方面做得比较好的工作是吴琦作为一作发表的另一篇论文Ask Me Anything: Free-form Visual Question Answering Based on Knowledge from External Sources。 该工作的模型框架如下。</p>
<p><img src="/2021/04/11/VQA%E7%9B%B8%E5%85%B3%E6%96%B9%E6%B3%95%E7%9A%84%E7%AE%80%E5%8D%95%E7%BB%BC%E8%BF%B0/img6.jpg" alt></p>
<p>模型虽然看似复杂，但理解起来不外乎以下几个要点：</p>
<ul>
<li>红色部分表示，对图像进行多标签分类，得到图像标签（attribute）。</li>
<li>蓝色部分表示，把上述图像标签中最明显的5个标签输入知识库DBpedia中检索出相关内容，然后利用Doc2Vec进行编码。</li>
<li>绿色部分表示，利用上述图像标签生成多个图像描述（caption），将这一组图像描述编码。</li>
<li>以上三项同时输入到一个Seq2Seq模型中作为其初始状态，然后该Seq2Seq模型将问题进行编码，解码出最终答案，并用MLE的方法进行训练。</li>
</ul>
<h3 id="Datasets-and-evaluation"><a href="#Datasets-and-evaluation" class="headerlink" title="Datasets and evaluation"></a>Datasets and evaluation</h3><p>VQA的data至少为一个image、question、answer的三元组。</p>
<h4 id="DAQUAR："><a href="#DAQUAR：" class="headerlink" title="DAQUAR："></a>DAQUAR：</h4><p>First VQA dataset designed as benchmark，DAtaset for QUestion Answering on Real-world images。1449 RGBD images，795 training，654 testing。有2种问答pairs，一种是使用8种模板合成的，一种是人工标注的。共有12468个问题，6794 training，5674 testing。</p>
<p><img src="/2021/04/11/VQA%E7%9B%B8%E5%85%B3%E6%96%B9%E6%B3%95%E7%9A%84%E7%AE%80%E5%8D%95%E7%BB%BC%E8%BF%B0/img7.png" alt></p>
<h4 id="COCO-QA："><a href="#COCO-QA：" class="headerlink" title="COCO-QA："></a>COCO-QA：</h4><p>123287 images，72783 training，38948 testing。每个image有一个问答pair，由COCO dataset中的image descriptions转换而来<br><img src="/2021/04/11/VQA%E7%9B%B8%E5%85%B3%E6%96%B9%E6%B3%95%E7%9A%84%E7%AE%80%E5%8D%95%E7%BB%BC%E8%BF%B0/img8.png" alt></p>
<h4 id="VQA："><a href="#VQA：" class="headerlink" title="VQA："></a>VQA：</h4><p>使用最广泛的数据集之一，2017年更新为VQA v2.0，包含使用真实图片的VQA-real和卡通图片的VQA-abstract。VQA-real包含123287 training和81424 test images from COCO，由真人提供开放型和是非型问题和多种候选答案，共614163个questions。VQA-abstract包括50000scenes，每个scene对应3个questions<br><img src="/2021/04/11/VQA%E7%9B%B8%E5%85%B3%E6%96%B9%E6%B3%95%E7%9A%84%E7%AE%80%E5%8D%95%E7%BB%BC%E8%BF%B0/img9.png" alt></p>
<h4 id="Visual-Genome："><a href="#Visual-Genome：" class="headerlink" title="Visual Genome："></a>Visual Genome：</h4><p>1.7 million questions/answer pairs，问题包括free-form和region-based两种形式，比VQA-real更具多样性。<br><img src="/2021/04/11/VQA%E7%9B%B8%E5%85%B3%E6%96%B9%E6%B3%95%E7%9A%84%E7%AE%80%E5%8D%95%E7%BB%BC%E8%BF%B0/img10.png" alt></p>
<h4 id="Visual7W："><a href="#Visual7W：" class="headerlink" title="Visual7W："></a>Visual7W：</h4><p>Visual Genome的一个子集。</p>
<h4 id="Other-datasets："><a href="#Other-datasets：" class="headerlink" title="Other datasets："></a>Other datasets：</h4><p>使用外部知识库的KB-VQA、FVQA，较简单的Diagrams、Shapes。</p>
<h3 id="Evaluation-Measures"><a href="#Evaluation-Measures" class="headerlink" title="Evaluation Measures"></a>Evaluation Measures</h3><p>为了对answer进行评估，句法和语义的正确性都需要考虑，因此大多数VQA datasets的answer被限制为3个words以下。</p>
<p><strong>WUPS：</strong> Wu-Palmer Similarity，在taxonomy tree中比较两者的common subsequence，当similarity超过某一阈值就认为是正确答案，一般使用0.9和0.0两种阈值。</p>
<p>在VQA数据集中，只有当3个以上的人（共10个）提供了该答案，才认为给出的答案是正确的</p>
]]></content>
  </entry>
  <entry>
    <title>渗透测试学习链接</title>
    <url>/2021/04/13/%E6%B8%97%E9%80%8F%E6%B5%8B%E8%AF%95%E5%AD%A6%E4%B9%A0%E9%93%BE%E6%8E%A5/</url>
    <content><![CDATA[<h3 id="Nmap常用指令和教程："><a href="#Nmap常用指令和教程：" class="headerlink" title="Nmap常用指令和教程："></a>Nmap常用指令和教程：</h3><p><a href="https://www.sqlsec.com/2017/07/nmap.html">https://www.sqlsec.com/2017/07/nmap.html</a></p>
<h3 id="dsniff-和-Ettercap-和-bettercap-详解-网络嗅探工具包："><a href="#dsniff-和-Ettercap-和-bettercap-详解-网络嗅探工具包：" class="headerlink" title="dsniff 和 Ettercap 和 bettercap 详解 - 网络嗅探工具包："></a>dsniff 和 Ettercap 和 bettercap 详解 - 网络嗅探工具包：</h3><p><a href="https://blog.csdn.net/freeking101/article/details/73105561">https://blog.csdn.net/freeking101/article/details/73105561</a></p>
]]></content>
  </entry>
  <entry>
    <title>Code-NNLM</title>
    <url>/2021/04/15/Code-NNLM/</url>
    <content><![CDATA[<h1 id="NNLM"><a href="#NNLM" class="headerlink" title="NNLM"></a>NNLM</h1><h2 id="来源：https-github-com-graykode-nlp-tutorial"><a href="#来源：https-github-com-graykode-nlp-tutorial" class="headerlink" title="来源：https://github.com/graykode/nlp-tutorial"></a>来源：<a href="https://github.com/graykode/nlp-tutorial">https://github.com/graykode/nlp-tutorial</a></h2><p><img src="/2021/04/15/Code-NNLM/img1.png" width="80%"></p>
<center style="font-size:14px;color:#C0C0C0;text-decoration:underline">NNLM 网络结构图</center>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># %%</span></span><br><span class="line"><span class="comment"># code by Tae Hwan Jung @graykode</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">make_batch</span>():</span><br><span class="line">    input_batch = []</span><br><span class="line">    target_batch = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> sen <span class="keyword">in</span> sentences:</span><br><span class="line">        word = sen.split() <span class="comment"># space tokenizer</span></span><br><span class="line">        <span class="built_in">input</span> = [word_dict[n] <span class="keyword">for</span> n <span class="keyword">in</span> word[:-<span class="number">1</span>]] <span class="comment"># create (1~n-1) as input</span></span><br><span class="line">        target = word_dict[word[-<span class="number">1</span>]] <span class="comment"># create (n) as target, We usually call this &#x27;casual language model&#x27;</span></span><br><span class="line"></span><br><span class="line">        input_batch.append(<span class="built_in">input</span>)</span><br><span class="line">        target_batch.append(target)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> input_batch, target_batch</span><br><span class="line"></span><br><span class="line"><span class="comment"># Model</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">NNLM</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(NNLM, self).__init__()</span><br><span class="line">        self.C = nn.Embedding(n_class, m)</span><br><span class="line">        self.H = nn.Linear(n_step * m, n_hidden, bias=<span class="literal">False</span>)</span><br><span class="line">        self.d = nn.Parameter(torch.ones(n_hidden))</span><br><span class="line">        self.U = nn.Linear(n_hidden, n_class, bias=<span class="literal">False</span>)</span><br><span class="line">        self.W = nn.Linear(n_step * m, n_class, bias=<span class="literal">False</span>)</span><br><span class="line">        self.b = nn.Parameter(torch.ones(n_class))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        X = self.C(X) <span class="comment"># X : [batch_size, n_step, n_class]</span></span><br><span class="line">        X = X.view(-<span class="number">1</span>, n_step * m) <span class="comment"># [batch_size, n_step * n_class]</span></span><br><span class="line">        tanh = torch.tanh(self.d + self.H(X)) <span class="comment"># [batch_size, n_hidden]</span></span><br><span class="line">        output = self.b + self.W(X) + self.U(tanh) <span class="comment"># [batch_size, n_class]</span></span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    n_step = <span class="number">2</span> <span class="comment"># number of steps, n-1 in paper</span></span><br><span class="line">    n_hidden = <span class="number">2</span> <span class="comment"># number of hidden size, h in paper</span></span><br><span class="line">    m = <span class="number">2</span> <span class="comment"># embedding size, m in paper</span></span><br><span class="line"></span><br><span class="line">    sentences = [<span class="string">&quot;i like dog&quot;</span>, <span class="string">&quot;i love coffee&quot;</span>, <span class="string">&quot;i hate milk&quot;</span>]</span><br><span class="line"></span><br><span class="line">    word_list = <span class="string">&quot; &quot;</span>.join(sentences).split()</span><br><span class="line">    word_list = <span class="built_in">list</span>(<span class="built_in">set</span>(word_list))</span><br><span class="line">    word_dict = &#123;w: i <span class="keyword">for</span> i, w <span class="keyword">in</span> <span class="built_in">enumerate</span>(word_list)&#125;</span><br><span class="line">    number_dict = &#123;i: w <span class="keyword">for</span> i, w <span class="keyword">in</span> <span class="built_in">enumerate</span>(word_list)&#125;</span><br><span class="line">    n_class = <span class="built_in">len</span>(word_dict)  <span class="comment"># number of Vocabulary</span></span><br><span class="line"></span><br><span class="line">    model = NNLM()</span><br><span class="line"></span><br><span class="line">    criterion = nn.CrossEntropyLoss()</span><br><span class="line">    optimizer = optim.Adam(model.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line"></span><br><span class="line">    input_batch, target_batch = make_batch()</span><br><span class="line">    input_batch = torch.LongTensor(input_batch)</span><br><span class="line">    target_batch = torch.LongTensor(target_batch)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Training</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5000</span>):</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        output = model(input_batch)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># output : [batch_size, n_class], target_batch : [batch_size]</span></span><br><span class="line">        loss = criterion(output, target_batch)</span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;Epoch:&#x27;</span>, <span class="string">&#x27;%04d&#x27;</span> % (epoch + <span class="number">1</span>), <span class="string">&#x27;cost =&#x27;</span>, <span class="string">&#x27;&#123;:.6f&#125;&#x27;</span>.<span class="built_in">format</span>(loss))</span><br><span class="line"></span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Predict</span></span><br><span class="line">    predict = model(input_batch).data.<span class="built_in">max</span>(<span class="number">1</span>, keepdim=<span class="literal">True</span>)[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Test</span></span><br><span class="line">    <span class="built_in">print</span>([sen.split()[:<span class="number">2</span>] <span class="keyword">for</span> sen <span class="keyword">in</span> sentences], <span class="string">&#x27;-&gt;&#x27;</span>, [number_dict[n.item()] <span class="keyword">for</span> n <span class="keyword">in</span> predict.squeeze()])</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>模型代码</category>
      </categories>
      <tags>
        <tag>NNLM</tag>
      </tags>
  </entry>
  <entry>
    <title>Code-Word2Vec</title>
    <url>/2021/04/15/Code-Word2Vec/</url>
    <content><![CDATA[<p><img src="/2021/04/15/Code-Word2Vec/img1.png" width="80%"></p>
<center style="font-size:14px;color:#C0C0C0;text-decoration:underline">图 1</center>

<h1 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h1><h2 id="来源：https-github-com-graykode-nlp-tutorial"><a href="#来源：https-github-com-graykode-nlp-tutorial" class="headerlink" title="来源：https://github.com/graykode/nlp-tutorial"></a>来源：<a href="https://github.com/graykode/nlp-tutorial">https://github.com/graykode/nlp-tutorial</a></h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># %%</span></span><br><span class="line"><span class="comment"># code by Tae Hwan Jung @graykode</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">random_batch</span>():</span><br><span class="line">    random_inputs = []</span><br><span class="line">    random_labels = []</span><br><span class="line">    random_index = np.random.choice(<span class="built_in">range</span>(<span class="built_in">len</span>(skip_grams)), batch_size, replace=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> random_index:</span><br><span class="line">        random_inputs.append(np.eye(voc_size)[skip_grams[i][<span class="number">0</span>]])  <span class="comment"># target</span></span><br><span class="line">        random_labels.append(skip_grams[i][<span class="number">1</span>])  <span class="comment"># context word</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> random_inputs, random_labels</span><br><span class="line"></span><br><span class="line"><span class="comment"># Model</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Word2Vec</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Word2Vec, self).__init__()</span><br><span class="line">        <span class="comment"># W and WT is not Traspose relationship</span></span><br><span class="line">        self.W = nn.Linear(voc_size, embedding_size, bias=<span class="literal">False</span>) <span class="comment"># voc_size &gt; embedding_size Weight</span></span><br><span class="line">        self.WT = nn.Linear(embedding_size, voc_size, bias=<span class="literal">False</span>) <span class="comment"># embedding_size &gt; voc_size Weight</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="comment"># X : [batch_size, voc_size]</span></span><br><span class="line">        hidden_layer = self.W(X) <span class="comment"># hidden_layer : [batch_size, embedding_size]</span></span><br><span class="line">        output_layer = self.WT(hidden_layer) <span class="comment"># output_layer : [batch_size, voc_size]</span></span><br><span class="line">        <span class="keyword">return</span> output_layer</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    batch_size = <span class="number">2</span> <span class="comment"># mini-batch size</span></span><br><span class="line">    embedding_size = <span class="number">2</span> <span class="comment"># embedding size</span></span><br><span class="line"></span><br><span class="line">    sentences = [<span class="string">&quot;apple banana fruit&quot;</span>, <span class="string">&quot;banana orange fruit&quot;</span>, <span class="string">&quot;orange banana fruit&quot;</span>,</span><br><span class="line">                 <span class="string">&quot;dog cat animal&quot;</span>, <span class="string">&quot;cat monkey animal&quot;</span>, <span class="string">&quot;monkey dog animal&quot;</span>]</span><br><span class="line"></span><br><span class="line">    word_sequence = <span class="string">&quot; &quot;</span>.join(sentences).split()</span><br><span class="line">    word_list = <span class="string">&quot; &quot;</span>.join(sentences).split()</span><br><span class="line">    word_list = <span class="built_in">list</span>(<span class="built_in">set</span>(word_list))</span><br><span class="line">    word_dict = &#123;w: i <span class="keyword">for</span> i, w <span class="keyword">in</span> <span class="built_in">enumerate</span>(word_list)&#125;</span><br><span class="line">    voc_size = <span class="built_in">len</span>(word_list)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Make skip gram of one size window</span></span><br><span class="line">    skip_grams = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(word_sequence) - <span class="number">1</span>):</span><br><span class="line">        target = word_dict[word_sequence[i]]</span><br><span class="line">        context = [word_dict[word_sequence[i - <span class="number">1</span>]], word_dict[word_sequence[i + <span class="number">1</span>]]]</span><br><span class="line">        <span class="keyword">for</span> w <span class="keyword">in</span> context:</span><br><span class="line">            skip_grams.append([target, w])</span><br><span class="line"></span><br><span class="line">    model = Word2Vec()</span><br><span class="line"></span><br><span class="line">    criterion = nn.CrossEntropyLoss()</span><br><span class="line">    optimizer = optim.Adam(model.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Training</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5000</span>):</span><br><span class="line">        input_batch, target_batch = random_batch()</span><br><span class="line">        input_batch = torch.Tensor(input_batch)</span><br><span class="line">        target_batch = torch.LongTensor(target_batch)</span><br><span class="line"></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        output = model(input_batch)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># output : [batch_size, voc_size], target_batch : [batch_size] (LongTensor, not one-hot)</span></span><br><span class="line">        loss = criterion(output, target_batch)</span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;Epoch:&#x27;</span>, <span class="string">&#x27;%04d&#x27;</span> % (epoch + <span class="number">1</span>), <span class="string">&#x27;cost =&#x27;</span>, <span class="string">&#x27;&#123;:.6f&#125;&#x27;</span>.<span class="built_in">format</span>(loss))</span><br><span class="line"></span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, label <span class="keyword">in</span> <span class="built_in">enumerate</span>(word_list):</span><br><span class="line">        W, WT = model.parameters()</span><br><span class="line">        x, y = W[<span class="number">0</span>][i].item(), W[<span class="number">1</span>][i].item()</span><br><span class="line">        plt.scatter(x, y)</span><br><span class="line">        plt.annotate(label, xy=(x, y), xytext=(<span class="number">5</span>, <span class="number">2</span>), textcoords=<span class="string">&#x27;offset points&#x27;</span>, ha=<span class="string">&#x27;right&#x27;</span>, va=<span class="string">&#x27;bottom&#x27;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
  </entry>
  <entry>
    <title>CRF（条件随机场）</title>
    <url>/2021/04/17/CRF%EF%BC%88%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA%EF%BC%89/</url>
    <content><![CDATA[<h1 id="CRF-条件随机场"><a href="#CRF-条件随机场" class="headerlink" title="CRF(条件随机场)"></a>CRF(条件随机场)</h1><h5 id="参考视频：https-www-bilibili-com-video-BV19t411R7QU"><a href="#参考视频：https-www-bilibili-com-video-BV19t411R7QU" class="headerlink" title="参考视频：https://www.bilibili.com/video/BV19t411R7QU"></a>参考视频：<a href="https://www.bilibili.com/video/BV19t411R7QU">https://www.bilibili.com/video/BV19t411R7QU</a></h5><h5 id="PDF笔记来源：https-github-com-ws13685555932-machine-learning-derivation"><a href="#PDF笔记来源：https-github-com-ws13685555932-machine-learning-derivation" class="headerlink" title="PDF笔记来源：https://github.com/ws13685555932/machine_learning_derivation"></a>PDF笔记来源：<a href="https://github.com/ws13685555932/machine_learning_derivation">https://github.com/ws13685555932/machine_learning_derivation</a></h5><p>

	<div class="row">
    <embed src="../../../../file/17条件随机场.pdf" width="100%" height="550" type="application/pdf">
	</div>


</p>]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title>统计学习方法</title>
    <url>/2021/04/19/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<h2 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a>朴素贝叶斯</h2><script type="math/tex; mode=display">
\begin{aligned}
P (Y \mid X) &= \frac{P (X \mid Y)P (Y)}{P (X)}\\
&= \frac{P (X \mid Y)P (Y)}{\sum_{i=1}^{ \N}  P (X \mid Y_{i})P (Y_{i})} \\
&= \frac{ \prod_{j=1}^{ \mathbb{D}}P (x_{j} \mid Y)  P (Y)}{\sum_{i=1}^{ \N}  P (X \mid Y_{i})P (Y_{i})} \\

\end{aligned}</script>]]></content>
  </entry>
  <entry>
    <title>自用Vim配置及快捷键速查</title>
    <url>/2021/04/25/%E8%87%AA%E7%94%A8Vim%E9%85%8D%E7%BD%AE%E5%8F%8A%E5%BF%AB%E6%8D%B7%E9%94%AE%E9%80%9F%E6%9F%A5/</url>
    <content><![CDATA[<h1 id="自用Vim配置及快捷键速查"><a href="#自用Vim配置及快捷键速查" class="headerlink" title="自用Vim配置及快捷键速查"></a>自用Vim配置及快捷键速查</h1><h2 id="配置文件地址"><a href="#配置文件地址" class="headerlink" title="配置文件地址"></a>配置文件地址</h2><p><a href="https://github.com/Lyon52222/neovim">https://github.com/Lyon52222/neovim</a></p>
<h2 id="快捷键"><a href="#快捷键" class="headerlink" title="快捷键"></a>快捷键</h2><h3 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h3><p>这里我只列举初一些常用的，需要记住的，更多的操作查看上一篇博客：Vim操作大全</p>
<h4 id="移动"><a href="#移动" class="headerlink" title="移动"></a>移动</h4><pre><code>gg -&gt; 跳转到文档第一行
G -&gt; 跳转到文档最后一行
nG,ngg -&gt; 跳转到第n行
% -&gt; 跳转到括号对应位置
H -&gt; 光标移动到当前可视区域最上面
M -&gt; 光标移动到当前可视区域中间
L -&gt; 光标移动到当前可视区域最下面
&lt;C-u&gt; -&gt; 向上翻半页
&lt;C-d&gt; -&gt; 向下翻半页
&lt;C-f&gt; -&gt; 向下翻页
&lt;C-b&gt; -&gt; 向上翻页
\$ -&gt; 移动到行首
^ -&gt; 移动到行尾
fa -&gt; 向后查找当前行a出现的位置，并跳转
Fa -&gt; 向前查找当前行a出现的位置，并跳转


m&#123;a-zA-Z&#125; -&gt; 给当前位置打一个标签，a-z对当前文档有效，A-Z可以跨文档使用
&#39;&#123;a-zA-Z&#125; -&gt; 跳转回标记位置
&#39;[ -&gt; 跳转到上次编辑的第一个字符
&#39;[ -&gt; 跳转到上次编辑的最后一个字符
</code></pre><h4 id="插入模式"><a href="#插入模式" class="headerlink" title="插入模式"></a>插入模式</h4><pre><code>CTRL-M/CTRL-J -&gt; 开始新行
CTRL-E -&gt; 插入光标下的内容
CTRL-Y -&gt; 插入光标上方的字符
CTRL-A -&gt; 插入上次插入的文本
CTRL-@ -&gt; 插入上次插入的文本并结束插入模式
CTRL-R &#123;0-9a-z%#:.-=&quot;&#125; -&gt; 插入寄存器的内容
CTRL-W -&gt; 删除光标前的一个单词
CTRL-U -&gt; 删除当前行的所有字符
CTRL-T -&gt; 在当前行首插入一个移位宽度的缩进
CTRL-D -&gt; 从当前行首删除一个移位宽度的缩进
0 CTRL-D -&gt; 删除当前行的所有缩进
^ CTRL-D -&gt; 删除当前行的所有缩进，恢复下一行的缩进
</code></pre><h4 id="编辑"><a href="#编辑" class="headerlink" title="编辑"></a>编辑</h4><pre><code>i -&gt; 在当前光标之前插入
I -&gt; 在当前行首插入
a -&gt; 在当前位置之后插入
A -&gt; 在当前行末插入
</code></pre><h4 id="修改"><a href="#修改" class="headerlink" title="修改"></a>修改</h4><pre><code>r -&gt; 替换当前位置字符
R -&gt; 从当前位置开始一次替换后面的字符，按ESC退出
cc -&gt; 修改光标所在行
cw -&gt; 删除从当前位置到该单词的末尾，并进入插入模式
caw -&gt; 删除当前光标所在单词，并进入插入模式
c\$ -&gt; 从当前位置删除到行尾，并进入插入模式
c^ -&gt; 从当前位置删除到行首，并进入插入模式
ci&quot; -&gt; 删除两端&quot;中间的内容，并进入插入模式
ca&quot; -&gt; 删除两端&quot;以及中间的内容，并进入插入模式
</code></pre><h4 id="删除"><a href="#删除" class="headerlink" title="删除"></a>删除</h4><pre><code>x -&gt; 删除当前位置字符
X -&gt; 删除当前位置前一个字符
dd,dw,daw,s\$,d^,di&quot;,da&quot; -&gt; 同上
</code></pre><h4 id="复制"><a href="#复制" class="headerlink" title="复制"></a>复制</h4><pre><code>yy,yw,yaw,y\$,y^,yi(,ya( -&gt; 同上
</code></pre><h4 id="撤销"><a href="#撤销" class="headerlink" title="撤销"></a>撤销</h4><pre><code>u -&gt; 撤销上一次操作
U -&gt; 撤销当前行的所有操作
</code></pre><h4 id="文件操作"><a href="#文件操作" class="headerlink" title="文件操作"></a>文件操作</h4><pre><code>fw -&gt; 保存文件
fq -&gt; 退出文件
fwq -&gt; 保存并退出文件
fqq -&gt; 放弃保存直接退出文件
</code></pre><h4 id="编辑模式光标移动"><a href="#编辑模式光标移动" class="headerlink" title="编辑模式光标移动"></a>编辑模式光标移动</h4><pre><code>&lt;M-l&gt; -&gt; 光标右移
&lt;M-k&gt; -&gt; 光标上移
&lt;M-j&gt; -&gt; 光标下移
&lt;M-h&gt; -&gt; 光标左移
</code></pre><h4 id="快速选择窗口"><a href="#快速选择窗口" class="headerlink" title="快速选择窗口"></a>快速选择窗口</h4><pre><code>&lt;S-l&gt; -&gt; 选择右边窗口
&lt;S-k&gt; -&gt; 选择上边窗口
&lt;S-j&gt; -&gt; 选择下边窗口
&lt;S-h&gt; -&gt; 选择左边窗口
</code></pre><h4 id="划分窗口"><a href="#划分窗口" class="headerlink" title="划分窗口"></a>划分窗口</h4><pre><code>&lt;leader&gt;sp -&gt; 纵向划分窗口    
&lt;leader&gt;vs -&gt; 横向划分窗口    
</code></pre><h4 id="调整窗口大小"><a href="#调整窗口大小" class="headerlink" title="调整窗口大小"></a>调整窗口大小</h4><pre><code>&lt;M-l&gt;
&lt;M-k&gt;
&lt;M-j&gt;
&lt;M-h&gt;
</code></pre><h4 id="其它"><a href="#其它" class="headerlink" title="其它"></a>其它</h4><pre><code>; -&gt; 重复上一次的查找操作
. -&gt; 重复上一次的修改操作
</code></pre><h3 id="插件快捷键"><a href="#插件快捷键" class="headerlink" title="插件快捷键"></a>插件快捷键</h3><h4 id="vim-startify"><a href="#vim-startify" class="headerlink" title="vim-startify"></a>vim-startify</h4><pre><code>&lt;leader&gt;st -&gt; 回到初始界面
</code></pre><h4 id="NERDTree"><a href="#NERDTree" class="headerlink" title="NERDTree"></a>NERDTree</h4><pre><code>&lt;S-n&gt; -&gt; 打开/关闭文件树
</code></pre><h4 id="MarkdownPreview"><a href="#MarkdownPreview" class="headerlink" title="MarkdownPreview"></a>MarkdownPreview</h4><pre><code>&lt;C-p&gt; -&gt; 打开/关闭Markdown预览
[[ -&gt; 跳转上一个标题
]] -&gt; 跳转下一个标题
]c -&gt; 跳转到当前标题
]u -&gt; 跳转到副标题
zr -&gt; 打开下一级折叠
zR -&gt; 打开所有折叠
zm -&gt; 折叠当前段落
zM -&gt; 折叠所有段落
</code></pre><h4 id="vim-airline"><a href="#vim-airline" class="headerlink" title="vim-airline"></a>vim-airline</h4><pre><code>&lt;leader&gt;&#123;i&#125; -&gt; 切换到第i个标签
&lt;leader&gt;- -&gt; 切换到前一个标签
&lt;leader&gt;+ -&gt; 切换到上一个标签
</code></pre><h4 id="leetcode"><a href="#leetcode" class="headerlink" title="leetcode"></a>leetcode</h4><pre><code>&lt;leader&gt;li -&gt; 登陆leetcode    
&lt;leader&gt;ll -&gt; 列出问题列表
&lt;leader&gt;lt -&gt; 测试代码    
&lt;leader&gt;ls -&gt; 提交代码
</code></pre><h4 id="easymotion"><a href="#easymotion" class="headerlink" title="easymotion"></a>easymotion</h4><pre><code>&lt;leader&gt;&lt;leader&gt;w -&gt; 触发单词级移动，文本会被高亮，输入对应字母进行跳转
&lt;leader&gt;&lt;leader&gt;b -&gt; 从光标开始往上在单词之间移动光标
&lt;leader&gt;&lt;leader&gt;s -&gt; 从光标开始同时向两侧在单词之间移动光标
&lt;leader&gt;&lt;leader&gt;fo -&gt; 字母o对应的地方会被高亮，输入对应字母进行跳转
</code></pre><h4 id="vim-surround"><a href="#vim-surround" class="headerlink" title="vim-surround"></a>vim-surround</h4><pre><code>ysiw&quot; -&gt; 单词周围加双引号 
ysiw( -&gt; 左括号带空格
ysiw) -&gt; 右括号不带空格
ysiWb -&gt; 以空格为边界加大括号，B代表不带空格的打括号
yss&lt;div&gt; -&gt; 整行前后加&lt;div&gt;
S,&quot; -&gt; v模式选中区域前后加&quot;
S,&lt;div&gt; -&gt; V模式前后区域加&lt;div&gt;,这里&lt;div&gt;会独占一行
cs&quot;&#39; -&gt; 修改选中内容两端的&quot;为&#39;
ds&quot; -&gt; 删除选中内容两端的&quot;
</code></pre><h4 id="nerdcommenter"><a href="#nerdcommenter" class="headerlink" title="nerdcommenter"></a>nerdcommenter</h4><pre><code>&lt;leader&gt;cc -&gt; 注释当前行
&lt;leader&gt;cn -&gt; 同cc，不过强制nesting
&lt;leader&gt;c&lt;leader&gt; -&gt; 切换当前行的注释状态
&lt;leader&gt;cm -&gt; 多行注释
&lt;leader&gt;ci -&gt; 切换选中行的注释状态
</code></pre><h4 id="vim-visual-multi"><a href="#vim-visual-multi" class="headerlink" title="vim-visual-multi"></a>vim-visual-multi</h4><pre><code>&lt;C-n&gt; -&gt; 选择单词
n/N -&gt; 选择下一个/上一个目标词
q -&gt; 跳过当前词
Q -&gt; 取消选择当前词
</code></pre><h4 id="ultisnips"><a href="#ultisnips" class="headerlink" title="ultisnips"></a>ultisnips</h4><pre><code>&lt;tab&gt; -&gt; 自动扩展
</code></pre><h4 id="autopep8"><a href="#autopep8" class="headerlink" title="autopep8"></a>autopep8</h4><pre><code>&lt;F8&gt; -&gt; 自动格式化Python代码
</code></pre><h4 id="coc-nvim"><a href="#coc-nvim" class="headerlink" title="coc.nvim"></a>coc.nvim</h4><pre><code>&lt;M-tab&gt; -&gt; 向下选择补全
&lt;S-tab&gt; -&gt; 向上选择补全
[g -&gt; coc-diagnostic-prev
]g -&gt; coc-diagnostic-next
gd -&gt; coc-definition
gy -&gt; coc-type-definition
gi -&gt; coc-inplementation
gr -&gt; coc-references
&lt;leader&gt;rn -&gt; coc-rename
&lt;leader&gt;f -&gt; formatting selected code
K -&gt; show the documentation
</code></pre><h4 id="vimtex"><a href="#vimtex" class="headerlink" title="vimtex"></a>vimtex</h4><pre><code>\ll -&gt; 开始编译，后面每次保存都会自动编译同步
\lv -&gt; 打开pdf预览
\lk or \ll -&gt; 关闭编译
\le -&gt; 显示错误和警告
\lc -&gt; 清除多余文件
</code></pre>]]></content>
  </entry>
  <entry>
    <title>Git</title>
    <url>/2021/04/27/Git/</url>
    <content><![CDATA[<h1 id="Git命令"><a href="#Git命令" class="headerlink" title="Git命令"></a>Git命令</h1><h2 id="文档链接"><a href="#文档链接" class="headerlink" title="文档链接"></a>文档链接</h2><p><a href="http://git-scm.com/book/zh/v2/">http://git-scm.com/book/zh/v2/</a></p>
<h2 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h2><ol>
<li><p>创建git仓库</p>
<pre><code> git init
</code></pre></li>
<li><p>克隆仓库</p>
<pre><code> git clone https://github.com/example/example.git
</code></pre></li>
<li><p>追踪文件</p>
<pre><code> git add a.cpp
 git add *.cpp
 git add *
</code></pre></li>
<li><p>查看当前文件状态</p>
<pre><code> git status
 --------------------
 On branch master
 Your branch is up-to-date with &#39;origin/master&#39;.
 Changes to be committed:
   (use &quot;git restore --staged &lt;file&gt;...&quot; to unstage)

 new file:   README
</code></pre></li>
<li><p>以简洁的方式查看当前文件状态</p>
<pre><code> git status -s
 --------------------
  M README
 MM Rakefile
 A  lib/git.rb
 M  lib/simplegit.rb
 ?? LICENSE.txt
</code></pre></li>
<li><p>.gitignore</p>
<pre><code> # 忽略所有的 .a 文件
 *.a

 # 但跟踪所有的 lib.a，即便你在前面忽略了 .a 文件
 !lib.a

 # 只忽略当前目录下的 TODO 文件，而不忽略 subdir/TODO
 /TODO

 # 忽略任何目录下名为 build 的文件夹
 build/

 # 忽略 doc/notes.txt，但不忽略 doc/server/arch.txt
 doc/*.txt

 # 忽略 doc/ 目录及其所有子目录下的 .pdf 文件
 doc/**/*.pdf
</code></pre></li>
<li><p>查看未暂存的文件有那些修改</p>
<pre><code> git diff
 --------------------
 diff --git a/CONTRIBUTING.md b/CONTRIBUTING.md
 index 8ebb991..643e24f 100644
 --- a/CONTRIBUTING.md
 +++ b/CONTRIBUTING.md
 @@ -65,7 +65,8 @@ branch directly, things can get messy.
  Please include a nice description of your changes when you submit your PR;
  if we have to read the whole diff to figure out why you&#39;re contributing
  in the first place, you&#39;re less likely to get feedback and have your change
 -merged in.
 +merged in. Also, split your changes into comprehensive chunks if your patch is
 +longer than a dozen lines.

  If you are starting to work on a particular area, feel free to submit a PR
  that highlights your work in progress (and note in the PR title that it&#39;s
</code></pre></li>
<li><p>查看已暂存文件和最后一次提交的差异</p>
<pre><code> git diff --staged
 --------------------
 diff --git a/README b/README
 new file mode 100644
 index 0000000..03902a1
 --- /dev/null
 +++ b/README
 @@ -0,0 +1 @@
 +My Project
</code></pre></li>
<li><p>提交暂存区的内容到仓库</p>
<pre><code> git commit -m &quot;message&quot;
</code></pre></li>
<li><p>将所有跟踪过的文件add后提交</p>
<pre><code>git commit -a -m &quot;message&quot;
</code></pre></li>
<li><p>从git中移除并且删除文件</p>
<pre><code>git rm -f README.md
</code></pre></li>
<li><p>从git中移除但并不删除文件</p>
<pre><code>git rm --cached README.md
</code></pre></li>
<li><p>修改文件名并添加到git中</p>
<pre><code>git mv README.md README
--------------------
相当于
\$ mv README.md README
\$ git rm README.md
\$ git add README
</code></pre></li>
</ol>
<h2 id="仓库合并操作"><a href="#仓库合并操作" class="headerlink" title="仓库合并操作"></a>仓库合并操作</h2><ol>
<li><p>查看提交历史</p>
<pre><code> git log
 --------------------
 commit ca82a6dff817ec66f44342007202690a93763949
 Author: Scott Chacon &lt;schacon@gee-mail.com&gt;
 Date:   Mon Mar 17 21:52:11 2008 -0700

     changed the version number

 commit 085bb3bcb608e1e8451d4b2432f8ecbe6306e7e7
 Author: Scott Chacon &lt;schacon@gee-mail.com&gt;
 Date:   Sat Mar 15 16:40:33 2008 -0700

     removed unnecessary test

 commit a11bef06a3f659402fe7563abf99ad00de2209e6
 Author: Scott Chacon &lt;schacon@gee-mail.com&gt;
 Date:   Sat Mar 15 10:31:28 2008 -0700

     first commit
</code></pre></li>
<li><p>查看最近两次提交所引入的差异</p>
<pre><code> git log -p -2
</code></pre></li>
<li><p>查看提交的简略统计信息</p>
<pre><code> git log --stat
</code></pre></li>
<li><p>以特有的形式显示提交信息</p>
<pre><code> git log --pretty=oneline/short/full/fuller
</code></pre></li>
<li><p>以格式化方式显示提交信息</p>
<pre><code> git log --pretty=format:&quot;%h - %an, %ar : %s&quot;
 --------------------
 %H        提交的完整哈希值
 %h        提交的简写哈希值
 %T        树的完整哈希值
 %t        树的简写哈希值
 %P        父提交的完整哈希值
 %p        父提交的简写哈希值
 %an        作者名字
 %ae        作者的电子邮件地址
 %ad        作者修订日期（可以用 --date=选项 来定制格式）
 %ar        作者修订日期，按多久以前的方式显示
 %cn        提交者的名字
 %ce        提交者的电子邮件地址
 %cd        提交日期
 %cr        提交日期（距今多长时间）
 %s        提交说明
</code></pre></li>
<li><p>git log的常用选项</p>
<pre><code> -p            按补丁格式显示每个提交引入的差异。
 --stat            显示每次提交的文件修改统计信息。
 --shortstat        只显示 --stat 中最后的行数修改添加移除统计。
 --name-only        仅在提交信息后显示已修改的文件清单。
 --name-status        显示新增、修改、删除的文件清单。
 --abbrev-commit    仅显示 SHA-1 校验和所有 40 个字符中的前几个字符。
 --relative-date    使用较短的相对时间而不是完整格式显示日期（比如“2 weeks ago”）。
 --graph        在日志旁以 ASCII 图形显示分支与合并历史。
 --pretty        使用其他格式显示历史提交信息。可用的选项包括 oneline、short、full、fuller 和 format（用来定义自己的格式）。
 --oneline        pretty=oneline --abbrev-commit 合用的简写。
</code></pre></li>
<li><p>git log 过滤内容</p>
<pre><code> -\&lt;n&gt;            仅显示最近的 n 条提交。
 --since, --after    仅显示指定时间之后的提交。
 --until, --before    仅显示指定时间之前的提交。
 --author        仅显示作者匹配指定字符串的提交。
 --committer        仅显示提交者匹配指定字符串的提交。
 --grep            仅显示提交说明中包含指定字符串的提交。
 -S            仅显示添加或删除内容匹配指定字符串的提交。
</code></pre></li>
</ol>
]]></content>
  </entry>
  <entry>
    <title>Vjudge刷题记录</title>
    <url>/2021/05/17/Vjudge%E5%88%B7%E9%A2%98%E8%AE%B0%E5%BD%95/</url>
    <content><![CDATA[<h1 id="Vjudge刷题记录"><a href="#Vjudge刷题记录" class="headerlink" title="Vjudge刷题记录"></a>Vjudge刷题记录</h1><p>以后从新开始每天在vjudge上每天刷一题，最开始会做些简单的找找感觉。<br>这里记录做题的思路以及学习到的东西。</p>
<h2 id="热血格斗场"><a href="#热血格斗场" class="headerlink" title="热血格斗场"></a><a href="https://vjudge.net/problem/%E8%AE%A1%E8%92%9C%E5%AE%A2-T1230">热血格斗场</a></h2><p>蒜头君新开了一家热血格斗场。格斗场实行会员制，但是新来的会员不需要交入会费，而只要同一名老会员打一场表演赛，证明自己的实力。</p>
<p>我们假设格斗的实力可以用一个正整数表示，成为实力值。另外，每个人都有一个唯一的<br>id<br>，也是一个正整数。为了使得比赛更好看，每一个新队员都会选择与他实力最为接近的人比赛，即比赛双方的实力值之差的绝对值越小越好，如果有两个人的实力值与他差别相同，则他会选择比他弱的那个（显然，虐人必被虐好）。</p>
<p>不幸的是，蒜头君一不小心把比赛记录弄丢了，但是他还保留着会员的注册记录。现在请你帮蒜头君恢复比赛纪录，按照时间顺序依次输出每场比赛双方的<br>id<br>。</p>
<p>输入格式第一行一个数 n(0&lt;n≤100000)，表示格斗场新来的会员数（不包括蒜头君）。以后 n 行每一行两个数，<br>按照入会的时间给出会员的 id(2≤id≤106) 和实力值(≤109)。一开始，蒜头君就算是会员，id 为 1，<br>实力值 1000000000。输入保证两人的实力值不同。输出格式N 行，每行两个数，为每场比赛双方的 id，新手的 id 写在前面。</p>
<h3 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h3><p>map的使用</p>
<h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#include&lt;iostream&gt;</span><br><span class="line">#include&lt;map&gt;</span><br><span class="line">using namespace std;</span><br><span class="line">int main()&#123;</span><br><span class="line">	int n,a,b,ans;</span><br><span class="line">	map&lt;int,int&gt;mp;</span><br><span class="line">	map&lt;int,int&gt;::iterator itr;</span><br><span class="line">	cin&gt;&gt;n;</span><br><span class="line">	mp[1e9]=1;</span><br><span class="line">	while(n--)&#123;</span><br><span class="line">		cin&gt;&gt;a&gt;&gt;b;</span><br><span class="line">		itr = mp.upper_bound(b);</span><br><span class="line">		if (itr==mp.end()) &#123;</span><br><span class="line">			itr--;</span><br><span class="line">			ans = itr-&gt;second;</span><br><span class="line">		&#125;else if(itr==mp.begin())&#123;</span><br><span class="line">			ans = itr-&gt;second;</span><br><span class="line">		&#125;else&#123;</span><br><span class="line">			int diff = itr-&gt;first - b;</span><br><span class="line">			ans = itr-&gt;second;</span><br><span class="line">			itr--;</span><br><span class="line">			if((b-itr-&gt;first)&lt;=diff)&#123;</span><br><span class="line">				ans = itr-&gt;second;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">		mp[b]=a;</span><br><span class="line">		cout&lt;&lt;a&lt;&lt;&#x27; &#x27;&lt;&lt;ans&lt;&lt;endl;</span><br><span class="line">	&#125;</span><br><span class="line">	return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="知识点"><a href="#知识点" class="headerlink" title="知识点"></a>知识点</h3><p>std::map 的底层是红黑树，是按照key有序的</p>
<p>std::unordered_map 的底层是哈希表，是无序的</p>
<p>map.lower_bound(key) 返回第一个不小于key的迭代器索引。</p>
<p>map.upper_bound(key) 返回第一个大于key的迭代器索引。</p>
<h2 id="Neighbor-House"><a href="#Neighbor-House" class="headerlink" title="Neighbor House"></a><a href="https://vjudge.net/contest/439383#problem/D">Neighbor House</a></h2><p>The people of Mohammadpur have decided to paint each of their houses red, green, or blue. They have also decided that no two neighboring houses will be painted the same color. The neighbors of house i are houses i-1 and i+1. The first and last houses are not neighbors.</p>
<p>You will be given the information of houses. Each house will contain three integers “R G B” (quotes for clarity only), where R, G and B are the costs of painting the corresponding house red, green, and blue, respectively. Return the minimal total cost required to perform the work.</p>
<h3 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#include&lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line">int main()&#123;</span><br><span class="line">	int n,t;</span><br><span class="line">	cin&gt;&gt;n;</span><br><span class="line">	for(int j=1;j&lt;=n;j++)&#123;</span><br><span class="line">		cin&gt;&gt;t;</span><br><span class="line">		int dp[t][3];</span><br><span class="line">		int data[t][3];</span><br><span class="line">		for(int i=0; i&lt;t ;i++)&#123;</span><br><span class="line">			cin&gt;&gt;data[i][0]&gt;&gt;data[i][1]&gt;&gt;data[i][2];</span><br><span class="line">		&#125;</span><br><span class="line">		dp[0][0]=data[0][0],dp[0][1]=data[0][1],dp[0][2]=data[0][2];</span><br><span class="line">		for(int i=1;i&lt;t;i++)&#123;</span><br><span class="line">			dp[i][0]=min(dp[i-1][1],dp[i-1][2])+data[i][0];</span><br><span class="line">			dp[i][1]=min(dp[i-1][2],dp[i-1][0])+data[i][1];</span><br><span class="line">			dp[i][2]=min(dp[i-1][1],dp[i-1][0])+data[i][2];</span><br><span class="line">		&#125;</span><br><span class="line">		cout&lt;&lt;&quot;Case &quot;&lt;&lt;j&lt;&lt;&quot;: &quot;&lt;&lt;min(dp[t-1][2],min(dp[t-1][0],dp[t-1][1]))&lt;&lt;endl;</span><br><span class="line">			</span><br><span class="line">	&#125;</span><br><span class="line">	return 0;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="Staircases"><a href="#Staircases" class="headerlink" title="Staircases"></a><a href="https://vjudge.net/problem/URAL-1017">Staircases</a></h2><p>One curious child has a set of N little bricks (5 ≤ N ≤ 500). From these bricks he builds different staircases. Staircase consists of steps of different sizes in a strictly descending order. It is not allowed for staircase to have steps equal sizes. Every staircase consists of at least two steps and each step contains at least one brick. </p>
<p>Your task is to write a program that reads the number N and writes the only number Q — amount of different staircases that can be built from exactly N bricks.</p>
<h3 id="思路-1"><a href="#思路-1" class="headerlink" title="思路"></a>思路</h3><p>dp[i][j]表示当前使用第i块砖头时，最后一列的砖块数为j的组合数量。<br>dp[i][j] = sum(dp[i-j][k]) (0&lt;=k&lt;j)</p>
<p>这里为什么要用i-j而不是直接使用j呢？？<br>因为j是从1到i变化的，而i-j是从i-1到0变化的。 这样可以保证这一轮的dp是完全由上一轮产生的。</p>
<h3 id="代码-2"><a href="#代码-2" class="headerlink" title="代码"></a>代码</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#include&lt;iostream&gt;</span><br><span class="line">#define maxn 510</span><br><span class="line">using namespace std;</span><br><span class="line">int main()&#123;</span><br><span class="line">	int dp[maxn][maxn],n;</span><br><span class="line">	dp[0][0]=1;</span><br><span class="line">	for(int i=1; i&lt;maxn; i++)&#123;</span><br><span class="line">		for(int j=1; j&lt;=i; j++)&#123;</span><br><span class="line">			for(int k=0; k&lt;j; k++)&#123;</span><br><span class="line">				dp[i][j]+=dp[i-j][k];</span><br><span class="line">			&#125;</span><br><span class="line">			</span><br><span class="line">		&#125;</span><br><span class="line">		</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	while( cin&gt;&gt;n )&#123;</span><br><span class="line">		long long ans=0;</span><br><span class="line">		for(int i=1; i&lt;n; i++)&#123;</span><br><span class="line">			ans+=dp[n][i];</span><br><span class="line">		&#125;</span><br><span class="line">		cout&lt;&lt;ans&lt;&lt;endl;</span><br><span class="line">		</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="优化版代码"><a href="#优化版代码" class="headerlink" title="优化版代码"></a>优化版代码</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#include&lt;iostream&gt;</span><br><span class="line">#include&lt;vector&gt;</span><br><span class="line">using namespace std;</span><br><span class="line">int main()&#123;</span><br><span class="line">	int N;</span><br><span class="line">	cin&gt;&gt;N;</span><br><span class="line">	vector&lt;long long&gt; dp(N+1);</span><br><span class="line">	dp[0]=1;</span><br><span class="line">	for(int i=1;i&lt;=N;i++)&#123;</span><br><span class="line">		for(int j=N;j&gt;=i;j--)&#123;</span><br><span class="line">			dp[j]+=dp[j-i];</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	cout&lt;&lt;dp[N]-1&lt;&lt;endl;</span><br><span class="line">	return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="解释"><a href="#解释" class="headerlink" title="解释"></a>解释</h4><p>这里我也没搞懂为什么可以这样写。</p>
<p>感觉像是通过和背包问题中相似的优化减少了一维。</p>
<h2 id="Basketball-Exercise"><a href="#Basketball-Exercise" class="headerlink" title="Basketball Exercise"></a><a href="https://vjudge.net/problem/CodeForces-1195C">Basketball Exercise</a></h2><p>Finally, a basketball court has been opened in SIS, so Demid has decided to hold a basketball exercise session. 2⋅n students have come to Demids exercise session, and he lined up them into two rows of the same size (there are exactly n people in each row). Students are numbered from 1 to n in each row in order from left to right.</p>
<p>Now Demid wants to choose a team to play basketball. He will choose players from left to right, and the index of each chosen player (excluding the first one taken) will be strictly greater than the index of the previously chosen player. To avoid giving preference to one of the rows, Demid chooses students in such a way that no consecutive chosen students belong to the same row. The first student can be chosen among all 2n students (there are no additional constraints), and a team can consist of any number of students.</p>
<p>Demid thinks, that in order to compose a perfect team, he should choose students in such a way, that the total height of all chosen students is maximum possible. Help Demid to find the maximum possible total height of players in a team he can choose.</p>
<p>2*n的数组从左往右选不能选择相邻的两个数，求最终能选择的最大和。</p>
<h3 id="思路-2"><a href="#思路-2" class="headerlink" title="思路"></a>思路</h3><p>dp[i][0]表示这一列选择第一个的最大值。</p>
<p>dp[i][1]表示这一列选择第二个的最大值。</p>
<p>dp[i][2]表示这一列都不选的最大值。</p>
<p>有状态转移方程</p>
<p>dp[i][0]=max(dp[i-1][1],dp[i-1][2])+data[i][0];</p>
<p>dp[i][1]=max(dp[i-1][0],dp[i-1][2])+data[i][1];</p>
<p>dp[i][2]=max(dp[i-1][0],max(dp[i-1][1],dp[i-1][2]));</p>
<h3 id="代码-3"><a href="#代码-3" class="headerlink" title="代码"></a>代码</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#include&lt;iostream&gt;</span><br><span class="line">#define maxn 100008</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">int main()&#123;</span><br><span class="line">	int n;</span><br><span class="line">	int data[maxn][2];</span><br><span class="line">	long long dp[maxn][3];</span><br><span class="line">	while(cin&gt;&gt;n)&#123;</span><br><span class="line">		for(int i=0; i&lt;n; i++)&#123;</span><br><span class="line">			cin&gt;&gt;data[i][0];</span><br><span class="line">		&#125;</span><br><span class="line">		for(int i=0; i&lt;n; i++)&#123;</span><br><span class="line">			cin&gt;&gt;data[i][1];</span><br><span class="line">		&#125;</span><br><span class="line">		dp[0][0]=data[0][0];</span><br><span class="line">		dp[0][1]=data[0][1];</span><br><span class="line">		dp[0][2]=0;</span><br><span class="line"></span><br><span class="line">		for(int i=1; i&lt;n; i++)&#123;</span><br><span class="line">			dp[i][0]=max(dp[i-1][1],dp[i-1][2])+data[i][0];</span><br><span class="line">			dp[i][1]=max(dp[i-1][0],dp[i-1][2])+data[i][1];</span><br><span class="line">			dp[i][2]=max(dp[i-1][0],max(dp[i-1][1],dp[i-1][2]));</span><br><span class="line"></span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		cout&lt;&lt;max(dp[n-1][0],max(dp[n-1][1],dp[n-1][2]))&lt;&lt;endl;</span><br><span class="line"></span><br><span class="line">		</span><br><span class="line">		</span><br><span class="line">	&#125;</span><br><span class="line">	return 0;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="Gargari-and-Permutations"><a href="#Gargari-and-Permutations" class="headerlink" title="Gargari and Permutations"></a><a href="https://vjudge.net/contest/439073#problem/A">Gargari and Permutations</a></h2><p>Gargari got bored to play with the bishops and now, after solving the problem about them, he is trying to do math homework. In a math book he have found k permutations. Each of them consists of numbers 1, 2, …, n in some order. Now he should find the length of the longest common subsequence of these permutations. Can you help Gargari?</p>
<h3 id="思路-3"><a href="#思路-3" class="headerlink" title="思路"></a>思路</h3><p>dp[i]表示到data[0][i]为止所能达到的最长公共子序列。</p>
<p>那么训练j 0-&gt;i 如果data[0][j]在其余序列中的位置都在data[0][i]之前，那是不是说明dp[i]能在dp[j]上扩展一位。</p>
<h3 id="代码-4"><a href="#代码-4" class="headerlink" title="代码"></a>代码</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#include&lt;iostream&gt;</span><br><span class="line">#include&lt;cstring&gt;</span><br><span class="line">#define maxn 1004</span><br><span class="line">#define maxk 5</span><br><span class="line">using namespace std;</span><br><span class="line">int main()&#123;</span><br><span class="line">	int n,k;</span><br><span class="line">	int data[maxk][maxn];</span><br><span class="line">	int pos[maxk][maxn];</span><br><span class="line">	int dp[maxn];</span><br><span class="line">	cin&gt;&gt;n&gt;&gt;k;</span><br><span class="line">	memset(dp, 0, sizeof(dp));</span><br><span class="line">	for(int i=0; i&lt;k; i++)&#123;</span><br><span class="line">		for(int j=0; j&lt;n; j++)&#123;</span><br><span class="line">			cin&gt;&gt;data[i][j];</span><br><span class="line">			pos[i][data[i][j]]=j;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	dp[0]=1;	</span><br><span class="line">	for(int i=0; i&lt;n; i++)&#123;</span><br><span class="line">		for(int j=0; j&lt;i; j++)&#123;</span><br><span class="line">			bool Flag=false;</span><br><span class="line">			for(int p=1; p&lt;k; p++)&#123;</span><br><span class="line">				if (pos[p][data[0][j]]&gt;pos[p][data[0][i]]) &#123;</span><br><span class="line">					Flag=true;</span><br><span class="line">				&#125;	</span><br><span class="line">			&#125;</span><br><span class="line">			if (!Flag) &#123;</span><br><span class="line">				dp[i]=max(dp[i],dp[j]+1);</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">		if(!dp[i]) dp[i]=1;</span><br><span class="line">		</span><br><span class="line">	&#125;</span><br><span class="line">	int ans=dp[0];</span><br><span class="line">	for(int i=1; i&lt;n; i++)&#123;</span><br><span class="line">		ans=max(ans,dp[i]);</span><br><span class="line">	&#125;</span><br><span class="line">	cout&lt;&lt;ans&lt;&lt;endl;</span><br><span class="line">	</span><br><span class="line">	return 0;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="Checkout-Assistant"><a href="#Checkout-Assistant" class="headerlink" title="Checkout Assistant"></a><a href="https://vjudge.net/contest/439073#problem/E">Checkout Assistant</a></h2><p>Bob came to a cash &amp; carry store, put n items into his trolley, and went to the checkout counter to pay. Each item is described by its price ci and time ti in seconds that a checkout assistant spends on this item. While the checkout assistant is occupied with some item, Bob can steal some other items from his trolley. To steal one item Bob needs exactly 1 second. What is the minimum amount of money that Bob will have to pay to the checkout assistant? Remember, please, that it is Bob, who determines the order of items for the checkout assistant.</p>
<h3 id="思路-4"><a href="#思路-4" class="headerlink" title="思路"></a>思路</h3><p>这里不需要考虑我结了那些，逃了那些，只要保证我结账的物品能够逃掉所有物品就行了。</p>
<p>转化为01背包问题也就是，我每件物品的花费为pi,然后我能清算掉ti+1件物品</p>
<p>这里要注意的是这里我们需要的是最终的价值大于等于背包容量，而不是小于等于所以当剩余背包容量不足的时候，也就是j&lt;data[i][0]的时候也需要将话费加入进来。 也就是dp[j]=data[i][1]</p>
<h3 id="代码-5"><a href="#代码-5" class="headerlink" title="代码"></a>代码</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#include &lt;iostream&gt;</span><br><span class="line">#define maxn 2005</span><br><span class="line">using namespace std;</span><br><span class="line">int main() &#123;</span><br><span class="line">  int n;</span><br><span class="line">  int data[maxn][2];</span><br><span class="line">  long long dp[maxn];</span><br><span class="line">  cin &gt;&gt; n;</span><br><span class="line">  for (int i = 1; i &lt;= n; i++) &#123;</span><br><span class="line">    cin &gt;&gt; data[i][0] &gt;&gt; data[i][1];</span><br><span class="line">    data[i][0]++;</span><br><span class="line">    dp[i] = 1e18;</span><br><span class="line">  &#125;</span><br><span class="line">  for (int i = 1; i &lt;= n; i++) &#123;</span><br><span class="line">    for (int j = n; j &gt;= 1; j--) &#123;</span><br><span class="line">      dp[j] = min(dp[j], dp[max(j - data[i][0],0)] + data[i][1]);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  cout &lt;&lt; dp[n] &lt;&lt; endl;</span><br><span class="line">  return 0;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>acm</category>
      </categories>
  </entry>
  <entry>
    <title>零知识证明</title>
    <url>/2021/05/14/%E9%9B%B6%E7%9F%A5%E8%AF%86%E8%AF%81%E6%98%8E/</url>
    <content><![CDATA[<h1 id="零知识证明-Zero-Knowledge-Proof"><a href="#零知识证明-Zero-Knowledge-Proof" class="headerlink" title="零知识证明(Zero Knowledge Proof)"></a>零知识证明(Zero Knowledge Proof)</h1><p>零知识证明是指证明者在不向验证者提供任何有用信息的情况下，使得验证证相信某个论断是正确的。</p>
<h2 id="零知识证明的三个性质"><a href="#零知识证明的三个性质" class="headerlink" title="零知识证明的三个性质"></a>零知识证明的三个性质</h2><p>一种零知识证明的方法需要具备如下三个性质：</p>
<ol>
<li><p>完备性(Completeness)：若证明方确实掌握了某论断的答案，则他肯定能找到方法向验证方证明他手中掌握数据的正确性，即真的假不了。</p>
</li>
<li><p>可靠性(Soundness)：若证明方根本不掌握某论断的答案，则他无法（或者 <strong>概率极低</strong>）说服验证方他手中数据的正确性，即假的真不了。</p>
</li>
<li><p>零知识性(Zero-knowlegeness)：验证方除了能判断出论断的真假外，无法获得其它任何有效信息。</p>
</li>
</ol>
<h2 id="零知识证明构造三段论"><a href="#零知识证明构造三段论" class="headerlink" title="零知识证明构造三段论"></a>零知识证明构造三段论</h2><ol>
<li>证明方先根据论断内容向验证方发个交底材料，这个样例论断需要是随机的或加密的；</li>
<li>这个样例论断需要是随机的或加密的验证方随机生成一个试探（学术名词是挑战，challenge），发给证明方；</li>
<li>证明方根据该试探和交底材料生成证明信息发给验证方。验证方自己将信息和交底材料一合计，判断证明方是否通过了该试探。</li>
</ol>
<h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><p><strong>例1：数独问题</strong></p>
<ul>
<li>背景：一个骨灰级数独题，数字范围是1-9</li>
<li>论断：证明方有该数独题的答案</li>
<li>过程：</li>
</ul>
<ol>
<li>证明方将数独题答案的每个数字（连同题面上的数字）写在一张卡片上，然后将每张卡片放到数独中对应的位置，同时将答案卡片翻面（题面数字对应的卡片依然朝上）；</li>
<li>验证方首先验证题面数字和朝上的卡片的数字一致，然后随意指定一行（或列）；</li>
<li>证明方将该行（或列）的卡片收拢并打乱，然后全部展示给验证方。验证方验证是否数字正好为1-9。</li>
</ol>
<p>这里第一步的交底材料是非常必要的，它有两个作用：<br>一是可以防止证明方根据试探的内容临时造假；<br>二是可以帮助证明方掩盖敏感信息，保证证明过程的零知识性。<br>第二步的试探也必须是随机的，否则如果证明方能提前知道试探的内容，<br>那即便他不知道原论断背后的答案他也能提前准备好交底材料和返回的信息。<br>最后，这个过程必须重复多次，因为通过一次或少数试探可能是运气，但如果一直能通过，则说明是实力了。</p>
<p><strong>例2：离散对数问题（参考论文[1]）</strong></p>
<ul>
<li>背景：函数 $f(x)=g^{x}$ ，其中 $g$ 为公开大整数。</li>
<li>论断：给定一个数字 $y$ ，证明方证明他知道离散对数 $\log _{g}y$ ，即 $y=f(x)$ 。</li>
<li>过程：</li>
</ul>
<ol>
<li>证明方生成一个随机数 $u$ ，并计算 $v=f(u)$ ，然后将 $v$ 发给验证方；</li>
<li>验证方生成一个随机数 $e$ ，并发给证明方；</li>
<li>证明方计算 $t=e+u$ ，并将 $t$ 发送给验证方。验证方验证 $f(t)=v \cdot y^{e}$ .</li>
</ol>
<h2 id="zkSNARKs"><a href="#zkSNARKs" class="headerlink" title="zkSNARKs"></a>zkSNARKs</h2><p><a href="https://zhuanlan.zhihu.com/p/31780893">https://zhuanlan.zhihu.com/p/31780893</a></p>
]]></content>
  </entry>
  <entry>
    <title>中国剩余定理</title>
    <url>/2021/05/20/%E4%B8%AD%E5%9B%BD%E5%89%A9%E4%BD%99%E5%AE%9A%E7%90%86/</url>
    <content><![CDATA[<h1 id="中国剩余定理"><a href="#中国剩余定理" class="headerlink" title="中国剩余定理"></a>中国剩余定理</h1><h2 id="例题"><a href="#例题" class="headerlink" title="例题"></a>例题</h2><p>人自出生起就有体力，情感和智力三个生理周期，分别为23，28和33天。一个周期内有一天为峰值，在这一天，人在对应的方面（体力，情感或智力）表现最好。通常这三个周期的峰值不会是同一天。现在给出三个日期，分别对应于体力，情感，智力出现峰值的日期。然后再给出一个起始日期，要求从这一天开始，算出最少再过多少天后三个峰值同时出现。</p>
<h3 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h3><p>首先我们要知道，任意两个峰值之间一定相距整数倍的周期。假设一年的第$N$天达到峰值，则下次达到峰值的时间为是$N+Tk$($T$是周期，$k$是任意正整数)。所以，三个峰值同时出现的那一天($S$)应满足</p>
<script type="math/tex; mode=display">
S = N_1 + T_1 * k_1 = N_2 + T_2 * k_2 = N_3 + T_3 * k_3</script><p>$N_1,N_2,N_3$分别为为体力，情感，智力出现峰值的日期，$T_1,T_2,T_3$分别为体力，情感，智力周期。 我们需要求出$k_1,k_2,k_3$三个非负整数使上面的等式成立。</p>
<p>　　想直接求出$k_1,k_2,k_3$貌似很难，但是我们的目的是求出$S$， 可以考虑从结果逆推。根据上面的等式，$S$满足三个要求：除以$T_1$余数为$N_1$，除以$T_2$余数为$N_2$，除以$T_3$余数为$N_3$。这样我们就把问题转化为求一个最小数，该数除以$T_1$余$N_1$，除以$T_2$余$N_2$，除以$T_3$余$N_3$。这就是著名的中国剩余定理，我们的老祖宗在几千年前已经对这个问题想出了一个精妙的解法。依据此解法的算法，时间复杂度可达到$O(1)$。</p>
<h2 id="中国剩余定理-1"><a href="#中国剩余定理-1" class="headerlink" title="中国剩余定理"></a>中国剩余定理</h2><p>在《孙子算经》中有这样一个问题：“今有物不知其数，三三数之剩二（除以3余2），五五数之剩三（除以5余3），七七数之剩二（除以7余2），问物几何？”这个问题称为“孙子问题”，该问题的一般解法国际上称为“中国剩余定理”。具体解法分三步：</p>
<ol>
<li>找出三个数：从3和5的公倍数中找出被7除余1的最小数15，从3和7的公倍数中找出被5除余1 的最小数21，最后从5和7的公倍数中找出除3余1的最小数70。</li>
<li>用15乘以2（2为最终结果除以7的余数），用21乘以3（3为最终结果除以5的余数），同理，用70乘以2（2为最终结果除以3的余数），然后把三个乘积相加15∗2+21∗3+70∗2得到和233。</li>
<li>用233除以3，5，7三个数的最小公倍数105，得到余数23，即233%105=23。这个余数23就是符合条件的最小数。
　</li>
</ol>
<p>就这么简单。我们在感叹神奇的同时不禁想知道古人是如何想到这个方法的，有什么基本的数学依据吗？</p>
<p>我们将“孙子问题”拆分成几个简单的小问题，从零开始，试图揣测古人是如何推导出这个解法的。</p>
<p>首先，我们假设$n_1$是满足除以3余2的一个数，比如2，5，8等等，也就是满足$3*k+2(k \ge 0)$ 的一个任意数。 同样，我们假设$n_2$是满足除以5余3的一个数，$n_3$是满足除以7余2的一个数。</p>
<p>有了前面的假设，我们先从$n_1$这个角度出发，已知$n_1$满足除以3余2，能不能使得$n_1+n_2$的和仍然满足除以3余2？进而使得$n_1+n_2+n_3$的和仍然满足除以3余2？</p>
<p>这就牵涉到一个最基本数学定理，如果有$a\%b=c$，则有$(a+k*b)\%b=c$($k$为非零整数)，换句话说，如果一个除法运算的余数为$c$，那么被除数与$k$倍的除数相加（或相减）的和（差）再与除数相除，余数不变。这个是很好证明的。</p>
<p>以此定理为依据，如果$n_2$是3的倍数，$n_1+n_2$就依然满足除以3余2。同理，如果$n_3$也是3的倍数，那么 $n_1+n_2+n_3$的和就满足除以3余2。这是从$n_1$的角度考虑的，再从$n_2$，$n_3$的角度出发，我们可推导出以下三点：</p>
<ol>
<li>为使$n_1+n_2+n_3$的和满足除以3余2，$n_2$和$n_3$必须是3的倍数。</li>
<li>为使$n_1+n_2+n_3$的和满足除以5余3，$n_1$和$n_3$必须是5的倍数。</li>
<li>为使$n_1+n_2+n_3$的和满足除以7余2，$n_1$和$n_2$必须是7的倍数。</li>
</ol>
<p>因此，为使$n_1+n_2+n_3$的和作为“孙子问题”的一个最终解，需满足：</p>
<ol>
<li>$n_1$除以3余2，且是5和7的公倍数。</li>
<li>$n_2$除以5余3，且是3和7的公倍数。</li>
<li>$n_3$除以7余2，且是3和5的公倍数。</li>
</ol>
<p>所以，孙子问题解法的本质是从5和7的公倍数中找一个除以3余2的数$n_1$，从3和7的公倍数中找一个除以5余3的数𝑛2，从3和5的公倍数中找一个除以7余2的数𝑛3，再将三个数相加得到解。在求$n_1,n_2,n_3$时又用了一个小技巧，以$n_1$为例，并非从5和7的公倍数中直接找一个除以3余2的数，而是先找一个除以3余1的数，再乘以2。也就是先求出5和7的公倍数模3下的逆元，再用逆元去乘余数。</p>
<p>这里又有一个数学公式，如果$a\%b=c$，那么$(a<em>k)\%b = a\%b + a\%b + … + a\%b = c + c + … + c = k </em> c(k&gt;0)$,也就是说，如果一个除法的余数为$c$，那么被除数的$k$倍与除数相除的余数为$k*c$。展开式中已证明。</p>
<p>最后，我们还要清楚一点，$n_1+n_2+n_3$只是问题的一个解，并不是最小的解。如何得到最小解？我们只需要从中最大限度的减掉掉3，5，7的公倍数105即可。道理就是前面讲过的定理“如果$a\%b=c$，则有$(a-k*b)\%b=c$”。所以$(n_1+n_2+n_3)\%105$就是最终的最小解。</p>
]]></content>
  </entry>
  <entry>
    <title>终端软件</title>
    <url>/2021/06/29/%E7%BB%88%E7%AB%AF%E8%BD%AF%E4%BB%B6/</url>
    <content><![CDATA[<h1 id="这里汇总一些我常用的终端软件"><a href="#这里汇总一些我常用的终端软件" class="headerlink" title="这里汇总一些我常用的终端软件"></a>这里汇总一些我常用的终端软件</h1><h3 id="tmux"><a href="#tmux" class="headerlink" title="tmux"></a>tmux</h3><p>终端重用神器</p>
<h3 id="ranger"><a href="#ranger" class="headerlink" title="ranger"></a>ranger</h3><p>终端文件浏览器</p>
<h3 id="lazygit"><a href="#lazygit" class="headerlink" title="lazygit"></a>lazygit</h3><p>终端git工具</p>
<h3 id="neofetch"><a href="#neofetch" class="headerlink" title="neofetch"></a>neofetch</h3><p>查看系统信息</p>
<h3 id="autojump"><a href="#autojump" class="headerlink" title="autojump"></a>autojump</h3><p>快速跳转</p>
<h3 id="neovim"><a href="#neovim" class="headerlink" title="neovim"></a>neovim</h3><p>最强编辑器</p>
<h3 id="bat"><a href="#bat" class="headerlink" title="bat"></a>bat</h3><p>cat增强版，高亮现实文件</p>
<h3 id="fzf"><a href="#fzf" class="headerlink" title="fzf"></a>fzf</h3><p>快速搜索文件</p>
<h3 id="htop"><a href="#htop" class="headerlink" title="htop"></a>htop</h3><p>top命令增强版，查看系统占用</p>
<h3 id="imgcat"><a href="#imgcat" class="headerlink" title="imgcat"></a>imgcat</h3><p>在终端查看图片</p>
<h3 id="thefuck"><a href="#thefuck" class="headerlink" title="thefuck"></a>thefuck</h3><p>快速修正命令</p>
<h3 id="tree"><a href="#tree" class="headerlink" title="tree"></a>tree</h3><p>输出文件树</p>
<h3 id="youtube-dl"><a href="#youtube-dl" class="headerlink" title="youtube-dl"></a>youtube-dl</h3><p>下载视频</p>
<h3 id="zathura"><a href="#zathura" class="headerlink" title="zathura"></a>zathura</h3><p>pdf浏览器</p>
<h3 id="git-open"><a href="#git-open" class="headerlink" title="git open"></a>git open</h3><p>快速用浏览器打开当前仓库界面</p>
<h3 id="cht-sh"><a href="#cht-sh" class="headerlink" title="cht.sh"></a>cht.sh</h3><p>查看命令的使用方法</p>
<h3 id="nmap"><a href="#nmap" class="headerlink" title="nmap"></a>nmap</h3><p>局域网扫描</p>
]]></content>
  </entry>
  <entry>
    <title>ImageCaptioning</title>
    <url>/2021/07/20/ImageCaptioning/</url>
    <content><![CDATA[<p>这里汇总下最近看的Image Captioning相关的论文的特点和不足。</p>
<h1 id="Knowing-When-to-Look-Adaptive-Attention-via-A-Visual-Sentinel-for-Image-Captioning"><a href="#Knowing-When-to-Look-Adaptive-Attention-via-A-Visual-Sentinel-for-Image-Captioning" class="headerlink" title="Knowing When to Look: Adaptive Attention via A Visual Sentinel for Image Captioning"></a><a href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Lu_Knowing_When_to_CVPR_2017_paper.pdf">Knowing When to Look: Adaptive Attention via A Visual Sentinel for Image Captioning</a></h1><p>这篇论文主要是提出了，Image Captioning生成一句caption的过程中，不是每个单词都需要使用图片信息，有的反而是利用文本信息比较多。 比如生成a herd of sheep grazing on a lush green hillside。这句话中的a,herd,of,a,lush都无法从图片中得到任何有用的信息，只能依靠语言模型来生成，这其中的sheep,hillside可以通过scene graph中的node得到有用信息，其中的graze,on这类词汇可以通过scene graph中的relation信息来补充，其中的green可以通过scene graph中的attributes信息来补充。</p>
<p>所以通过这篇论文可以确立我论文的几个基本点：</p>
<ol>
<li><p>首先使用使用经典的scene graph网络将图片转换为一个scene graph其中包含，node, relation, attributes这三种信息。  这时，scene graph包含的node, ralation和attributes的数量都是很大的。所以后面还需要考虑如何有效的选择这三类属性。</p>
</li>
<li><p>第二个点就是需要像这篇网络中一样，设计一个可以判读何时更多的利用scene graph的信息和何时更多的利用以及生成的文本信息。 这篇论文是基于LSTM来做的， 因为最近Transformer很火，所以这里就考虑基于transformer 来设计这样一个网络。</p>
</li>
</ol>
<p>然后最近Transformer在各个领域都有很多的尝试。 </p>
<p>下面对使用Transformer做Image Captioning任务的论文做一个整理</p>
<h1 id="CPTR-FULL-TRANSFORMER-NETWORK-FOR-IMAGE-CAPTIONING"><a href="#CPTR-FULL-TRANSFORMER-NETWORK-FOR-IMAGE-CAPTIONING" class="headerlink" title="CPTR: FULL TRANSFORMER NETWORK FOR IMAGE CAPTIONING"></a><a href="https://arxiv.org/pdf/2101.10804.pdf">CPTR: FULL TRANSFORMER NETWORK FOR IMAGE CAPTIONING</a></h1><p>这篇论文就是存粹的将 transformer  用到了 image captioning中， 而且是直接 seq2seq 直接生成整个句子，而不是一个词一个词生成的。</p>
<h1 id="X-Linear-Attention-Networks-for-Image-Captioning"><a href="#X-Linear-Attention-Networks-for-Image-Captioning" class="headerlink" title="X-Linear Attention Networks for Image Captioning"></a><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Pan_X-Linear_Attention_Networks_for_Image_Captioning_CVPR_2020_paper.pdf">X-Linear Attention Networks for Image Captioning</a></h1><p>这篇论文算是在transformer的基础上做了一个增强，也是使用了object的信息。</p>
<p>下面对使用Cene graph做Image Captioning任务的论文做一个整理</p>
]]></content>
  </entry>
  <entry>
    <title>分界线</title>
    <url>/2022/10/08/%E5%88%86%E7%95%8C%E7%BA%BF/</url>
    <content><![CDATA[<h1 id="分界线"><a href="#分界线" class="headerlink" title="分界线"></a>分界线</h1>]]></content>
  </entry>
  <entry>
    <title>attention_is_all_you_need</title>
    <url>/2020/12/03/attention-is-all-you-need/</url>
    <content><![CDATA[<h1 id="Attention-Is-All-You-Need"><a href="#Attention-Is-All-You-Need" class="headerlink" title="Attention Is All You Need"></a>Attention Is All You Need</h1><h5 id="论文来源：NIPS-2017"><a href="#论文来源：NIPS-2017" class="headerlink" title="论文来源：NIPS 2017"></a>论文来源：NIPS 2017</h5><h5 id="论文链接：https-arxiv-org-pdf-1706-03762-pdf"><a href="#论文链接：https-arxiv-org-pdf-1706-03762-pdf" class="headerlink" title="论文链接：https://arxiv.org/pdf/1706.03762.pdf"></a>论文链接：<a href="https://arxiv.org/pdf/1706.03762.pdf">https://arxiv.org/pdf/1706.03762.pdf</a></h5><h5 id="代码链接：https-github-com-pytorch-fairseq"><a href="#代码链接：https-github-com-pytorch-fairseq" class="headerlink" title="代码链接：https://github.com/pytorch/fairseq"></a>代码链接：<a href="https://github.com/pytorch/fairseq">https://github.com/pytorch/fairseq</a></h5><hr>
<p>Attention Is All You Need，提出了一个<strong>只基于attention的结构</strong>来处理序列模型相关的问题，比如机器翻译。传统的神经机器翻译大都是利用RNN或者CNN来作为encoder-decoder的模型基础，而谷歌最新的只基于Attention的Transformer模型摒弃了固有的定式，并没有用任何CNN或者RNN的结构。该模型可以高度并行地工作，所以在提升翻译性能的同时训练速度也特别快。</p>
<p>作者采用Attention机制的原因是考虑到<strong>RNN（或者LSTM，GRU等）的计算限制为是顺序的</strong>，也就是说RNN相关算法只能从左向右依次计算或者从右向左依次计算，这种机制带来了两个问题：</p>
<p>时间片 $t$ 的计算依赖 $t-1$ 时刻的计算结果，这样<strong>限制了模型的并行能力；</strong></p>
<p>顺序计算的过程中<strong>信息会丢失</strong>，尽管LSTM等门机制的结构一定程度上缓解了长期依赖的问题，但是对于特别长期的依赖现象,LSTM依旧无能为力。</p>
<p>Transformer的提出解决了上面两个问题，首先它使用了Attention机制，将序列中的任意两个位置之间的距离是缩小为一个常量；其次它不是类似RNN的顺序结构，因此具有更好的并行性，符合现有的GPU框架。论文中给出Transformer的定义是：Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence aligned RNNs or convolution。</p>
<h2 id="Transformer结构示意图"><a href="#Transformer结构示意图" class="headerlink" title="Transformer结构示意图"></a>Transformer结构示意图</h2><p><img src="/2020/12/03/attention-is-all-you-need/model_architecture.png" width="80%"></p>
<h2 id="Transformer详解"><a href="#Transformer详解" class="headerlink" title="Transformer详解"></a>Transformer详解</h2><h3 id="高层Transformer"><a href="#高层Transformer" class="headerlink" title="高层Transformer"></a>高层Transformer</h3><p>论文中的验证Transformer的实验室基于机器翻译的，下面我们就以机器翻译为例子详细剖析Transformer的结构，在机器翻译中，Transformer可概括为如图1：</p>
<p><img src="/2020/12/03/attention-is-all-you-need/img1.png" width="80%"></p>
<p>Transformer的本质上是一个Encoder-Decoder的结构，那么图1可以表示为图2的结构：</p>
<p><img src="/2020/12/03/attention-is-all-you-need/img2.png" width="80%"></p>
<p>如论文中所设置的，编码器由6个编码block组成，同样解码器是6个解码block组成。与所有的生成模型相同的是，编码器的输出会作为解码器的输入，如图3所示：</p>
<p><img src="/2020/12/03/attention-is-all-you-need/img3.png" width="80%"></p>
<p>我们继续分析每个encoder的详细结构：在Transformer的encoder中，数据首先会经过一个叫做‘self-attention’的模块得到一个加权之后的特征向量 $Z$ ，这个 $Z$ 便是论文公式1中的 $Attention(Q,K,V)$ ：</p>
<script type="math/tex; mode=display">Z = Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V</script><p>第一次看到这个公式你可能会一头雾水，在后面的文章中我们会揭开这个公式背后的实际含义，在这一段暂时将其叫做 $Z$ 。</p>
<p>得到 $Z$ 之后，它会被送到encoder的下一个模块，即Feed Forward Neural Network。这个全连接有两层，第一层的激活函数是ReLU，第二层是一个线性激活函数，可以表示为：</p>
<script type="math/tex; mode=display">FFN(Z)=max(0,ZW_i+b_1)W_2+b_2</script><p>Encoder的结构如图4所示：</p>
<p><img src="/2020/12/03/attention-is-all-you-need/img4.png" width="80%"></p>
<p>Decoder的结构如图5所示，它和encoder的不同之处在于Decoder多了一个Encoder-Decoder Attention，两个Attention分别用于计算输入和输出的权值：</p>
<ul>
<li>Self-Attention：当前翻译和已经翻译的前文之间的关系；</li>
<li>Encoder-Decnoder Attention：当前翻译和编码的特征向量之间的关系。</li>
</ul>
<p><img src="/2020/12/03/attention-is-all-you-need/img5.png" width="80%"></p>
<h3 id="输入编码"><a href="#输入编码" class="headerlink" title="输入编码"></a>输入编码</h3><p>上节介绍的就是Transformer的主要框架，下面我们将介绍它的输入数据。如图6所示，首先通过Word2Vec等词嵌入方法将输入语料转化成特征向量，论文中使用的词嵌入的维度为 $d_{model}=512$ 。</p>
<p><img src="/2020/12/03/attention-is-all-you-need/img6.png" width="80%"></p>
<p>在最底层的block中， $x$ 将直接作为Transformer的输入，而在其他层中，输入则是上一个block的输出。为了画图更简单，我们使用更简单的例子来表示接下来的过程，如图7所示：</p>
<p><img src="/2020/12/03/attention-is-all-you-need/img7.png" width="80%"></p>
<h3 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self-Attention"></a>Self-Attention</h3><p>Self-Attention是Transformer最核心的内容，然而作者并没有详细讲解，下面我们来补充一下作者遗漏的地方。其核心内容是为输入向量的每个单词学习一个权重，例如在下面的例子中我们判断it代指的内容</p>
<blockquote>
<p>The animal didn’t cross the street because it was too tired</p>
</blockquote>
<p>通过加权之后可以得到类似图8的加权情况，在讲解self-attention的时候我们也会使用图8类似的表示方式</p>
<p><img src="/2020/12/03/attention-is-all-you-need/img8.png" width="80%"></p>
<p>在self-attention中，每个单词有3个不同的向量，它们分别是Query向量（ $Q$ ），Key向量（ $K$ ）和Value向量（ $V$ ），长度均是64。它们是通过3个不同的权值矩阵由嵌入向量 $X$ 乘以三个不同的权值矩阵 $W^Q$ ， $W^K$ ， $W^V$ 得到，其中三个矩阵的尺寸也是相同的。均是 $512×64$ 。</p>
<p><img src="/2020/12/03/attention-is-all-you-need/img9.png" width="80%"></p>
<p>那么Query，Key，Value是什么意思呢？它们在Attention的计算中扮演着什么角色呢？我们先看一下Attention的计算方法，整个过程可以分成7步：</p>
<ol>
<li>如上文，将输入单词转化成嵌入向量；</li>
<li>根据嵌入向量得到 $q$ ， $k$ ， $v$ 三个向量；</li>
<li>为每个向量计算一个score： $score=q・k$ ；</li>
<li>为了梯度的稳定，Transformer使用了score归一化，即除以 $\sqrt{d_k}$ ；</li>
<li>对score施以softmax激活函数；</li>
<li>softmax点乘Value值 $v$ ，得到加权的每个输入向量的评分 $v$ ；</li>
<li>相加之后得到最终的输出结果 $z:z=\sum v$ 。</li>
</ol>
<p>上面步骤的可以表示为图10的形式。</p>
<p><img src="/2020/12/03/attention-is-all-you-need/img10.png" width="80%"></p>
<p>实际计算过程中是采用基于矩阵的计算方式，那么论文中的 $Q$ ， $V$ ， $K$ 的计算方式如图11：</p>
<p><img src="/2020/12/03/attention-is-all-you-need/img11.png" width="80%"></p>
<p>图10总结为如图12所示的矩阵形式:</p>
<p><img src="/2020/12/03/attention-is-all-you-need/img12.png" width="80%"></p>
<p>在self-attention需要强调的最后一点是其采用了残差网络中的short-cut结构，目的当然是解决深度学习中的退化问题，得到的最终结果如图13。</p>
<p><img src="/2020/12/03/attention-is-all-you-need/img13.png" width="80%"></p>
<p>Query，Key，Value的概念取自于信息检索系统，举个简单的搜索的例子来说。当你在某电商平台搜索某件商品（年轻女士冬季穿的红色薄款羽绒服）时，你在搜索引擎上输入的内容便是Query，然后搜索引擎根据Query为你匹配Key（例如商品的种类，颜色，描述等），然后根据Query和Key的相似度得到匹配的内容（Value)。</p>
<p>self-attention中的Q，K，V也是起着类似的作用，在矩阵计算中，点积是计算两个矩阵相似度的方法之一，因此式1中使用了 $QK^T$ 进行相似度的计算。接着便是根据相似度进行输出的匹配，这里使用了加权匹配的方式，而权值就是query与key的相似度。</p>
<h3 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h3><p>Multi-Head Attention相当于 $h$ 个不同的self-attention的集成（ensemble），在这里我们以 $h=8$ 举例说明。Multi-Head Attention的输出分成3步：</p>
<ol>
<li>将数据 $X$ 分别输入到图13所示的8个self-attention中，得到8个加权后的特征矩阵 $Z_i,i\in\{1,2,…,8\}$ 。</li>
<li>将8个 $Z_i$ 按列拼成一个大的特征矩阵；</li>
<li>特征矩阵经过一层全连接后得到输出 $Z$ 。</li>
</ol>
<p>整个过程如图14所示：</p>
<p><img src="/2020/12/03/attention-is-all-you-need/img14.png" width="80%"></p>
<p>同self-attention一样，multi-head attention也加入了short-cut机制。</p>
<h3 id="Encoder-Decoder-Attention"><a href="#Encoder-Decoder-Attention" class="headerlink" title="Encoder-Decoder Attention"></a>Encoder-Decoder Attention</h3><p>在解码器中，Transformer block比编码器中多了个encoder-cecoder attention。在encoder-decoder attention中， $Q$ 来之与解码器的上一个输出， $K$ 和 $V$ 则来自于与编码器的输出。其计算方式完全和图10的过程相同。</p>
<p>由于在机器翻译中，解码过程是一个顺序操作的过程，也就是当解码第 $k$ 个特征向量时，我们只能看到第 $k-1$ 及其之前的解码结果，论文中把这种情况下的multi-head attention叫做masked multi-head attention。</p>
<h3 id="损失层"><a href="#损失层" class="headerlink" title="损失层"></a>损失层</h3><p>解码器解码之后，解码的特征向量经过一层激活函数为softmax的全连接层之后得到反映每个单词概率的输出向量。此时我们便可以通过CTC等损失函数训练模型了。</p>
<p>而一个完整可训练的网络结构便是encoder和decoder的堆叠（各 $N$ 个， $N=6$ ），我们可以得到图15中的完整的Transformer的结构（即论文中的图1）：</p>
<p><img src="/2020/12/03/attention-is-all-you-need/model_architecture.png" width="80%"></p>
<h2 id="位置编码"><a href="#位置编码" class="headerlink" title="位置编码"></a>位置编码</h2><p>截止目前为止，我们介绍的Transformer模型并没有捕捉顺序序列的能力，也就是说无论句子的结构怎么打乱，Transformer都会得到类似的结果。换句话说，Transformer只是一个功能更强大的词袋模型而已。</p>
<p>为了解决这个问题，论文中在编码词向量时引入了位置编码（Position Embedding）的特征。具体地说，位置编码会在词向量中加入了单词的位置信息，这样Transformer就能区分不同位置的单词了。</p>
<p>那么怎么编码这个位置信息呢？常见的模式有：a. 根据数据学习；b. 自己设计编码规则。在这里作者采用了第二种方式。那么这个位置编码该是什么样子呢？通常位置编码是一个长度为 $d_{model}$ 的特征向量，这样便于和词向量进行单位加的操作，如图16。</p>
<p><img src="/2020/12/03/attention-is-all-you-need/img16.png" width="80%"></p>
<p>论文给出的编码公式如下：</p>
<script type="math/tex; mode=display">PE(pos,2i)=sin(\frac{pos}{10000^{\frac{2i}{d_{model}}}})</script><script type="math/tex; mode=display">PE(pos,2i+1)=cos(\frac{pos}{10000^{\frac{2i}{d_{model}}}})</script><p>在上式中， $pos$ 表示单词的位置， $i$ 表示单词的维度。关于位置编码的实现可在Google开源的算法中get_timing_signal_1d()函数找到对应的代码。</p>
<p>作者这么设计的原因是考虑到在NLP任务中，除了单词的绝对位置，单词的相对位置也非常重要。根据公式 $\sin(\alpha+\beta)=\sin\alpha \cos\beta + \cos\alpha \sin\beta$ 以及$\cos(\alpha+\beta)=\cos\alpha \cos\beta - \sin\alpha \sin\beta$ ，这表明位置 $k+p$ 的位置向量可以表示为位置 $k$ 的特征向量的线性变化，这为模型捕捉单词之间的相对位置关系提供了非常大的便利。</p>
<h2 id="Layer-normalization"><a href="#Layer-normalization" class="headerlink" title="Layer normalization"></a>Layer normalization</h2><p>在transformer中，每一个子层（self-attetion，ffnn）之后都会接一个残差模块，并且有一个Layer normalization</p>
<p><img src="/2020/12/03/attention-is-all-you-need/img17.png" width="80%"></p>
<p>在进一步探索其内部计算方式，我们可以将上面图层可视化为下图：</p>
<p><img src="/2020/12/03/attention-is-all-you-need/img18.png" width="80%"></p>
<p>残差模块相信大家都很清楚了，这里不再讲解，主要讲解下Layer normalization。Normalization有很多种，但是它们都有一个共同的目的，那就是把输入转化成均值为0方差为1的数据。我们在把数据送入激活函数之前进行normalization（归一化），因为我们不希望输入数据落在激活函数的饱和区。</p>
<p>说到 normalization，那就肯定得提到 Batch Normalization。BN的主要思想就是：在每一层的每一批数据上进行归一化。我们可能会对输入数据进行归一化，但是经过该网络层的作用后，我们的数据已经不再是归一化的了。随着这种情况的发展，数据的偏差越来越大，我的反向传播需要考虑到这些大的偏差，这就迫使我们只能使用较小的学习率来防止梯度消失或者梯度爆炸。</p>
<p>BN的具体做法就是对每一小批数据，在批这个方向上做归一化。如下图所示：<br><img src="/2020/12/03/attention-is-all-you-need/img19.png" width="80%"></p>
<p>可以看到，右半边求均值是沿着数据 batch_size的方向进行的，其计算公式如下：</p>
<script type="math/tex; mode=display">BN(x_i) = \alpha \times \frac{x_i - \mu_b}{\sqrt{\sigma_B^2+\epsilon}}+\beta</script><p>那么什么是 Layer normalization 呢？它也是归一化数据的一种方式，不过 LN 是在每一个样本上计算均值和方差，而不是BN那种在批方向计算均值和方差！</p>
<p><img src="/2020/12/03/attention-is-all-you-need/img20.png" width="80%"></p>
<script type="math/tex; mode=display">BN(x_i) = \alpha \times \frac{x_i - \mu_L}{\sqrt{\sigma_L^2+\epsilon}}+\beta</script><p>到这里为止就是全部encoders的内容了，如果把两个encoders叠加在一起就是这样的结构，在self-attention需要强调的最后一点是其采用了残差网络中的short-cut结构，目的是解决深度学习中的退化问题。<br><img src="/2020/12/03/attention-is-all-you-need/img21.png" width="80%"></p>
<h2 id="Decoder层"><a href="#Decoder层" class="headerlink" title="Decoder层"></a>Decoder层</h2><p>decoder部分其实和encoder部分大同小异，不过在最下面额外多了一个masked mutil-head attetion，这里的mask也是transformer一个很关键的技术，我们一起来看一下。</p>
<h3 id="Mask"><a href="#Mask" class="headerlink" title="Mask"></a>Mask</h3><p>mask 表示掩码，它对某些值进行掩盖，使其在参数更新时不产生效果。Transformer 模型里面涉及两种 mask，分别是 padding mask 和 sequence mask。<br>其中，padding mask 在所有的 scaled dot-product attention 里面都需要用到，而 sequence mask 只有在 decoder 的 self-attention 里面用到。</p>
<h3 id="Padding-Mask"><a href="#Padding-Mask" class="headerlink" title="Padding Mask"></a>Padding Mask</h3><p>什么是 padding mask 呢？因为每个批次输入序列长度是不一样的也就是说，我们要对输入序列进行对齐。具体来说，就是给在较短的序列后面填充 0。但是如果输入的序列太长，则是截取左边的内容，把多余的直接舍弃。因为这些填充的位置，其实是没什么意义的，所以我们的attention机制不应该把注意力放在这些位置上，所以我们需要进行一些处理。</p>
<p>具体的做法是，把这些位置的值加上一个非常大的负数(负无穷)，这样的话，经过 softmax，这些位置的概率就会接近0！</p>
<p>而我们的 padding mask 实际上是一个张量，每个值都是一个Boolean，值为 false 的地方就是我们要进行处理的地方。</p>
<h3 id="Sequence-mask"><a href="#Sequence-mask" class="headerlink" title="Sequence mask"></a>Sequence mask</h3><p>文章前面也提到，sequence mask 是为了使得 decoder 不能看见未来的信息。也就是对于一个序列，在 time_step 为 t 的时刻，我们的解码输出应该只能依赖于 t 时刻之前的输出，而不能依赖 t 之后的输出。因此我们需要想一个办法，把 t 之后的信息给隐藏起来。</p>
<p>那么具体怎么做呢？也很简单：产生一个上三角矩阵，上三角的值全为0。把这个矩阵作用在每一个序列上，就可以达到我们的目的。</p>
<p>对于 decoder 的 self-attention，里面使用到的 scaled dot-product attention，同时需要padding mask 和 sequence mask 作为 attn_mask，具体实现就是两个mask相加作为attn_mask。<br>其他情况，attn_mask 一律等于 padding mask。<br>编码器通过处理输入序列启动。然后将顶部编码器的输出转换为一组注意向量k和v。每个解码器将在其“encoder-decoder attention”层中使用这些注意向量，这有助于解码器将注意力集中在输入序列中的适当位置：</p>
<p>编码器通过处理输入序列启动。然后将顶部编码器的输出转换为一组注意向量k和v。每个解码器将在其“encoder-decoder attention”层中使用这些注意向量，这有助于解码器将注意力集中在输入序列中的适当位置：<br><img src="/2020/12/03/attention-is-all-you-need/22.gif" width="80%"></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p><strong>优点：</strong></p>
<p>（1）虽然Transformer最终也没有逃脱传统学习的套路，Transformer也只是一个全连接（或者是一维卷积）加Attention的结合体。但是其设计已经足够有创新，因为其抛弃了在NLP中最根本的RNN或者CNN并且取得了非常不错的效果，算法的设计非常精彩，值得每个深度学习的相关人员仔细研究和品位。</p>
<p>（2）Transformer的设计最大的带来性能提升的关键是将任意两个单词的距离是1，这对解决NLP中棘手的长期依赖问题是非常有效的。</p>
<p>（3）Transformer不仅仅可以应用在NLP的机器翻译领域，甚至可以不局限于NLP领域，是非常有科研潜力的一个方向。</p>
<p>（4）算法的并行性非常好，符合目前的硬件（主要指GPU）环境。</p>
<p><strong>缺点：</strong></p>
<p>（1）粗暴的抛弃RNN和CNN虽然非常炫技，但是它也使模型丧失了捕捉局部特征的能力，RNN + CNN + Transformer的结合可能会带来更好的效果。</p>
<p>（2）Transformer失去的位置信息其实在NLP中非常重要，而论文中在特征向量中加入Position Embedding也只是一个权宜之计，并没有改变Transformer结构上的固有缺陷。</p>
]]></content>
      <categories>
        <category>paper</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>attention mechanism</tag>
      </tags>
  </entry>
  <entry>
    <title>数理统计和多元统计</title>
    <url>/2020/12/09/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E5%92%8C%E5%A4%9A%E5%85%83%E7%BB%9F%E8%AE%A1/</url>
    <content><![CDATA[<h1 id="数理统计和多元统计知识点汇总"><a href="#数理统计和多元统计知识点汇总" class="headerlink" title="数理统计和多元统计知识点汇总"></a>数理统计和多元统计知识点汇总</h1><hr>
<h2 id="分布函数"><a href="#分布函数" class="headerlink" title="分布函数"></a>分布函数</h2><h3 id="0-1分布"><a href="#0-1分布" class="headerlink" title="0-1分布"></a>0-1分布</h3><p>$X \sim B(1,p)$</p>
<p>$P\{x=k\} = p^k(1-p)^{1-k} \quad k=0,1$</p>
<p>$E(X) = p \qquad D(X) = p(1-p)$</p>
<h3 id="几何分布"><a href="#几何分布" class="headerlink" title="几何分布"></a>几何分布</h3><p>$P(A) = p$</p>
<p>第k次首次发生，前k-1次均为发生。</p>
<p>$P\{x=k\} = (1-p)^{k-1}p$</p>
<h3 id="二项分布-baibinomial"><a href="#二项分布-baibinomial" class="headerlink" title="二项分布 baibinomial"></a>二项分布 baibinomial</h3><p>$X \sim B(n,p)$</p>
<p>$P(A)=p$</p>
<p>n次实验发生了k次。</p>
<p>$P\{x = k\} = C_n^kp^k(1-p)^{n-k} \quad k = 0,1,2,…,n$</p>
<p>$E(X)=np \qquad D(X)=np(1-p)$</p>
<h3 id="泊松分布-poisson"><a href="#泊松分布-poisson" class="headerlink" title="泊松分布 poisson"></a>泊松分布 poisson</h3><p>$X \sim \pi(\lambda)$</p>
<p>$P\{x=k\} = \frac{\lambda^k}{k!} e^{-\lambda} \quad k=1,2,… \quad \lambda &gt; 0$</p>
<p>$E(X) = \lambda \qquad D(X)=\lambda$</p>
<h3 id="均匀分布-uniforn"><a href="#均匀分布-uniforn" class="headerlink" title="均匀分布 uniforn"></a>均匀分布 uniforn</h3><p>$X \sim U[a,b]$</p>
<p>$f(x) = \begin{cases}<br>   \frac{1}{b-a} &amp; &amp; a\leq x \leq b \\\\<br>   0 &amp; &amp; 其它<br>\end{cases}$</p>
<p>$F(x) = \begin{cases}<br>   0 &amp; &amp; x&lt;a \\\\<br>   \frac{x-a}{b-a} &amp; &amp; a \leq x &lt;b \\\\<br>   1 &amp; &amp; x \geq b<br>\end{cases}$</p>
<p>$E(X)= \frac{a+b}{2} \qquad D(X) = \frac{(b-a)^2}{12}$</p>
<h3 id="指数分布-exponential"><a href="#指数分布-exponential" class="headerlink" title="指数分布 exponential"></a>指数分布 exponential</h3><p>$X \sim E(\lambda)$</p>
<p>$f(x) = \begin{cases}<br>   \lambda e ^{-\lambda x} &amp; &amp; x&gt;0 \\\\<br>   0 &amp; &amp; x\leq 0<br>\end{cases} \quad \lambda &gt; 0$</p>
<p>$F(x)= \begin{cases}<br>   1-e^{- \lambda x} &amp; &amp; x&gt;0\\\\<br>   0 &amp; &amp; x\leq 0<br>\end{cases}$</p>
<p>$E(X) = \frac{1}{\lambda} \qquad D(X)=\frac{1}{\lambda^2}$</p>
<h2 id="第一章"><a href="#第一章" class="headerlink" title="第一章"></a>第一章</h2><h3 id="期望和方差的性质"><a href="#期望和方差的性质" class="headerlink" title="期望和方差的性质"></a>期望和方差的性质</h3><ol>
<li>$E(c) = c$</li>
<li>$E(aX+bY) = aE(X)+bE(Y)$</li>
<li>$D(X)=E\{[X-E(X)]^2\}=E(X^2)-E(X)^2$</li>
<li>$D(\sum\limits_{i=1}^nC_iX_i) = \sum\limits_{i=1}^n {C_i}^2 D(X_i)$</li>
</ol>
<h3 id="正态分布的性质"><a href="#正态分布的性质" class="headerlink" title="正态分布的性质"></a>正态分布的性质</h3><p>$f(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{(\frac{-(x-\mu)^2}{2\sigma^2})}$</p>
<ol>
<li>$X \sim N(\mu , \sigma^2) \rightarrow X-a \sim N(\mu-a,\sigma^2)$</li>
<li>$X \sim N(\mu , \sigma^2) \rightarrow aX \sim N(a\mu,a^2\sigma^2)$</li>
<li><p>$X \sim N({\mu}_x , {\sigma_x}^2) ,Y \sim N({\mu}_y , {\sigma_y}^2)$</p>
<p>$X+Y \sim N(\mu_x+\mu_y,{\sigma_x}^2+{\sigma_y}^2)$</p>
<p>$X-Y \sim N(\mu_x-\mu_y,{\sigma_x}^2+{\sigma_y}^2)$</p>
</li>
<li><p>$z_\alpha$为$N(0,1)分布的$上$\alpha$分为点。$z_\alpha = -z_{1-\alpha}$</p>
</li>
</ol>
<h3 id="常见统计量"><a href="#常见统计量" class="headerlink" title="常见统计量"></a>常见统计量</h3><ol>
<li>样本方差：   $S^2=\frac{1}{n-1}\sum\limits_{i=1}^n(X_i-\bar{X})^2$</li>
<li>标准差：     $S=\sqrt{S^2}$</li>
<li>二阶中心距：  ${S_n}^2=\frac{1}{n}\sum\limits_{i=1}^n(X_i-\bar{X})^2$</li>
<li>次序统计量： $X_{(i)}$为$(X_1,X_2,…,X_n)$中第$i$大的值。</li>
</ol>
<h3 id="chi-2-分布及其性质"><a href="#chi-2-分布及其性质" class="headerlink" title="$\chi^2$分布及其性质"></a>$\chi^2$分布及其性质</h3><p>$X_1,X_2,…,X_n \sim N(0,1)$</p>
<p>$Y \stackrel{def}{\rightarrow}\sum\limits_{i=1}^n{X_i}^2 \sim \chi^2(n)$</p>
<ol>
<li>$E(\chi^2(n))=n$</li>
<li>$D(\chi^2(n))=2n$</li>
<li>$X\sim\chi^2(n),Y\sim\chi^2(m) \rightarrow X+Y\sim\chi^2(n+m)$</li>
</ol>
<p>$n \leq 45$时，$\chi^2_{\alpha}(n)$直接查表，当$n&gt;45$时，$\chi^2_{\alpha}(n) \approx \frac{1}{2}(z_\alpha+\sqrt{2n-1})^2$</p>
<h3 id="t分布及其性质"><a href="#t分布及其性质" class="headerlink" title="t分布及其性质"></a>t分布及其性质</h3><p>$X \sim N(0,1), Y\sim\chi^2(n)$,$X$与$Y$相互独立。</p>
<p>$T \stackrel{def}{\rightarrow} \frac{X}{\sqrt{ {Y}/{n} } } \sim t(n)$</p>
<ol>
<li>$t_{\alpha}(n)=-t_{1-{\alpha}}(n)$</li>
</ol>
<h3 id="F分布及其性质"><a href="#F分布及其性质" class="headerlink" title="F分布及其性质"></a>F分布及其性质</h3><p>$X\sim\chi^2(n),Y\sim\chi^2(m)$,$X$与$Y$相互独立。</p>
<p>$F \stackrel{def}{\rightarrow}\frac{X/n}{Y/m} \sim F(n,m)$</p>
<ol>
<li>$F_{\alpha}(n,m)=\frac{1}{F_{1-{\alpha}}(m,n)}$</li>
</ol>
<h2 id="第二章"><a href="#第二章" class="headerlink" title="第二章"></a>第二章</h2><h3 id="正态总体分布单样本抽样定理"><a href="#正态总体分布单样本抽样定理" class="headerlink" title="正态总体分布单样本抽样定理"></a>正态总体分布单样本抽样定理</h3><p>设$(X_1,X_2,…,X_n)$是取自总体$N(\mu,\sigma^2)$的一组简单随机样本，则有：</p>
<ol>
<li>$\bar{X}\sim N(\mu,\frac{\sigma^2}{n}) \stackrel{标准化}{\rightarrow} \frac{\bar{X}-\mu}{\sigma/\sqrt{(n)}}\sim N(0,1)$</li>
<li>$\sum\limits_{i=1}^n(\frac{X_i-\mu}{\sigma})^2 = \frac{1}{\sigma^2}\sum\limits_{i=1}^n(X_i-\mu)^2\sim\chi^2(n)$</li>
<li>$\frac{(n-1)S^2}{\sigma^2}=\frac{nS_n^2}{\sigma^2}\sim\chi^2(n-1)$</li>
<li>$\bar{X}$与$S^2$ 相互独立</li>
<li>$\frac{\bar{X}-\mu}{S/\sqrt{n}} = \frac{\bar{X}-\mu}{S_n/\sqrt{n-1}}\sim t(n-1)$</li>
</ol>
<h3 id="正态分布双样本抽样定理"><a href="#正态分布双样本抽样定理" class="headerlink" title="正态分布双样本抽样定理"></a>正态分布双样本抽样定理</h3><p>$(X_1,X_2,…,X_n)$取自$X\sim N(\mu_1,{\sigma_1}^2)$, $(Y_1,Y_2,…,Y_m)$取自$Y\sim N(\mu_2,{\sigma_2}^2)$</p>
<p>$X$与$Y$相互独立，则：</p>
<ol>
<li><p>$\bar{X}\sim N(\mu_1,\frac{\sigma_1^2}{n})$, $\bar{Y}\sim N(\mu_2,\frac{\sigma_2^2}{m})$</p>
<p>$\bar{X}-\bar{Y}\sim N(\mu_1-\mu_2,\frac{\sigma_1^2}{n}+\frac{\sigma_2^2}{m})$</p>
<p>$\frac{(\bar{X}-\bar{Y})-(\mu_1-\mu_2)}{\sqrt{(\frac{\sigma_1^2}{n}+\frac{\sigma_2^2}{m})}}\sim N(0,1)$</p>
</li>
<li><p>$\frac{(n-1)S_1^2}{ { \sigma_1}^2} \sim \chi^2(n-1), \frac{(m-1)S_2^2}{ { \sigma_2}^2} \sim \chi^2(m-1)$</p>
<p>$\frac{(n-1)S_1^2}{ {\sigma_1}^2} + \frac{(m-1)S_2^2}{ {\sigma_2}^2} \sim \chi^2(n+m-2)$</p>
</li>
</ol>
<p>当$\sigma_1^2=\sigma_2^2$时：</p>
<p>   $\frac{(n-1)S_1^2+(m-1)S_2^2}{\sigma^2}\sim \chi^2(n+m-2)$</p>
<ol>
<li><p>$\frac{(n-1)S_1^2}{ {\sigma_1}^2} \sim \chi^2(n-1), \frac{(m-1)S_2^2}{ {\sigma_2}^2} \sim \chi^2(m-1)$</p>
<p>$\frac{S_1^2/\sigma_1^2}{S_2^2/\sigma_2^2}\sim F(n-1,m-1)$</p>
</li>
</ol>
<h3 id="矩估计"><a href="#矩估计" class="headerlink" title="矩估计"></a>矩估计</h3><p>$E(X)=? \qquad E(X^2)=?$</p>
<p>$\theta=f(E(X),E(X^2),…)$</p>
<p>用$A_1,A_2,…$代替$E(X),E(X^2)$,$A_1=\frac{1}{n}\sum\limits_{i=1}^{n}X_i,A_2=\frac{1}{n}\sum\limits_{i=1}^{n}X_i^2$</p>
<p>$\hat{\theta}=f(A_1,A_2,…)$</p>
<h3 id="极大似然估计"><a href="#极大似然估计" class="headerlink" title="极大似然估计"></a>极大似然估计</h3><p>$L(X_1,X_2,…,X_n,\theta)=\prod\limits_{i=1}^nf(X_i,\theta)$</p>
<p>$\frac{\partial L_n(L)}{\partial \theta}=0$ 解得$\theta=\hat{\theta}$</p>
<p>$\frac{\partial^2 L_n(L)}{ {\partial \theta} ^2}|_{\theta=\hat {\theta} }&lt;0$得$\hat\theta$为$\theta$的极大似然估计。</p>
<h3 id="估计量和估计值"><a href="#估计量和估计值" class="headerlink" title="估计量和估计值"></a>估计量和估计值</h3><p>估计量：$X_1,X_2,…,X_n$表示出$\theta$。</p>
<p>估计值：用观测值$x_1,x_2,…,x_n$带入估计量求出$\theta$。</p>
<h3 id="无偏性和有效性"><a href="#无偏性和有效性" class="headerlink" title="无偏性和有效性"></a>无偏性和有效性</h3><p>$\hat\theta$为$\theta$的一个估计量，则$E(\hat\theta)=\theta \Rightarrow 无偏$，$D(\hat\theta)$越小，$\hat\theta$越有效。</p>
<h3 id="置信区间"><a href="#置信区间" class="headerlink" title="置信区间"></a>置信区间</h3><p>$P\{\underline{\theta}(X_1,X_2,…,X_n)\leq\theta\leq \overline\theta(X_1,X_2,…,X_n)\}\geq1-\alpha$</p>
<p>其中，$1-\alpha$称为置信度，区间$[\underline{\theta}(X_1,X_2,…,X_n),\overline\theta(X_1,X_2,…,X_n)]$称为未知参数$\theta$的置信度为$1-\alpha$的置信区间。</p>
<p><strong>步骤：</strong></p>
<ol>
<li>点估计$\hat\theta$ </li>
<li>$J(\theta,\hat\theta),J分布已知，可查表$</li>
<li>$P\{a\leq J(\theta,\hat\theta) \leq b\}\geq 1-\alpha \Rightarrow a,b(查表得a,b)$</li>
<li>由3变形得到 $\hat\theta\in[a’,b’]$</li>
</ol>
<p><strong>示例：</strong></p>
<p>已知$N(\mu,\sigma^2)$的$\sigma^2$,求$\mu$的双侧$1-\alpha$的置信区间。</p>
<ol>
<li>点估计$\hat\mu=\overline x$</li>
<li>$P(\overline x \leq \mu \leq \overline x +b)=1-\alpha$</li>
<li>变形 $P(a’\leq \frac{\overline x - \mu}{\sigma/\sqrt n }\leq b’)=1-\alpha$</li>
<li>取$a’=-Z_{1-\frac{\alpha}{2}},b’=Z_{-\frac{\alpha}{2}}$</li>
</ol>
<h2 id="第三章"><a href="#第三章" class="headerlink" title="第三章"></a>第三章</h2><h3 id="假设检验"><a href="#假设检验" class="headerlink" title="假设检验"></a>假设检验</h3><p>原假设$H_0$,例$\mu=\mu_0$  </p>
<p>备择假设$H_1$,例$\mu \neq \mu_0,\mu&gt;\mu_0\And\mu&lt;\mu_0$</p>
<p>显著性水平$\alpha$:表示容忍犯第一类错误概率的上限，通常较小。</p>
<p>结论：</p>
<ol>
<li>当找到足够的证据支持备择假设则拒绝$H_0$，接受$H_1$。</li>
<li>当未找到足够的证据支持$H_1$,则拒绝$H_1$,接受$H_0$。</li>
</ol>
<h3 id="两类错误"><a href="#两类错误" class="headerlink" title="两类错误"></a>两类错误</h3><div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>$H_0$成立</th>
<th>$H_1$成立/$H_0$不成立</th>
</tr>
</thead>
<tbody>
<tr>
<td>接受$H_0$</td>
<td>正确</td>
<td>第一类错误</td>
</tr>
<tr>
<td>拒绝$H_0$</td>
<td>第二类错误</td>
<td>正确</td>
</tr>
</tbody>
</table>
</div>
<p>$P(第一类错误)=P(拒绝H_0｜H_0成立)=P(落入拒绝域｜H_0成立)$</p>
<p>$P(第二类错误)=P(接受H_0｜H_0不成立)=P(接受H_0｜H_1成立)=P(未落入拒绝域｜H_1成立)$</p>
<h3 id="假设检验的基本步骤"><a href="#假设检验的基本步骤" class="headerlink" title="假设检验的基本步骤"></a>假设检验的基本步骤</h3><ol>
<li>建立原假设和备择假设，$H_0和H_1$。</li>
<li>求未知参数$\theta$的一个点估计$\hat\theta$</li>
<li>构造$J(\theta,\hat\theta)$,$J$分布已知</li>
<li>由$H_0,H_1和J(\theta,\hat\theta)$构造出$W$拒绝域</li>
<li>判定是否落入拒绝域$\Rightarrow 拒绝H_0/接受H_0$</li>
</ol>
<h3 id="其它分布的假设检验"><a href="#其它分布的假设检验" class="headerlink" title="其它分布的假设检验"></a>其它分布的假设检验</h3><h4 id="比例-p-的假设检验"><a href="#比例-p-的假设检验" class="headerlink" title="比例$p$的假设检验"></a>比例$p$的假设检验</h4><p>$X \sim B(1,p)$</p>
<p>$n\overline{X} = \sum\limits_{i=1}^nX_i \sim B(n,p)$</p>
<h4 id="泊松分布的假设检验"><a href="#泊松分布的假设检验" class="headerlink" title="泊松分布的假设检验"></a>泊松分布的假设检验</h4><p>$X \sim \pi(\lambda)$</p>
<p>$n\overline{X} = \sum\limits_{i=1}^nX_i \sim \pi(n\lambda)$</p>
<h4 id="大样本检验"><a href="#大样本检验" class="headerlink" title="大样本检验"></a>大样本检验</h4><p>$X \sim B(1,p)$</p>
<p>$\frac{\overline{X}-p}{\sqrt{p(1-p)/n}}\sim N(0,1)$</p>
<p>$X \sim \pi(\lambda)$</p>
<p>$\frac{\overline{x}-\lambda}{\sqrt{\lambda/n}} \sim N(0,1)$</p>
<p>上面这两个都是中心极限定理得到的。</p>
<p>$\frac{\overline{x}-E(X)}{\sqrt{D(X)/n}} \sim N(0,1)$</p>
<h4 id="指数分布的参数假设"><a href="#指数分布的参数假设" class="headerlink" title="指数分布的参数假设"></a>指数分布的参数假设</h4><p>$X \sim E(\lambda)$</p>
<p>$2n\lambda \overline{X} \sim \chi^{2}(2n)$</p>
<h4 id="均匀分布的参数假设"><a href="#均匀分布的参数假设" class="headerlink" title="均匀分布的参数假设"></a>均匀分布的参数假设</h4><p>$X \sim U(0,\theta)$</p>
<p>$J = \frac{X_{(n)}}{\theta}$</p>
<p>$<br>   F_{J}(x) =<br>   \begin{cases}<br>      0 &amp;,&amp; x &lt; 0\\\\<br>      x^n &amp;,&amp; 0 \leq x\leq1 \\\\<br>      1 &amp;,&amp; x &gt;1<br>   \end{cases}<br>$</p>
<h3 id="卡方检验"><a href="#卡方检验" class="headerlink" title="卡方检验"></a>卡方检验</h3><ol>
<li>假设$H_0$:样本服从某某分布， $H_1$: 样本不服从某某分布</li>
<li>用极大似然法估计分布函数中的未知参数。</li>
<li>使用分布函数的概率函数求出每个$\hat{p_i}$</li>
<li>计算$\sum\limits_{i=1}^r \frac{(n_i-n\hat p_i)^2}{n\hat p_i} = \hat\chi^2$</li>
<li>查表得$\chi_\alpha^2(r-m-1)$,$r$为分组的个数，$m$为使用极大似然估计估计的参数个数。</li>
<li>若$\hat\chi^2 &gt; \chi_\alpha^2(r-m-1)$ 则不服从，若$\hat\chi^2 \leq \chi_\alpha^2(r-m-1)$ 则服从。</li>
</ol>
<h2 id="第四章"><a href="#第四章" class="headerlink" title="第四章"></a>第四章</h2><h3 id="次序统计量"><a href="#次序统计量" class="headerlink" title="次序统计量"></a>次序统计量</h3><p>总体$X$的分布函数为$F(x)$,$X_{(r)}$的分布函数为$F_r(x)$</p>
<p>$F_r(x) = P\{X_{(r)}\leq x\} = P\{X_1,X_2,…,X_n中至少有r个取值小于等于x\}= \sum\limits_{j=r}^nC_n^jF(x)^j[1-F(x)]^{n-j} \quad r=(1,2,…,n)$</p>
<p>当$r=1$时，$F_r(x) = 1-[1-F(x)]^n$</p>
<p>当$r=n$时，$F_r(x) = F(x)^n$</p>
<h3 id="极差"><a href="#极差" class="headerlink" title="极差"></a>极差</h3><p>$R = X_{(n)} - X_{(1)}$</p>
<h3 id="p分位数"><a href="#p分位数" class="headerlink" title="p分位数"></a>p分位数</h3><p>$[\alpha]$为不超过$\alpha$的最大整数，则对任意的$0&lt;p&lt;1$，称$X_{([np]+1)}$是样本的p分为数，记为$\tilde{X_p}$</p>
<h3 id="中位数"><a href="#中位数" class="headerlink" title="中位数"></a>中位数</h3><p>总数为奇数时为中间那个，偶数时，中间两个相加除以2。</p>
<h3 id="秩统计量"><a href="#秩统计量" class="headerlink" title="秩统计量"></a>秩统计量</h3><p>每得到一组观测值$(x_1,…,x_n)$,将其由小到大排序，若$X_i$的观测值$x_i$排在第$r_i$位，就用$r_i$作为$R_i$的观测值，$(R_1,…,R_n)$称为秩统计量。</p>
<p>当样本中有重复值的时候，秩统计量需要求均值。</p>
<h3 id="切尾均值"><a href="#切尾均值" class="headerlink" title="切尾均值"></a>切尾均值</h3><p>排序后去掉头尾各α%个数，再求均值。</p>
<h3 id="Winsor化均值："><a href="#Winsor化均值：" class="headerlink" title="Winsor化均值："></a>Winsor化均值：</h3><p>排序后去掉头尾各α%个数，用新的头尾的数填充头尾，使得数量不变，再求均值。</p>
<h3 id="总体分位数的估计"><a href="#总体分位数的估计" class="headerlink" title="总体分位数的估计"></a>总体分位数的估计</h3><p>设总体X的分布函数为$F(X)$,对于给定的$p(0&lt;p&lt;1)$称满足条件$F(\xi_p-0) \leq p \leq F(\xi_p)$ 的$\xi_p$为X分布的p分位数。 表示总体取值不超过它的概率刚好是p。</p>
<p>对给定的置信度$1-\alpha$,求r和s满足$P\{X_{(r)}\leq \xi_p \leq X_{(s)}\} \geq 1-\alpha$</p>
<p>这里只需要使$P\{\xi_p &lt; X_{(r)}\}\leq \frac{\alpha}{2} \quad P\{\xi_p&gt;X_{(s)}\}\leq \frac{\alpha}{2}$</p>
<p>查二项分布表得到r,s。</p>
<p>$r=max\{c|\sum\limits_{j=0}^{c-1} C_n^jp^j[1-p]^{n-j}\leq \frac{\alpha}{2}\}$</p>
<p>$s=min\{c|\sum\limits_{j=c}^{n} C_n^jp^j[1-p]^{n-j}\leq \frac{\alpha}{2}\}$</p>
<p>而有$\xi_p$的$1-\alpha$置信区间为：$[X_{(r)},X_{(s)}]$</p>
<h3 id="总体分位数的检验"><a href="#总体分位数的检验" class="headerlink" title="总体分位数的检验"></a>总体分位数的检验</h3><p>对于给定的$p(0&lt;p&lt;1)$，设总体的分布函数为$F(x)$，且$p$分位数$\xi_p$唯一。 假设检验：</p>
<p>$H_0:\xi_p =b(b为已知数) \qquad H_1:\xi_p \neq b$</p>
<p>令$Y_i = \begin{cases}<br>   1 &amp;,&amp; X_i &gt; b\\\\<br>   0 &amp;,&amp; X_i \leq b<br>\end{cases}$</p>
<p>显著性水平$\alpha$下$H_0$的拒绝域为：</p>
<p>$W=\{(y_1,…,y_n)| \sum\limits_{i=1}^{n} y_i &lt; c_1 \quad or \quad \sum\limits_{i=1}^{n}y_i &gt; c_2\}$</p>
<p>其中：</p>
<p>$c_1=max\{d|\sum\limits_{j=0}^{d-1} C_n^jp^j[1-p]^{n-j}\leq \frac{\alpha}{2}\}$</p>
<p>$c_2=min\{d|\sum\limits_{j=d+1}^{n} C_n^jp^j[1-p]^{n-j}\leq \frac{\alpha}{2}\}$</p>
<h2 id="第五章"><a href="#第五章" class="headerlink" title="第五章"></a>第五章</h2><p><strong>回归分析研究变量间的什么关系？</strong></p>
<p>回归分析研究变量间相关关系的有无和相关关系的形式。</p>
<p><strong>阐述回归函数的作用和意义</strong></p>
<p>回归函数刻画了自变量对因变量取值的主导作用。</p>
<p><strong>回归分析的首要问题是什么？</strong></p>
<p>回归分析的首要问题是推断回归函数的具体形式。</p>
<p><strong>回归分析中随机误差项$\epsilon$的意义是什么？</strong></p>
<p>$\epsilon$为随机波动，刻画的是随机因素综合作用的效果，均值为零。</p>
<p><strong>在回归分析中，残差平方和的大小意味着什么？ 什么情况下残差平方和为零？</strong></p>
<p>残差平方和描述观测值和回归函数之间的拟合程度，当随机误差项$\epsilon$为0时，残差平方和为零。</p>
<p><strong>简述在回归分析中用最小二乘法估计回归系数的基本思想</strong><br>取使残差平方和达到最小的$\hat{b_0},\hat{b_2},…,\hat{b_n}$作为回归系数的估计。即使因变量对个个自变量的偏导数等与0，进而求出回归系数的最小二乘。</p>
<h3 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h3><p>$\hat y=\hat{b_0}+\hat{b_1}x_1+\hat{b_2}x_2+…+\hat{b_n}x_n$</p>
<p>为变量$y$关于变量$x_1,x_2,…,x_n$的经验回归方程。</p>
<p>当$p=1$时，一元线性回归模型为</p>
<p>$<br>   \begin{cases}<br>      y &amp;=&amp; a+bx+\epsilon \\\\<br>      E(\epsilon) &amp;=&amp; 0<br>   \end{cases}<br>$</p>
<p>关于$y$和$x$之间$n$次实验观测数据为$(x_i,y_i),(i=1,2,…,n)$</p>
<p>$<br>\begin{gathered}<br>Y = \begin{bmatrix}<br>   y_1\\y_2\\ … \\y_n<br>\end{bmatrix}<br>\quad<br>X = \begin{bmatrix}<br>1 &amp; x_1\\1 &amp; x_2 \\ … &amp; …\\ 1 &amp; x_n<br>\end{bmatrix}<br>\beta =<br>\begin{pmatrix}<br>a \\ b<br>\end{pmatrix}<br>\end{gathered}<br>$</p>
<p>$\hat \beta = \hat{\begin{pmatrix}<br>   a \\ b<br>\end{pmatrix}} = (X^TX)^{-1}X^TY$</p>
<p>$a = \frac{\overline{y}(\sum x_i^2)- \overline{x}(\sum x_iy_i)}{\sum(x_i-\overline{x})^2}$</p>
<p>$b = \frac{\sum(x_i-\overline{x})(y_i-\overline{y})}{\sum(x_i-\overline{x})^2}$</p>
<h3 id="线性回归的显著性分析"><a href="#线性回归的显著性分析" class="headerlink" title="线性回归的显著性分析"></a>线性回归的显著性分析</h3><p>对于$y=a+b_1x_1+b_2x_2+…+b_px_p$</p>
<p>$H_0:b_1=b_2=b_p=0 \quad H_1:b_1,b_2,…,b_p不全等与0$</p>
<p>拒绝域：$W = \{\frac{SS_{回}}{RSS}&gt;\frac{p}{n-p-1}F_{\alpha}(p,n-p-1)\}$</p>
<p>残差平方和：$RSS = (Y-X\hat{\beta})^{‘}(Y-X\hat{\beta})= \sum\limits_{i=1}^{n}(y_i-\hat{y_i})^2 \quad \hat{y_i} = a+b_1x_{i1}+b_2x_{i2}+…+b_px_{ip}$</p>
<p>回归平方和：$SS_回 = \sum\limits_{i=1}^{n}(\hat{y_i}-\overline{y})^2$</p>
<p>总偏差平方和：$TSS = \sum\limits_{i=1}^{n}(y_i-\overline{y})^2 = RSS+SS_回$</p>
<p>误差方差的无偏估计$\sigma^2 = \frac{1}{n-p-1} RSS$</p>
<p>样本相关系数：$R=+\sqrt{R^2} \qquad R^2 \overset{def}{=} \frac{SS_回}{TSS}$</p>
<p>计算，带入，若落入拒绝域，则拒绝$H_0$，有线性关系，否则，接受$H_0$,没有线性关系。</p>
<h3 id="两个总体的比较"><a href="#两个总体的比较" class="headerlink" title="两个总体的比较"></a>两个总体的比较</h3><p><strong>问题形式：</strong></p>
<p>给两个容量为$m$和$n$的样板$X,Y,(X_1,…,X_m) 和 （Y_1,…,Y_n）$</p>
<p>问：</p>
<ol>
<li>X和Y的分布是否相同/差不多（X=Y?）</li>
<li>Y是否比较X变大，X是否较Y变大？</li>
</ol>
<p>令$F(X)$和$G(X)$为$X$和$Y$的分布函数，$m,n$为$X$和$Y$的样本容量，有$P(X&gt;Y)&gt;\frac{1}{2} \leftrightarrow F(X)&lt;G(X)$</p>
<p>令$Z:(Z_1,…,Z_{n+m}) = (X_1,…,X_m,Y_1,…,Y_n)$, $(R_1,…,R_{m+n})$ 为$Z$的秩统计量。</p>
<p>$T = \sum\limits_{i=1}^{m}R_i$为X在和样本中的秩和。</p>
<h4 id="对于-X-Y"><a href="#对于-X-Y" class="headerlink" title="对于$X=Y$?"></a>对于$X=Y$?</h4><p>$H_0:F(X)=G(X) \qquad H_1:F(X)\neq G(X)$</p>
<p>拒绝域$W = \{T\leq C_1或 T\geq C_2\}$</p>
<p>要使得$P(T\leq C_1) \leq \frac{\alpha}{2}, \quad P(T \geq C_2) \leq \frac{\alpha}{2}$</p>
<p>查附表5求得$C_1$和$C_2$</p>
<p>得到$W$,判断T是否落入$W$,</p>
<p>若落入，则拒绝$H_0$,即$X \neq Y$</p>
<p>否则，接受$H_0$,即$X=Y$</p>
<h4 id="对于X-gt-Y"><a href="#对于X-gt-Y" class="headerlink" title="对于X&gt;Y?"></a>对于X&gt;Y?</h4><p>$H_0:F(X)=G(X) \qquad H_1:F(X)\leq G(X)$</p>
<p>拒绝域$W = \{ T\geq C_2\}$</p>
<p>要使得$\quad P(T \geq C_2) \leq \alpha$</p>
<p>查附表5求得$C_2$</p>
<p>得到$W$,判断T是否落入$W$,</p>
<p>若落入，则拒绝$H_0$</p>
<p>否则，接受$H_0$</p>
<h2 id="第六章"><a href="#第六章" class="headerlink" title="第六章"></a>第六章</h2><h3 id="单因素方差分析"><a href="#单因素方差分析" class="headerlink" title="单因素方差分析"></a>单因素方差分析</h3><p>基本问题：判别因素$A$对结果影响是否显著。</p>
<p>描述：因素$A$有$s$个水平，$A_1,A_2,…,A_s$,第$i$个水平有$n_i$个样本，样本的观测值为要研究的结果的值。</p>
<p>基本假定：方差齐性：个水平方差相等。</p>
<p>假设检验：$H_0:\mu_1=\mu_2=…=\mu_s \quad H_1:\mu_1,\mu_2,…,\mu_s不全相等。$</p>
<p>总平均：$\overline{x} = \frac{1}{n}\sum\limits_{i=1}^s \sum\limits_{j=1}^{n_i}x_{ij}$</p>
<p>总变差平方和：$S_T^2 = \sum\limits_{i=1}^s \sum\limits_{j=1}^{n_i}(x_{ij}-\overline{x})^2$</p>
<p>水平$A_i$时的样本平均值：$\overline{x_i} = \frac{1}{n_i}\sum\limits_{j=1}^{n_i}x_{ij}$</p>
<p>组内平方和（误差平方和）：$S_E^2 = \sum\limits_{i=1}^s \sum\limits_{j=1}^{n_i}(x_{ij}-\overline{x_i})^2$</p>
<p>组间平方和（因素平方和）：$S_A^2 = \sum\limits_{i=1}^s \sum\limits_{j=1}^{n_i}(\overline{x_i}-\overline{x})^2$</p>
<p>$S_T^2 = S_A^2+S_E^2$</p>
<p>$W = \{\frac{(n-s)S_A^2}{(s-1)S_E^2} &gt; F_\alpha(s-1,n-s)\}$</p>
<p>如果取$\alpha=0.01$时，$H_0$被拒绝，则称因素$A$对结果影响高度显著。</p>
<p>如果取$\alpha=0.01$时，$H_0$无法被拒绝，$\alpha=0.05$时，$H_0$被拒绝，则称因素$A$对结果影响显著。</p>
<p>如果取$\alpha=0.05$时，$H_0$无法被拒绝，$\alpha=0.1$时，$H_0$被拒绝，则称因素$A$对结果有一定影响。</p>
<p>如果取$\alpha=0.1$时，$H_0$仍无法被拒绝，则称因素$A$对结果无显著影响。</p>
<h4 id="方差分析表"><a href="#方差分析表" class="headerlink" title="方差分析表"></a>方差分析表</h4><p><img src="/2020/12/09/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E5%92%8C%E5%A4%9A%E5%85%83%E7%BB%9F%E8%AE%A1/table.png" width="80%"></p>
<h3 id="判别分析"><a href="#判别分析" class="headerlink" title="判别分析"></a>判别分析</h3><p>基本问题：有两个$p$维总体$G_1,G_2$,$x=(x_1,…,x_p)^{‘}$是一$p$维样品，要判断$x$是来自$G_1$还是$G_2$。（或者是多个总体$G_1,G_2,…,G_n$）</p>
<h4 id="距离判别法"><a href="#距离判别法" class="headerlink" title="距离判别法"></a>距离判别法</h4><p>基本思想：$x$属于距离$x$最近的总体$G_i$,用$\mu_i$均值向量代表$G_i$。</p>
<p>马氏距离：$D(x,G_i) = \sqrt{(x-\mu_i)^{‘} \sum_{i}^{-1}(x-\mu_i)}$</p>
<p>$\mu_i$表示总体$G_i$的均值向量，$\sum_i$表示总体$G_i$的协方差矩阵。</p>
<p>$\mu_i=\overline{x} = \frac{1}{n} \sum\limits_{k=1}^n x_{(k)}, x_{(k)}$为总体$G_i$中抽取的样本。</p>
<p>$\sum_i=S^2=\frac{1}{n-1} \sum\limits_{k=1}^n (x_{(k)}-\overline{x})(x_{(k)}-\overline{x})^{‘}$</p>
<p>取$k = \underset{i}{argmin}D(x,G_i),G_k$为$x$所属的总体。</p>
<h4 id="Bayes判别法"><a href="#Bayes判别法" class="headerlink" title="Bayes判别法"></a>Bayes判别法</h4><p>考虑先验概率和判错损失，求一判别规则是的判错损失最小。</p>
<ol>
<li>先验概率：$G_i$的出现概率为$q_i，\sum\limits_{i=1}^m q_i=1$</li>
<li>将$i$判错为$j$的损失：$C(j|i)\geq 0 \qquad C(i|i)=0$</li>
<li>将$i$判错为$j$的概率：$P(j|i,R) = \int_{R_j} f_i(x) dx$</li>
<li>$i$造成的平均损失：$r(i,R)=\sum\limits_{i=1}^m [c(j|i)P(j|i,R)]$</li>
<li>总平均损失：$g(R)=\sum\limits_{i=1}^m q_ir(i,R)$</li>
</ol>
<p>贝叶斯判别法的目标，适当的划分$R=R_1 \cup R_2\cup,…,\cup R_m$使得$g(R)$最小。</p>
<p>中间的步骤不写了， 反正最后两总体的划分如下：</p>
<p>$R_1 = \{x|\frac{f_1(x)}{f_2(x)} \geq \frac{c(1|2)q_2}{c(2|1)q_1}\}$</p>
<p>$R_2 = \{x|\frac{f_1(x)}{f_2(x)} &lt; \frac{c(1|2)q_2}{c(2|1)q_1}\}$</p>
<p>$\frac{f_1(x)}{f_2(x)} = exp\{\frac{1}{2} (x-\mu_2)^{‘}\sum^{-1}(x-\mu_2) -  \frac{1}{2} (x-\mu_1)^{‘}\sum^{-1}(x-\mu_1)\}$</p>
<h3 id="主成分分析"><a href="#主成分分析" class="headerlink" title="主成分分析"></a>主成分分析</h3><p>基本问题：$n$个样本，$p$个指标，要降维到$m$个指标。</p>
<p>即：原先$x_{(i)}=(x_{i1},x_{i2},…,x_{ip})^{‘}$</p>
<p>降维后：$y_{(i)}=(y_{i1},y_{i2},…,y_{im})^{‘} = (l’_1x_{(i)},…,l’_mx_{(i)})$</p>
<p>求法：求解系数$C$</p>
<ol>
<li><p>通过矩估计求的$\underset{p\times p}{\sum} = \frac{1}{n-1}\sum\limits_{i=1}^n(x_{(i)}-\overline{x})(x_{(i)}-\overline{x})^{‘} \quad \overline{x} = \frac{1}{n} \sum\limits_{i=1}^n x_{(i)}$</p>
</li>
<li><p>对$\sum$求特征值$\lambda_1,\lambda_2,…,\lambda_p$及其特征向量$P_1,P_2,…,P_p$,将特征向量标准化（模长为1）。</p>
</li>
<li>取前$m$大的$\lambda_i$和其对于的$P_i$</li>
</ol>
<p>$\frac{\sum\limits_{i=1}^m \lambda_i}{\sum\limits_{i=1}^p \lambda_i}$ 称为前$m$个主成分的贡献率。</p>
<ol>
<li>$y_{i1}=P_1^{‘}x_{(i)},y_{i2}=P_2^{‘}x_{(i)},…$</li>
</ol>
<p><strong>要求第一主成分的方差达到最大意味着什么？</strong></p>
<p>第一主成分方差达到最大意味着其包含的信息最多。</p>
<p><strong>为什么要求个主成分之间的协方差为零？</strong></p>
<p>因为主成分分析就是要求个主成分所包含的信息互不重叠，也就是要求它们互不关联，所以协方差为零。</p>
<h3 id="聚类分析"><a href="#聚类分析" class="headerlink" title="聚类分析"></a>聚类分析</h3><p>基本问题：将$n$个$p$维样品，分成$k$类。</p>
<p>系统聚类：每次将距离最近的两个类合并成一个类，直到只剩一个类。</p>
<p>样本间距离：</p>
<ul>
<li>绝对距离：$d(X,Y)=\sum\limits_{i=1}^P|x_i-y_i|$</li>
<li>欧式距离：$d(X,Y)=[\sum\limits_{i=1}^P(x_i-y_i)^2]^{\frac{1}{2}}$</li>
<li>切比雪夫距离：$d(X,Y)= \underset{1\leq i \leq P}{max} |x_i-y_i|$</li>
<li>闵可夫斯基距离：$d(X,Y)=[\sum\limits_{i=1}^P|x_i-y_i|^q]^{\frac{1}{q}}(q&gt;0)$</li>
<li>马氏距离：$d(X,Y)=\sqrt {(x-y)^T \sum ^{-1} (x-y)}$</li>
<li>兰氏距离：$d(X,Y)=\frac{1}{p}\sum\limits_{i=1}^p\frac{|x_i-y_i|}{x_i+y_i}$</li>
</ul>
<p>类间距离：</p>
<ul>
<li>最短距离：$D_{pq}=min(d_{ij})$</li>
<li>最长距离：$D_{pq}=max(d_{ij})$</li>
</ul>
<p>聚类画图法：</p>
<ol>
<li>写距离表</li>
<li>合并距离最小的两类</li>
<li>重写距离表</li>
<li>合并</li>
<li>…</li>
<li>…</li>
<li>直到只有一类</li>
</ol>
]]></content>
      <categories>
        <category>课程</category>
      </categories>
  </entry>
  <entry>
    <title>如何写一篇合格的NLP论文</title>
    <url>/2021/04/03/%E5%A6%82%E4%BD%95%E5%86%99%E4%B8%80%E7%AF%87%E5%90%88%E6%A0%BC%E7%9A%84NLP%E8%AE%BA%E6%96%87/</url>
    <content><![CDATA[<h1 id="如何写一篇合格的NLP论文"><a href="#如何写一篇合格的NLP论文" class="headerlink" title="如何写一篇合格的NLP论文"></a>如何写一篇合格的NLP论文</h1><h5 id="原文链接：https-zhuanlan-zhihu-com-p-58752815"><a href="#原文链接：https-zhuanlan-zhihu-com-p-58752815" class="headerlink" title="原文链接：https://zhuanlan.zhihu.com/p/58752815"></a>原文链接：<a href="https://zhuanlan.zhihu.com/p/58752815">https://zhuanlan.zhihu.com/p/58752815</a></h5><h5 id="作者：刘志远"><a href="#作者：刘志远" class="headerlink" title="作者：刘志远"></a>作者：刘志远</h5><hr>
<h2 id="论文在NLP学术研究中的意义"><a href="#论文在NLP学术研究中的意义" class="headerlink" title="论文在NLP学术研究中的意义"></a>论文在NLP学术研究中的意义</h2><p>NLP是一门重视实践和应用的领域，创新成果可以是新的算法、任务、应用、数据、发现等，务求一个“新”字，其影响力则取决于它对该领域发展的推动作用。如下图所示，学术研究是一项系统工程，包括多个环节，共同完成对“创新”的追求：问题务求挑战，模型务求创新，实现务求准确，实验务求深入。</p>
<p><img src="/2021/04/03/%E5%A6%82%E4%BD%95%E5%86%99%E4%B8%80%E7%AF%87%E5%90%88%E6%A0%BC%E7%9A%84NLP%E8%AE%BA%E6%96%87/img1.jpg" width="80%"></p>
<center style="font-size:14px;color:#C0C0C0;text-decoration:underline">学术研究是一项系统工程</center>

<p>在这个系统工程中，论文的作用则是，向学术界同行清晰准确地描述成果的创新点、技术思路、算法细节和验证结果。明白这一点，才能正确的对待论文写作：一项乏善可陈的工作，很难通过写作变得众星捧月；一项充满创新的成果，却有可能因为糟糕的写作而无法向审稿人准确传递重要价值所在，延误成果发表。</p>
<h2 id="一篇NLP论文的典型结构"><a href="#一篇NLP论文的典型结构" class="headerlink" title="一篇NLP论文的典型结构"></a>一篇NLP论文的典型结构</h2><p>NLP学术会议（甚至包括期刊）论文已经形成比较固定的结构。绝大部分论文由以下六大部分构成：摘要（Abstract）、介绍（Introduction）、相关工作（Related Work）、方法（Method）、实验（Experiment）、结论（Conclusion）。少数论文会根据创新成果形式不同而略有不同，例如提出新数据集的论文，可能会把Method部分调整为Dataset的标注与分析，但不影响论文整体构成。每个部分作用不同：</p>
<ul>
<li>摘要：用100-200词简介研究任务与挑战、解决思路与方法、实验效果与结论。</li>
<li>介绍：用1页左右篇幅，比摘要更详细地介绍研究任务、已有方法、主要挑战、解决思路、具体方法、实验结果。</li>
<li>相关工作：用0.5-1页左右篇幅介绍研究任务的相关工作，说明本文工作与已有工作的异同。</li>
<li>方法：用2-3页篇幅介绍本文提出的方法模型细节。</li>
<li>实验：用2-3页篇幅介绍验证本文方法有效性的实验设置、数据集合、实验结果、分析讨论等。</li>
<li>结论：简单总结本文主要工作，展望未来研究方向。</li>
</ul>
<p>乍看这样每篇论文显得死板，实际上这正凸显了学术论文的真正意义，不追求在形式上给读者带来意外，而将读者注意力集中在论文介绍的研究成果上。</p>
<p>如前所说，论文的作用是向学术界同行清晰准确地描述成果的创新点、技术思路、算法细节和验证结果。由于学术界的<strong>同行评审</strong>制度，贯穿全文的线索和目标就是要论证这份工作的<strong>创新价值</strong>，每个部分都要各司其职为这个目标而服务。为了实现这个目标，需要作者特别注意以下几点：</p>
<p><strong>(1) 学会换位思考</strong> 要始终站在审稿人或读者的角度审视论文，思考如何更清晰地表达。这是初学者最容易忽视的问题：作为研究成果的亲历者，论文作者掌握所有细节，如果不多加留意，写作中就会出现新概念没有被明确定义就被使用等情况，很多描述和分析缺少逻辑衔接。对作者而言，这些省去的东西并不影响他对这些文字的理解；但对并不了解这份工作的读者而言，这无疑是一场噩梦，因为他们并没有作者脑中的那套背景信息。因此，写作时要时时留神，读者读这句时能否理解，所需要的背景知识前文是否已经介绍。</p>
<p><strong>(2)注意逻辑严谨</strong> 严谨是学术论文的底色，从引用格式、公式符号到谋章造句，虽不至于美国法学期刊的Bluebook那么变态，都力求风格统一，行文严谨。引用、公式、拼写等方面都容易学，初学者更需要注意行文严谨，力求全文从章节、段落、句子等不同级别都逻辑严密，争取做到没有一句话没来由，没有一句话没呼应：</p>
<ul>
<li>章节层面，Introduciton提到已有方法面临的几个挑战，就要对应本文提出的几个创新思路，对应Method中的几个具体算法，对应Experiment中的几个实验验证。</li>
<li>段落和句子层面，段间要注意照应，是并列、递进、转折还是总分关系，需要谋划妥当，要有相应句子或副词衔接。段内各句，有总有分，中心思想句和围绕论述句分工协作。</li>
</ul>
<p>除了整体结构上的建议外，每个部分也各有定式，下面按各部分提供一些写作建议，同时用我们最近发表的一篇ACL 2018论文 [2] 作为例子。</p>
<h3 id="Abstract和Introduction怎么写"><a href="#Abstract和Introduction怎么写" class="headerlink" title="Abstract和Introduction怎么写"></a>Abstract和Introduction怎么写</h3><p>Abstract可以看做对Introduction的提要，所以我们先介绍Introduction的写法，然后再说如何写Abstract。Introduction是对整个工作的全面介绍，是决定一篇论文能否被录用的关键。一般Introduction这么写：起手介绍<strong>研究任务</strong>和意义；随后简介面向这个任务的<strong>已有方法</strong>；接着说明已有方法面临的<strong>关键挑战</strong>；针对这些挑战，本文提出什么<strong>创新思路</strong>和具体方法；最后介绍<strong>实验结果</strong>证明本文提出方法的有效性。这几个部分各挡一面，同时又有严密的内在逻辑。每个部分也各有章法，下面分别介绍对各部分的建议：</p>
<p><strong>(1) 研究任务</strong> 介绍本文的研究任务及其在该研究领域的重要价值和意义。如果是领域公认的重要任务的话，则可以不用详细论述其研究价值/意义；如果是新提出的研究任务，则需要花费比较多篇幅论证该任务的价值。如下所示论文[2]的第1段集中说明阅读理解研究任务。</p>
<p><img src="/2021/04/03/%E5%A6%82%E4%BD%95%E5%86%99%E4%B8%80%E7%AF%87%E5%90%88%E6%A0%BC%E7%9A%84NLP%E8%AE%BA%E6%96%87/img2.jpg" width="80%"></p>
<p><strong>(2) 已有方法</strong> 从研究任务递进一步，介绍这个任务的已有代表方法。如下所示论文[2]的第2段，开始介绍DS-QA。需要注意，这个已有方法需要是目前最好、最具代表性的，也是本文工作准备改进的。所谓站在巨人的肩膀上，一篇值得发表的论文需要找到那个最高的巨人。</p>
<p><img src="/2021/04/03/%E5%A6%82%E4%BD%95%E5%86%99%E4%B8%80%E7%AF%87%E5%90%88%E6%A0%BC%E7%9A%84NLP%E8%AE%BA%E6%96%87/img3.jpg" width="80%"></p>
<p><strong>(3) 面临挑战</strong> 已有方法一定仍然存在某些不足或挑战，才需要进一步研究改进。因此，需要总结已有方法面临的挑战。这是Introduction的关键部分，起着承上启下的作用。初学者特别注意，这部分涉及对已有工作的评价，务必保证精准客观。要知道，当论文投稿至NLP国际会议后，是通过同行评审决定是否录用发表，评审人一般是小同行，有很大概率是已有工作的作者。所以这部分论述一定要做到客观公正，让这些工作作者本人也能信服。</p>
<p>如下所示论文[2]的第3、4段，先介绍DS-QA的noisy labeling挑战，并且通过举例直观呈现。面对这个挑战，已有一些相关工作，还需说明他们各自有什么不足和挑战，为引出本文创新思路做好铺垫。<br><img src="/2021/04/03/%E5%A6%82%E4%BD%95%E5%86%99%E4%B8%80%E7%AF%87%E5%90%88%E6%A0%BC%E7%9A%84NLP%E8%AE%BA%E6%96%87/img4.jpg" width="80%"></p>
<p><strong>(4)创新思路</strong> 水来土掩，兵来将挡，既然已有方法有这些不足和挑战，就需要有新的创新思路和方法。这部分需要注意与上面的”挑战“部分严丝合缝，密切呼应，让读者清楚领会到这些创新思路与方法的确能够解决或缓解这些挑战问题。</p>
<p>如下所示论文[2]的第5段，就是介绍创新思路和方法。可以看到，一般”面临挑战“和”创新思路“部分还配图示，更直观地展示本文要解决的挑战问题和创新思路。例如论文[2]这张丑丑的图，比较直观地展示了创新方法包括Selector和Reader两个模块和作用。也可以随便看我们的其他论文[3]，大部分论文都会在Introduction中提供图示。</p>
<p><img src="/2021/04/03/%E5%A6%82%E4%BD%95%E5%86%99%E4%B8%80%E7%AF%87%E5%90%88%E6%A0%BC%E7%9A%84NLP%E8%AE%BA%E6%96%87/img5.jpg" width="80%"></p>
<p><strong>(5)实验结论</strong> 除了在”创新思路“部分图文两开花地说明本文创新工作外，还要通过合理的实验验证方法的有效性。一般要得到”our method achieves significant and consistent improvement as compared to other baselines“的结论，从而验证本文工作的创新性。</p>
<p><img src="/2021/04/03/%E5%A6%82%E4%BD%95%E5%86%99%E4%B8%80%E7%AF%87%E5%90%88%E6%A0%BC%E7%9A%84NLP%E8%AE%BA%E6%96%87/img6.jpg" width="80%"></p>
<p>有些论文最后还会体贴的总结本文的主要贡献，一般说”In summary, the key contributions are x-fold: (1)…(2)…(3)…“。这样做的好处是，可以帮助审稿人总结本文的创新点放在审稿意见中，节省不少工作量。但需要注意，这些创新点要简洁明了，不能是前文的简单重复，也不能overclaim。如果要说”首次“提出或发现，一般也要前置”to the best of our knowledge“。此外还有论文最后一段会介绍接下来几个Section结构，个人感觉对一篇8页论文可能并不需要。</p>
<p>对于Abstract，可以看做对Introduction的简介，最简单的做法是，以上每部分都精简为1-2句话组成Abstract皆可。如下是论文[2]的Abstract内容，可以看出与Introduction的对应关系。</p>
<p><img src="/2021/04/03/%E5%A6%82%E4%BD%95%E5%86%99%E4%B8%80%E7%AF%87%E5%90%88%E6%A0%BC%E7%9A%84NLP%E8%AE%BA%E6%96%87/img7.jpg" width="80%"></p>
<h3 id="Method怎么写"><a href="#Method怎么写" class="headerlink" title="Method怎么写"></a>Method怎么写</h3><p>这部分要详细介绍本文创新方法的具体细节，由于涉及非常艰涩的细节，要采用”总-分“结构来介绍。</p>
<p>这部分起手”总“的部分要介绍本文任务的符号定义，以及本文方法的框架组成，或者按步骤来介绍或者按模块来写，让读者对本文方法有全景式的理解。如下所示论文[2]的Methodology”总“的部分，就先介绍一些符号，然后分别介绍了Selector和Reader两个模块的主要功能。</p>
<p><img src="/2021/04/03/%E5%A6%82%E4%BD%95%E5%86%99%E4%B8%80%E7%AF%87%E5%90%88%E6%A0%BC%E7%9A%84NLP%E8%AE%BA%E6%96%87/img8.jpg" width="80%"></p>
<p>然后进入”分“的部分，则需对应”总“中的框架，分别介绍各关键模块/步骤。例如，论文[2]的Methodology”分“的部分，就包括3.1 Paragraph Selector、3.2 Paragraph Reader、3.3 Learning and Prediction。读者在”总“的部分已经对方法有全景式的了解，有的放矢，就比较容易理解每个模块的具体细节。而每个”分“的部分中，又可以进一步采用”总-分“结构进行介绍，例如3.1小节做完总体介绍后，又会按照Paragraph Encoding和Question Encoding分别介绍。为了更清晰地体现”总-分“结构，可以将各“分”的部分命名并加粗。</p>
<p><img src="/2021/04/03/%E5%A6%82%E4%BD%95%E5%86%99%E4%B8%80%E7%AF%87%E5%90%88%E6%A0%BC%E7%9A%84NLP%E8%AE%BA%E6%96%87/img9.jpg" width="80%"></p>
<p>初学者特别注意，（1）Introduction中对创新思路与方法的介绍，不要在Method中简单重复，否则会让认真通读全文的审稿人颇感厌烦。要做到前后照应，有所递进，前略后详，不妨使用“as mentioned in Section 1”来做关联。（2）Method部分往往包含大量公式，需要保证公式风格和符号使用前后统一，新符号使用均需显式解释。</p>
<h3 id="Experiment怎么写"><a href="#Experiment怎么写" class="headerlink" title="Experiment怎么写"></a>Experiment怎么写</h3><p>这部分要详细介绍与实验相关的具体细节。一般先介绍实验数据、评测标准和比较方法等基本信息。以论文[2]为例，实验部分首先介绍实验数据与评测标准（4.1 Datasets and Evaluation Metrics）、实验比较的已有代表方法（4.2 Baselines）、实验方法的参数设置（4.3 Experimental Settings）等基本信息。</p>
<p>在介绍完实验基本信息后，主要开展两种实验：</p>
<p><strong>(1)主实验</strong> 目的是证明本文方法与已有方法相比的有效性。一般需要选取业界公认的数据集合或已有工作采用的实验验证方式，提升实验的可信性。对于学术论文而言，并不需要比该任务上最好的方法相比，只要证明采用本文创新方法与不采用本文方法相比更有效即可，也就是说，实验中尽量控制其他变量，只聚焦于本文关注的挑战问题即可。当然，如果能够因为本文创新思路，得到该任务上的最好效果，会更有吸引力，但不必总是强求。</p>
<p>一般实验结果用图表展示，然后在正文进行观察分析。例如，论文[2]的主实验部分先介绍不同Selector和Reader对实验效果的影响（4.4 Effect of Different Paragraph Selectors、4.5 Effect of Different Paragraph Readers），接着介绍主实验结果和观察分析（4.6 Overall Results）。其中表格中会把最好效果加粗显示，一般应大部分位于本文提出的方法；为了更加清晰明了，观察分析结论可用（1）（2）（3）列出，其中第1条一般要得出主要结论，即本文方法要显著优于已有方法。</p>
<p><img src="/2021/04/03/%E5%A6%82%E4%BD%95%E5%86%99%E4%B8%80%E7%AF%87%E5%90%88%E6%A0%BC%E7%9A%84NLP%E8%AE%BA%E6%96%87/img10.jpg" width="80%"></p>
<center style="font-size:14px;color:#C0C0C0;text-decoration:underline">主实验结果</center>


<p><img src="/2021/04/03/%E5%A6%82%E4%BD%95%E5%86%99%E4%B8%80%E7%AF%87%E5%90%88%E6%A0%BC%E7%9A%84NLP%E8%AE%BA%E6%96%87/img11.jpg" width="80%"></p>
<center style="font-size:14px;color:#C0C0C0;text-decoration:underline">主实验分析</center>

<p><strong>(2)辅助实验</strong> 目的是展示本文创新方法的优势和特点。例如，不同超参数对本文方法的影响（Hyper-Parameter Effect），不同模块对本文方法效果的贡献（Ablation Test），不同数据划分对本文方法的影响（如Few-shot Learning相关工作比较常见），本文方法的主要错误类型（Error Analysis），本文方法能够改进效果的典型样例（Case Study）等。这些实验需要根据论文创新工作特点而有针对性的设计，一切要为体现本文的创新价值而服务。</p>
<p>例如，论文[2]的辅助实验包括4.7 Paragraph Selector Performance Analysis、4.8 Performance with different numbers of paragraphs、4.9 Potential improvement、4.10 Case study等，从各方面呈现本文提出方法的特点。</p>
<p>Experiment部分的特点是要图文并茂，注重通过多个表格和图示来呈现本文方法的优势和特点，需要注意图表风格统一。初学者特别注意，要做到仅凭图表下方的说明文字就可以理解每张图表内容，不要让读者还要到跑到正文寻找相关说明。因为，很多有经验的审稿人在看完Introduction后，会直接跳到Experiment图表中寻找对比效果。</p>
<h3 id="Related-Work怎么写"><a href="#Related-Work怎么写" class="headerlink" title="Related Work怎么写"></a>Related Work怎么写</h3><p>这部分主要是介绍本文任务和方法的相关工作，目标是通过对已有工作的梳理，凸显本文工作的创新价值。对已有工作的梳理，不应是对每个工作的简单介绍，而应当注意汇总、分类、分析，或者按照时间发展顺序，或者按照技术路线划分，例如论文[2]就是按照时间脉络介绍。</p>
<p>在对相关工作的介绍中，要注意暗合本文创新思路要解决的挑战，不应是单纯的介绍，而是夹叙夹议，时刻注意与本文工作的照应。在Related Work的最后，应该落脚到本文工作与已有工作相比，有什么新的思路，解决了什么挑战问题。</p>
<p>初学者特别注意，Introduction和Related Work部分是特别需要导师或其他有经验学者帮助把关的。一是，不能遗漏重要相关工作，这点需要论文作者对相关领域工作保持跟踪；二是，与Introduction要求类似，对已有工作的评述务必精准客观。</p>
<p>Related Work一般放在Introduction之后，或者Conclusion之前，这一般取决于论文工作的特点。对于那些与已有工作联系紧密、创新精微的工作，一般建议放在Introduction之后，方便读者全面了解本文工作与已有工作的关系，然后开始在Method介绍本文方法。而对于有些框架性创新工作，如果主要是对已有方法的组合，一般建议Related Work放在Method、Experiment之后即可。这点并无成法，完全根据行文方便来定。</p>
<p><img src="/2021/04/03/%E5%A6%82%E4%BD%95%E5%86%99%E4%B8%80%E7%AF%87%E5%90%88%E6%A0%BC%E7%9A%84NLP%E8%AE%BA%E6%96%87/img12.jpg" width="80%"></p>
<h3 id="Conclusion怎么写"><a href="#Conclusion怎么写" class="headerlink" title="Conclusion怎么写"></a>Conclusion怎么写</h3><p>在论文最后会有总结展望，一般用一段来再次总结和强调本文的创新思路和实验结果，然后说明未来建议的研究方向和开放问题。这部分相对来讲比较固定。稍微留意的是，在准备论文最后阶段，如果发现论文有哪些应当做还没来得及做的，可以写作本文的未来工作。至少可以向审稿人表明你也想到这个问题了，赢得一点同情分。</p>
<p><img src="/2021/04/03/%E5%A6%82%E4%BD%95%E5%86%99%E4%B8%80%E7%AF%87%E5%90%88%E6%A0%BC%E7%9A%84NLP%E8%AE%BA%E6%96%87/img13.jpg" width="80%"></p>
<h2 id="其他建议"><a href="#其他建议" class="headerlink" title="其他建议"></a>其他建议</h2><p>要想写出一篇合格的NLP论文，首先是<strong>态度问题</strong>，只有态度重视，才有可能不厌其烦地反复修改，才会“不择手段”地寻找各种办法来尽力改进论文（找学长找外教借助Grammarly工具等）。其次是<strong>动手问题</strong>，只有写下来，才可能不断改，只要改就能不断进步。最后是<strong>经验问题</strong>，要写得精彩可能需要天赋，而要写得合格，只要坚持写，不断根据评阅人和其他人的意见进行思考和修改，就可以进步。总之，坚持就是胜利。</p>
<p>实际上，我觉得论文写作，是对思维模式的训练。也许未来你并不会从事学术研究，但通过论文写作锻炼的凝练工作创新价值的能力、清晰传递复杂信息的表达能力，对未来工作中无论是工作沟通、成果展示等，都有重要帮助。所以还希望大家都能重视这个科研道路上难得的锻炼机会。加油！</p>
]]></content>
      <categories>
        <category>nlp</category>
      </categories>
      <tags>
        <tag>论文</tag>
      </tags>
  </entry>
  <entry>
    <title>正则表达式</title>
    <url>/2021/04/16/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/</url>
    <content><![CDATA[<h2 id="原文链接：https-gitee-com-thinkyoung-learn-regex"><a href="#原文链接：https-gitee-com-thinkyoung-learn-regex" class="headerlink" title="原文链接：https://gitee.com/thinkyoung/learn_regex"></a>原文链接：<a href="https://gitee.com/thinkyoung/learn_regex">https://gitee.com/thinkyoung/learn_regex</a></h2><h2 id="网址推荐：https-regex101-com"><a href="#网址推荐：https-regex101-com" class="headerlink" title="网址推荐：https://regex101.com/"></a>网址推荐：<a href="https://regex101.com/">https://regex101.com/</a></h2><h1 id="正则表达式学习手册中文版"><a href="#正则表达式学习手册中文版" class="headerlink" title="正则表达式学习手册中文版"></a>正则表达式学习手册中文版</h1><h2 id="什么是正则表达式"><a href="#什么是正则表达式" class="headerlink" title="什么是正则表达式?"></a>什么是正则表达式?</h2><blockquote>
<p>正则表达式是一组由字母和符号组成的特殊文本, 它可以用来从文本中找出满足你想要的格式的句子.</p>
</blockquote>
<p>一个正则表达式是在一个主体字符串中从左到右匹配字符串时的一种样式.<br>例如”Regular expression”是一个完整的句子, 但我们常使用缩写的术语”regex”或”regexp”.<br>正则表达式可以用来替换文本中的字符串,验证形式,提取字符串等等.</p>
<p>想象你正在写一个应用, 然后你想设定一个用户命名的规则, 让用户名包含字符,数字,下划线和连字符,以及限制字符的个数,好让名字看起来没那么丑.<br>我们使用以下正则表达式来验证一个用户名:</p>
<p><img src="/2021/04/16/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/regexp-cn.png" width="80%"></p>
<center style="font-size:14px;color:#C0C0C0;text-decoration:underline">正则表达式</center>

<p>以上的正则表达式可以接受 <code>john_doe</code>, <code>jo-hn_doe</code>, <code>john12_as</code>.<br>但不匹配<code>Jo</code>, 因为它包含了大写的字母而且太短了.</p>
<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h1><ul>
<li><a href="#1-基本匹配">1. 基本匹配</a></li>
<li><a href="#2-元字符">2. 元字符</a><ul>
<li><a href="#21-点运算符-">2.1 点运算符 .</a></li>
<li><a href="#22-字符集">2.2 字符集</a><ul>
<li><a href="#221-否定字符集">2.2.1 否定字符集</a><ul>
<li><a href="#23-重复次数">2.3 重复次数</a></li>
</ul>
</li>
<li><a href="#231--号">2.3.1 * 号</a></li>
<li><a href="#232--号">2.3.2   号</a></li>
<li><a href="#233--号">2.3.3 ? 号</a><ul>
<li><a href="#24--号">2.4 {} 号</a></li>
<li><a href="#25--特征标群">2.5 (…) 特征标群</a></li>
<li><a href="#26--或运算符">2.6 | 或运算符</a></li>
<li><a href="#27-转码特殊字符">2.7 转码特殊字符</a></li>
<li><a href="#28-锚点">2.8 锚点</a></li>
</ul>
</li>
<li><a href="#281--号">2.8.1 ^ 号</a></li>
<li><a href="#282--号">2.8.2 $ 号</a><ul>
<li><a href="#正则表达式学习手册中文版">正则表达式学习手册中文版</a></li>
<li><a href="#什么是正则表达式">什么是正则表达式?</a></li>
<li><a href="#目录">目录</a></li>
<li><a href="#1-基本匹配">1. 基本匹配</a></li>
<li><a href="#2-元字符">2. 元字符</a></li>
<li><a href="#21-点运算符-">2.1 点运算符 <code>.</code></a></li>
<li><a href="#22-字符集">2.2 字符集</a></li>
<li><a href="#221-否定字符集">2.2.1 否定字符集</a></li>
<li><a href="#23-重复次数">2.3 重复次数</a></li>
<li><a href="#231--号">2.3.1 <code>*</code> 号</a></li>
<li><a href="#232--号">2.3.2 <code>+</code> 号</a></li>
<li><a href="#233--号">2.3.3 <code>?</code> 号</a></li>
<li><a href="#24--号">2.4 <code>&#123;&#125;</code> 号</a></li>
<li><a href="#25--特征标群">2.5 <code>(...)</code> 特征标群</a></li>
<li><a href="#26--或运算符">2.6 <code>|</code> 或运算符</a></li>
<li><a href="#27-转码特殊字符">2.7 转码特殊字符</a></li>
<li><a href="#28-锚点">2.8 锚点</a></li>
<li><a href="#281--号">2.8.1 <code>^</code> 号</a></li>
<li><a href="#282--号">2.8.2 <code>$</code> 号</a></li>
<li><a href="#3-简写字符集">3. 简写字符集</a></li>
<li><a href="#4-前后关联约束前后预查">4. 前后关联约束(前后预查)</a></li>
<li><a href="#41--前置约束存在">4.1 <code>?=...</code> 前置约束(存在)</a></li>
<li><a href="#42--前置约束-排除">4.2 <code>?!...</code> 前置约束-排除</a></li>
<li><a href="#43---后置约束-存在">4.3 <code>?&lt;= ...</code> 后置约束-存在</a></li>
<li><a href="#44--后置约束-排除">4.4 <code>?&lt;!...</code> 后置约束-排除</a></li>
<li><a href="#5-标志">5. 标志</a></li>
<li><a href="#51-忽略大小写-case-insensitive">5.1 忽略大小写 (Case Insensitive)</a></li>
<li><a href="#52-全局搜索-global-search">5.2 全局搜索 (Global search)</a></li>
<li><a href="#53-多行修饰符-multiline">5.3 多行修饰符 (Multiline)</a></li>
<li><a href="#额外补充">额外补充</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="1-基本匹配"><a href="#1-基本匹配" class="headerlink" title="1. 基本匹配"></a>1. 基本匹配</h2><p>正则表达式其实就是在执行搜索时的格式, 它由一些字母和数字组合而成.<br>例如: 一个正则表达式 <code>the</code>, 它表示一个规则: 由字母<code>t</code>开始,接着是<code>h</code>,再接着是<code>e</code>.</p>
<pre>
"the" => The fat cat sat on <a href="#learn-regex"><strong>the</strong></a> mat. 
</pre>


<p>正则表达式<code>123</code>匹配字符串<code>123</code>. 它逐个字符的与输入的正则表达式做比较.</p>
<p>正则表达式是大小写敏感的, 所以<code>The</code>不会匹配<code>the</code>.</p>
<pre>
"The" => <a href="#learn-regex"><strong>The</strong></a> fat cat sat on the mat.
</pre>

<h2 id="2-元字符"><a href="#2-元字符" class="headerlink" title="2. 元字符"></a>2. 元字符</h2><p>正则表达式主要依赖于元字符.<br>元字符不代表他们本身的字面意思, 他们都有特殊的含义. 一些元字符写在方括号中的时候有一些特殊的意思. 以下是一些元字符的介绍:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">元字符</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">.</td>
<td>句号匹配任意单个字符除了换行符.</td>
</tr>
<tr>
<td style="text-align:center">[ ]</td>
<td>字符种类. 匹配方括号内的任意字符.</td>
</tr>
<tr>
<td style="text-align:center"><sup><a href="#fn_ " id="reffn_ "> </a></sup></td>
<td>否定的字符种类. 匹配除了方括号里的任意字符</td>
</tr>
<tr>
<td style="text-align:center">*</td>
<td>匹配&gt;=0个重复的在*号之前的字符.</td>
</tr>
<tr>
<td style="text-align:center">+</td>
<td>匹配&gt;1个重复的+号前的字符.</td>
</tr>
<tr>
<td style="text-align:center">?</td>
<td>标记?之前的字符为可选.</td>
</tr>
<tr>
<td style="text-align:center">{n,m}</td>
<td>匹配num个中括号之前的字符 (n &lt;= num &lt;= m).</td>
</tr>
<tr>
<td style="text-align:center">(xyz)</td>
<td>字符集, 匹配与 xyz 完全相等的字符串.</td>
</tr>
<tr>
<td style="text-align:center">&#124;</td>
<td>或运算符,匹配符号前或后的字符.</td>
</tr>
<tr>
<td style="text-align:center">&#92;</td>
<td>转义字符,用于匹配一些保留的字符 <code>[ ] ( ) &#123; &#125; . * + ? ^ $ \ &#124;</code></td>
</tr>
<tr>
<td style="text-align:center">^</td>
<td>从开始行开始匹配.</td>
</tr>
<tr>
<td style="text-align:center">$</td>
<td>从末端开始匹配.</td>
</tr>
</tbody>
</table>
</div>
<h2 id="2-1-点运算符"><a href="#2-1-点运算符" class="headerlink" title="2.1 点运算符 ."></a>2.1 点运算符 <code>.</code></h2><p><code>.</code>是元字符中最简单的例子.<br><code>.</code>匹配任意单个字符, 但不匹配换行符.<br>例如, 表达式<code>.ar</code>匹配一个任意字符后面跟着是<code>a</code>和<code>r</code>的字符串.</p>
<pre>
".ar" => The <a href="#learn-regex"><strong>car</strong></a> <a href="#learn-regex"><strong>par</strong></a>ked in the <a href="#learn-regex"><strong>gar</strong></a>age.
</pre>


<h2 id="2-2-字符集"><a href="#2-2-字符集" class="headerlink" title="2.2 字符集"></a>2.2 字符集</h2><p>字符集也叫做字符类.<br>方括号用来指定一个字符集.<br>在方括号中使用连字符来指定字符集的范围.<br>在方括号中的字符集不关心顺序.<br>例如, 表达式<code>[Tt]he</code> 匹配 <code>the</code> 和 <code>The</code>.</p>
<pre>
"[Tt]he" => <a href="#learn-regex"><strong>The</strong></a> car parked in <a href="#learn-regex"><strong>the</strong></a> garage.
</pre>


<p>方括号的句号就表示句号.<br>表达式 <code>ar[.]</code> 匹配 <code>ar.</code>字符串</p>
<pre>
"ar[.]" => A garage is a good place to park a c<a href="#learn-regex"><strong>ar.</strong></a>
</pre>


<h3 id="2-2-1-否定字符集"><a href="#2-2-1-否定字符集" class="headerlink" title="2.2.1 否定字符集"></a>2.2.1 否定字符集</h3><p>一般来说 <code>^</code> 表示一个字符串的开头, 但它用在一个方括号的开头的时候, 它表示这个字符集是否定的.<br>例如, 表达式<code>[^c]ar</code> 匹配一个后面跟着<code>ar</code>的除了<code>c</code>的任意字符.</p>
<pre>
"[^c]ar" => The car <a href="#learn-regex"><strong>par</strong></a>ked in the <a href="#learn-regex"><strong>gar</strong></a>age.
</pre>


<h2 id="2-3-重复次数"><a href="#2-3-重复次数" class="headerlink" title="2.3 重复次数"></a>2.3 重复次数</h2><p>后面跟着元字符 <code>+</code>, <code>*</code> or <code>?</code> 的, 用来指定匹配子模式的次数.<br>这些元字符在不同的情况下有着不同的意思.</p>
<h3 id="2-3-1-号"><a href="#2-3-1-号" class="headerlink" title="2.3.1 * 号"></a>2.3.1 <code>*</code> 号</h3><p><code>*</code>号匹配 在<code>*</code>之前的字符出现<code>大于等于0</code>次.<br>例如, 表达式 <code>a*</code> 匹配以0或更多个a开头的字符, 因为有0个这个条件, 其实也就匹配了所有的字符. 表达式<code>[a-z]*</code> 匹配一个行中所有以小写字母开头的字符串.</p>
<pre>
"[a-z]*" => T<a href="#learn-regex"><strong>he</strong></a> <a href="#learn-regex"><strong>car</strong></a> <a href="#learn-regex"><strong>parked</strong></a> <a href="#learn-regex"><strong>in</strong></a> <a href="#learn-regex"><strong>the</strong></a> <a href="#learn-regex"><strong>garage</strong></a> #21.
</pre>


<p><code>*</code>字符和<code>.</code>字符搭配可以匹配所有的字符<code>.*</code>.<br><code>*</code>和表示匹配空格的符号<code>\s</code>连起来用, 如表达式<code>\s*cat\s*</code>匹配0或更多个空格开头和0或更多个空格结尾的cat字符串.</p>
<pre>
"\s*cat\s*" => The fat<a href="#learn-regex"><strong> cat </strong></a>sat on the <a href="#learn-regex">con<strong>cat</strong>enation</a>.
</pre>


<h3 id="2-3-2-号"><a href="#2-3-2-号" class="headerlink" title="2.3.2 + 号"></a>2.3.2 <code>+</code> 号</h3><p><code>+</code>号匹配<code>+</code>号之前的字符出现 &gt;=1 次个字符.<br>例如表达式<code>c.+t</code> 匹配以首字母<code>c</code>开头以<code>t</code>结尾,中间跟着任意个字符的字符串.</p>
<pre>
"c.+t" => The fat <a href="#learn-regex"><strong>cat sat on the mat</strong></a>.
</pre>


<h3 id="2-3-3-号"><a href="#2-3-3-号" class="headerlink" title="2.3.3 ? 号"></a>2.3.3 <code>?</code> 号</h3><p>在正则表达式中元字符 <code>?</code> 标记在符号前面的字符为可选, 即出现 0 或 1 次.<br>例如, 表达式 <code>[T]?he</code> 匹配字符串 <code>he</code> 和 <code>The</code>.</p>
<pre>
"[T]he" => <a href="#learn-regex"><strong>The</strong></a> car is parked in the garage.
</pre>


<pre>
"[T]?he" => <a href="#learn-regex"><strong>The</strong></a> car is parked in t<a href="#learn-regex"><strong>he</strong></a> garage.
</pre>

<h2 id="2-4-号"><a href="#2-4-号" class="headerlink" title="2.4 {} 号"></a>2.4 <code>&#123;&#125;</code> 号</h2><p>在正则表达式中 <code>&#123;&#125;</code> 是一个量词, 常用来一个或一组字符可以重复出现的次数.<br>例如,  表达式 <code>[0-9]&#123;2,3&#125;</code> 匹配 2~3 位 0~9 的数字.</p>
<pre>
"[0-9]{2,3}" => The number was 9.<a href="#learn-regex"><strong>999</strong></a>7 but we rounded it off to <a href="#learn-regex"><strong>10</strong></a>.0.
</pre>

<p>我们可以省略第二个参数.<br>例如, <code>[0-9]&#123;2,&#125;</code> 匹配至少两位 0~9 的数字.</p>
<p>如果逗号也省略掉则表示重复固定的次数.<br>例如, <code>[0-9]&#123;3&#125;</code> 匹配3位数字</p>
<pre>
"[0-9]{2,}" => The number was 9.<a href="#learn-regex"><strong>9997</strong></a> but we rounded it off to <a href="#learn-regex"><strong>10</strong></a>.0.
</pre>

<pre>
"[0-9]{3}" => The number was 9.<a href="#learn-regex"><strong>999</strong></a>7 but we rounded it off to 10.0.
</pre>

<h2 id="2-5-特征标群"><a href="#2-5-特征标群" class="headerlink" title="2.5 (...) 特征标群"></a>2.5 <code>(...)</code> 特征标群</h2><p>特征标群是一组写在 <code>(...)</code> 中的子模式. 例如之前说的 <code>&#123;&#125;</code> 是用来表示前面一个字符出现指定次数. 但如果在 <code>&#123;&#125;</code> 前加入特征标群则表示整个标群内的字符重复 N 次. 例如, 表达式 <code>(ab)*</code> 匹配连续出现 0 或更多个 <code>ab</code>.</p>
<p>我们还可以在 <code>()</code> 中用或字符 <code>|</code> 表示或. 例如, <code>(c|g|p)ar</code> 匹配 <code>car</code> 或 <code>gar</code> 或 <code>par</code>.</p>
<pre>
"(c|g|p)ar" => The <a href="#learn-regex"><strong>car</strong></a> is <a href="#learn-regex"><strong>par</strong></a>ked in the <a href="#learn-regex"><strong>gar</strong></a>age.
</pre>

<h2 id="2-6-或运算符"><a href="#2-6-或运算符" class="headerlink" title="2.6 | 或运算符"></a>2.6 <code>|</code> 或运算符</h2><p>或运算符就表示或, 用作判断条件.</p>
<p>例如 <code>(T|t)he|car</code> 匹配 <code>(T|t)he</code> 或 <code>car</code>.</p>
<pre>
"(T|t)he|car" => <a href="#learn-regex"><strong>The</strong></a> <a href="#learn-regex"><strong>car</strong></a> is parked in <a href="#learn-regex"><strong>the</strong></a> garage.
</pre>

<h2 id="2-7-转码特殊字符"><a href="#2-7-转码特殊字符" class="headerlink" title="2.7 转码特殊字符"></a>2.7 转码特殊字符</h2><p>反斜线 <code>\</code> 在表达式中用于转码紧跟其后的字符. 用于指定 <code>&#123; &#125; [ ] / \ + * . $ ^ | ?</code> 这些特殊字符. 如果想要匹配这些特殊字符则要在其前面加上反斜线 <code>\</code>.</p>
<p>例如 <code>.</code> 是用来匹配除换行符外的所有字符的. 如果想要匹配句子中的 <code>.</code> 则要写成 <code>\.</code>.</p>
<pre>
"(f|c|m)at\.?" => The <a href="#learn-regex"><strong>fat</strong></a> <a href="#learn-regex"><strong>cat</strong></a> sat on the <a href="#learn-regex"><strong>mat.</strong></a>
</pre>

<h2 id="2-8-锚点"><a href="#2-8-锚点" class="headerlink" title="2.8 锚点"></a>2.8 锚点</h2><p>在正则表达式中, 想要匹配指定开头或结尾的字符串就要使用到锚点. <code>^</code> 指定开头, <code>$</code> 指定结尾.</p>
<h3 id="2-8-1-号"><a href="#2-8-1-号" class="headerlink" title="2.8.1 ^ 号"></a>2.8.1 <code>^</code> 号</h3><p><code>^</code> 用来检查匹配的字符串是否在所匹配字符串的开头.</p>
<p>例如, 在 <code>abc</code> 中使用表达式 <code>^a</code> 会得到结果 <code>a</code>. 但如果使用 <code>^b</code> 将匹配不到任何结果. 应为在字符串 <code>abc</code> 中并不是以 <code>b</code> 开头.</p>
<p>例如, <code>^(T|t)he</code> 匹配以 <code>The</code> 或 <code>the</code> 开头的字符串.</p>
<pre>
"(T|t)he" => <a href="#learn-regex"><strong>The</strong></a> car is parked in <a href="#learn-regex"><strong>the</strong></a> garage.
</pre>

<pre>
"^(T|t)he" => <a href="#learn-regex"><strong>The</strong></a> car is parked in the garage.
</pre>


<h3 id="2-8-2-号"><a href="#2-8-2-号" class="headerlink" title="2.8.2 $ 号"></a>2.8.2 <code>$</code> 号</h3><p>同理于 <code>^</code> 号, <code>$</code> 号用来匹配字符是否是最后一个.</p>
<p>例如, <code>(at\.)$</code> 匹配以 <code>at.</code> 结尾的字符串.</p>
<pre>
"(at\.)" => The fat c<a href="#learn-regex"><strong>at.</strong></a> s<a href="#learn-regex"><strong>at.</strong></a> on the m<a href="#learn-regex"><strong>at.</strong></a>
</pre>

<pre>
"(at\.)$" => The fat cat. sat. on the m<a href="#learn-regex"><strong>at.</strong></a>
</pre>


<h2 id="3-简写字符集"><a href="#3-简写字符集" class="headerlink" title="3. 简写字符集"></a>3. 简写字符集</h2><p>正则表达式提供一些常用的字符集简写. 如下:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">简写</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">.</td>
<td>除换行符外的所有字符</td>
</tr>
<tr>
<td style="text-align:center">\w</td>
<td>匹配所有字母数字, 等同于 <code>[a-zA-Z0-9_]</code></td>
</tr>
<tr>
<td style="text-align:center">\W</td>
<td>匹配所有非字母数字, 即符号, 等同于: <code>[^\w]</code></td>
</tr>
<tr>
<td style="text-align:center">\d</td>
<td>匹配数字: <code>[0-9]</code></td>
</tr>
<tr>
<td style="text-align:center">\D</td>
<td>匹配非数字: <code>[^\d]</code></td>
</tr>
<tr>
<td style="text-align:center">\s</td>
<td>匹配所有空格字符, 等同于: <code>[\t\n\f\r\p&#123;Z&#125;]</code></td>
</tr>
<tr>
<td style="text-align:center">\S</td>
<td>匹配所有非空格字符: <code>[^\s]</code></td>
</tr>
</tbody>
</table>
</div>
<h2 id="4-前后关联约束-前后预查"><a href="#4-前后关联约束-前后预查" class="headerlink" title="4. 前后关联约束(前后预查)"></a>4. 前后关联约束(前后预查)</h2><p>前置约束和后置约束都属于<strong>非捕获簇</strong>(用于匹配不在匹配列表中的格式).<br>前置约束用于判断所匹配的格式是否在另一个确定的格式之后.</p>
<p>例如, 我们想要获得所有跟在 <code>$</code> 符号后的数字, 我们可以使用正向向后约束 <code>(?&lt;=\$)[0-9\.]*</code>.<br>这个表达式匹配 <code>$</code> 开头, 之后跟着 <code>0,1,2,3,4,5,6,7,8,9,.</code> 这些字符可以出现大于等于 0 次.</p>
<p>前后关联约束如下:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">符号</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">?=</td>
<td>前置约束-存在</td>
</tr>
<tr>
<td style="text-align:center">?!</td>
<td>前置约束-排除</td>
</tr>
<tr>
<td style="text-align:center">?&lt;=</td>
<td>后置约束-存在</td>
</tr>
<tr>
<td style="text-align:center">?&lt;!</td>
<td>后置约束-排除</td>
</tr>
</tbody>
</table>
</div>
<h3 id="4-1-前置约束-存在"><a href="#4-1-前置约束-存在" class="headerlink" title="4.1 ?=... 前置约束(存在)"></a>4.1 <code>?=...</code> 前置约束(存在)</h3><p><code>?=...</code> 前置约束(存在), 表示第一部分表达式必须跟在 <code>?=...</code>定义的表达式之后.</p>
<p>返回结果只瞒住第一部分表达式.<br>定义一个前置约束(存在)要使用 <code>()</code>. 在括号内部使用一个问号和等号: <code>(?=...)</code>. </p>
<p>前置约束的内容写在括号中的等号后面.<br>例如, 表达式 <code>[T|t]he(?=\sfat)</code> 匹配 <code>The</code> 和 <code>the</code>, 在括号中我们又定义了前置约束(存在) <code>(?=\sfat)</code> ,即 <code>The</code> 和 <code>the</code> 后面紧跟着 <code>(空格)fat</code>.</p>
<pre>
"[T|t]he(?=\sfat)" => <a href="#learn-regex"><strong>The</strong></a> fat cat sat on the mat.
</pre>

<h3 id="4-2-前置约束-排除"><a href="#4-2-前置约束-排除" class="headerlink" title="4.2 ?!... 前置约束-排除"></a>4.2 <code>?!...</code> 前置约束-排除</h3><p>前置约束-排除 <code>?!</code> 用于筛选所有匹配结果, 筛选条件为 其后不跟随着定义的格式<br><code>前置约束-排除</code>  定义和 <code>前置约束(存在)</code> 一样, 区别就是 <code>=</code> 替换成 <code>!</code> 也就是 <code>(?!...)</code>. </p>
<p>表达式 <code>[T|t]he(?!\sfat)</code> 匹配 <code>The</code> 和 <code>the</code>, 且其后不跟着 <code>(空格)fat</code>.</p>
<pre>
"[T|t]he(?!\sfat)" => The fat cat sat on <a href="#learn-regex"><strong>the</strong></a> mat.
</pre>

<h3 id="4-3-lt-后置约束-存在"><a href="#4-3-lt-后置约束-存在" class="headerlink" title="4.3 ?&lt;= ... 后置约束-存在"></a>4.3 <code>?&lt;= ...</code> 后置约束-存在</h3><p>后置约束-存在 记作<code>(?&lt;=...)</code> 用于筛选所有匹配结果, 筛选条件为 其前跟随着定义的格式.<br>例如, 表达式 <code>(?&lt;=[T|t]he\s)(fat|mat)</code> 匹配 <code>fat</code> 和 <code>mat</code>, 且其前跟着 <code>The</code> 或 <code>the</code>.</p>
<pre>
"(?<=[T|t]he\s)(fat|mat)" => The <a href="#learn-regex"><strong>fat</strong></a> cat sat on the <a href="#learn-regex"><strong>mat</strong></a>.
</=[T|t]he\s)(fat|mat)"></pre>


<h3 id="4-4-lt-后置约束-排除"><a href="#4-4-lt-后置约束-排除" class="headerlink" title="4.4 ?&lt;!... 后置约束-排除"></a>4.4 <code>?&lt;!...</code> 后置约束-排除</h3><p>后置约束-排除 记作 <code>(?&lt;!...)</code> 用于筛选所有匹配结果, 筛选条件为 其前不跟着定义的格式.<br>例如, 表达式 <code>(?&lt;!(T|t)he\s)(cat)</code> 匹配 <code>cat</code>, 且其前不跟着 <code>The</code> 或 <code>the</code>.</p>
<pre>
"(?&lt;![T|t]he\s)(cat)" => The cat sat on <a href="#learn-regex"><strong>cat</strong></a>.
</pre>

<h2 id="5-标志"><a href="#5-标志" class="headerlink" title="5. 标志"></a>5. 标志</h2><p>标志也叫修饰语, 因为它可以用来修改表达式的搜索结果.<br>这些标志可以任意的组合使用, 它也是整个正则表达式的一部分.</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">标志</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">i</td>
<td>忽略大小写.</td>
</tr>
<tr>
<td style="text-align:center">g</td>
<td>全局搜索.</td>
</tr>
<tr>
<td style="text-align:center">m</td>
<td>多行的: 锚点元字符 <code>^</code> <code>$</code> 工作范围在每行的起始.</td>
</tr>
</tbody>
</table>
</div>
<h3 id="5-1-忽略大小写-Case-Insensitive"><a href="#5-1-忽略大小写-Case-Insensitive" class="headerlink" title="5.1 忽略大小写 (Case Insensitive)"></a>5.1 忽略大小写 (Case Insensitive)</h3><p>修饰语 <code>i</code> 用于忽略大小写.<br>例如, 表达式 <code>/The/gi</code> 表示在全局搜索 <code>The</code>, 在后面的 <code>i</code> 将其条件修改为忽略大小写, 则变成搜索 <code>the</code> 和 <code>The</code>, <code>g</code> 表示全局搜索.</p>
<pre>
"The" => <a href="#learn-regex"><strong>The</strong></a> fat cat sat on the mat.
</pre>


<pre>
"/The/gi" => <a href="#learn-regex"><strong>The</strong></a> fat cat sat on <a href="#learn-regex"><strong>the</strong></a> mat.
</pre>


<h3 id="5-2-全局搜索-Global-search"><a href="#5-2-全局搜索-Global-search" class="headerlink" title="5.2 全局搜索 (Global search)"></a>5.2 全局搜索 (Global search)</h3><p>修饰符 <code>g</code> 常用于执行一个全局搜索匹配, 即(不仅仅返回第一个匹配的, 而是返回全部).<br>例如, 表达式 <code>/.(at)/g</code> 表示搜索 任意字符(除了换行) + <code>at</code>, 并返回全部结果.</p>
<pre>
"/.(at)/" => The <a href="#learn-regex"><strong>fat</strong></a> cat sat on the mat.
</pre>


<pre>
"/.(at)/g" => The <a href="#learn-regex"><strong>fat</strong></a> <a href="#learn-regex"><strong>cat</strong></a> <a href="#learn-regex"><strong>sat</strong></a> on the <a href="#learn-regex"><strong>mat</strong></a>.
</pre>



<h3 id="5-3-多行修饰符-Multiline"><a href="#5-3-多行修饰符-Multiline" class="headerlink" title="5.3 多行修饰符 (Multiline)"></a>5.3 多行修饰符 (Multiline)</h3><p>多行修饰符 <code>m</code> 常用于执行一个多行匹配.</p>
<p>像之前介绍的 <code>(^,$)</code> 用于检查格式是否是在待检测字符串的开头或结尾. 但我们如果想要它在每行的开头和结尾生效, 我们需要用到多行修饰符 <code>m</code>.</p>
<p>例如, 表达式 <code>/at(.)?$/gm</code> 表示在待检测字符串每行的末尾搜索 <code>at</code>后跟一个或多个 <code>.</code> 的字符串, 并返回全部结果.</p>
<pre>
"/.at(.)?$/" => The fat
                cat sat
                on the <a href="#learn-regex"><strong>mat.</strong></a>
</pre>



<pre>
"/.at(.)?$/gm" => The <a href="#learn-regex"><strong>fat</strong></a>
                  cat <a href="#learn-regex"><strong>sat</strong></a>
                  on the <a href="#learn-regex"><strong>mat.</strong></a>
</pre>



<h2 id="额外补充"><a href="#额外补充" class="headerlink" title="额外补充"></a>额外补充</h2><ul>
<li><em>正整数</em>: <code>^\d+$</code></li>
<li><em>负整数</em>: <code>^-\d+$</code></li>
<li><em>手机国家号</em>: <code>^+?[\d\s]&#123;3,&#125;$</code></li>
<li><em>手机号</em>: <code>^+?[\d\s]+(?[\d\s]&#123;10,&#125;$</code></li>
<li><em>整数</em>: <code>^-?\d+$</code></li>
<li><em>用户名</em>: <code>^[\w\d_.]&#123;4,16&#125;$</code></li>
<li><em>数字和英文字母</em>: <code>^[a-zA-Z0-9]*$</code></li>
<li><em>数字和应为字母和空格</em>: <code>^[a-zA-Z0-9 ]*$</code></li>
<li><em>密码</em>: <code>^(?=^.&#123;6,&#125;$)((?=.*[A-Za-z0-9])(?=.*[A-Z])(?=.*[a-z]))^.*$</code></li>
<li><em>邮箱</em>: <code>^([a-zA-Z0-9._%-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]&#123;2,4&#125;)*$</code></li>
<li><em>IP4 地址</em>: <code>^((?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.)&#123;3&#125;(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?))*$</code></li>
<li><em>纯小写字母</em>: <code>^([a-z])*$</code></li>
<li><em>纯大写字母</em>: <code>^([A-Z])*$</code></li>
<li><em>URL</em>: <code>^(((http|https|ftp):\/\/)?([[a-zA-Z0-9]\-\.])+(\.)([[a-zA-Z0-9]])&#123;2,4&#125;([[a-zA-Z0-9]\/+=%&amp;_\.~?\-]*))*$</code></li>
<li><em>VISA 信用卡号</em>: <code>^(4[0-9]&#123;12&#125;(?:[0-9]&#123;3&#125;)?)*$</code></li>
<li><em>日期 (MM/DD/YYYY)</em>: <code>^(0?[1-9]|1[012])[- /.](0?[1-9]|[12][0-9]|3[01])[- /.](19|20)?[0-9]&#123;2&#125;$</code></li>
<li><em>日期 (YYYY/MM/DD)</em>: <code>^(19|20)?[0-9]&#123;2&#125;[- /.](0?[1-9]|1[012])[- /.](0?[1-9]|[12][0-9]|3[01])$</code></li>
<li><em>MasterCard 信用卡号</em>: <code>^(5[1-5][0-9]&#123;14&#125;)*$</code></li>
</ul>
]]></content>
  </entry>
  <entry>
    <title>vim基本操作</title>
    <url>/2021/04/17/vim/</url>
    <content><![CDATA[<h1 id="Vim基本操作学习指南"><a href="#Vim基本操作学习指南" class="headerlink" title="Vim基本操作学习指南"></a>Vim基本操作学习指南</h1><p>===============================================</p>
<p>===============================================</p>
<p>===============================================</p>
<hr>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">===============================================================================</span><br><span class="line">=      欢     迎     阅     读   《 V I M  教  程 》   ——      版本 1.7       =</span><br><span class="line">===============================================================================</span><br><span class="line"></span><br><span class="line">     Vim 是一个具有很多命令的功能非常强大的编辑器。限于篇幅，在本教程当中</span><br><span class="line">     就不详细介绍了。本教程的设计目标是讲述一些必要的基本命令，而掌握好这</span><br><span class="line">     些命令，您就能够很容易地将 Vim 当作一个通用编辑器来使用了。</span><br><span class="line"></span><br><span class="line">     完成本教程的内容大约需要25-30分钟，取决于您训练的时间。</span><br><span class="line"></span><br><span class="line">     注意：</span><br><span class="line">     每一节的命令操作将会更改本文。推荐您复制本文的一个副本，然后在副本上</span><br><span class="line">     进行训练(如果您是通过&quot;vimtutor&quot;来启动教程的，那么本文就已经是副本了)。</span><br><span class="line"></span><br><span class="line">     切记一点：本教程的设计思路是在使用中进行学习的。也就是说，您需要通过</span><br><span class="line">     执行命令来学习它们本身的正确用法。如果您只是阅读而不操作，那么您可能</span><br><span class="line">     会很快遗忘这些命令的！</span><br><span class="line"></span><br><span class="line">     好了，现在请确定您的Shift-Lock(大小写锁定键)还没有按下，然后按键盘上</span><br><span class="line">     的字母键 j 足够多次来移动光标，直到第一节的内容能够完全充满屏幕。</span><br><span class="line"></span><br><span class="line">~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span><br><span class="line">            第一讲第一节：移动光标</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">         ** 要移动光标，请依照说明分别按下 h、j、k、l 键。 **</span><br><span class="line"></span><br><span class="line">         ^</span><br><span class="line">         k            提示： h 的键位于左边，每次按下就会向左移动。</span><br><span class="line">       &lt; h     l &gt;           l 的键位于右边，每次按下就会向右移动。</span><br><span class="line">         j               j 键看起来很象一支尖端方向朝下的箭头。</span><br><span class="line">         v</span><br><span class="line"></span><br><span class="line">  1. 请随意在屏幕内移动光标，直至您觉得舒服为止。</span><br><span class="line"></span><br><span class="line">  2. 按下下行键(j)，直到出现光标重复下行。</span><br><span class="line"></span><br><span class="line">---&gt; 现在您应该已经学会如何移动到下一讲吧。</span><br><span class="line"></span><br><span class="line">  3. 现在请使用下行键，将光标移动到第一讲第二节。</span><br><span class="line"></span><br><span class="line">提示：如果您不敢确定您所按下的字母，请按下&lt;ESC&gt;键回到正常(Normal)模式。</span><br><span class="line">      然后再次从键盘输入您想要的命令。</span><br><span class="line"></span><br><span class="line">提示：光标键应当也能正常工作的。但是使用hjkl键，在习惯之后您就能够更快</span><br><span class="line">      地在屏幕内四处移动光标。真的是这样！</span><br><span class="line"></span><br><span class="line">~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span><br><span class="line">            第一讲第二节：VIM的进入和退出</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  !! 特别提示：敬请阅读本一节的完整内容，然后再执行以下所讲解的命令。</span><br><span class="line"></span><br><span class="line">  1. 按&lt;ESC&gt;键(这是为了确保您处在正常模式)。</span><br><span class="line"></span><br><span class="line">  2. 然后输入：            :q! &lt;回车&gt;</span><br><span class="line">     这种方式的退出编辑器会丢弃您进入编辑器以来所做的改动。</span><br><span class="line"></span><br><span class="line">  3. 如果您看到了命令行提示符，请输入能够带您回到本教程的命令，那就是：</span><br><span class="line">     vimtutor &lt;回车&gt;</span><br><span class="line"></span><br><span class="line">  4. 如果您自信已经牢牢记住了这些步骤的话，请从步骤1执行到步骤3退出，然</span><br><span class="line">     后再次进入编辑器。</span><br><span class="line"></span><br><span class="line">提示： :q! &lt;回车&gt; 会丢弃您所做的任何改动。几讲之后您将学会如何保存改动到文件。</span><br><span class="line"></span><br><span class="line">  5. 将光标下移到第一讲第三节。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span><br><span class="line">            第一讲第三节：文本编辑之删除</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">   ** 在正常(Normal)模式下，可以按下 x 键来删除光标所在位置的字符。**</span><br><span class="line"></span><br><span class="line">  1. 请将光标移动到本节中下面标记有 ---&gt; 的那一行。</span><br><span class="line"></span><br><span class="line">  2. 为了修正输入错误，请将光标移至准备删除的字符的位置处。</span><br><span class="line"></span><br><span class="line">  3. 然后按下 x 键将错误字符删除掉。</span><br><span class="line"></span><br><span class="line">  4. 重复步骤2到步骤4，直到句子修正为止。</span><br><span class="line"></span><br><span class="line">---&gt; The ccow jumpedd ovverr thhe mooon.</span><br><span class="line"></span><br><span class="line">  5. 好了，该行已经修正了，下面是第一讲第四节。</span><br><span class="line"></span><br><span class="line">特别提示：在浏览本教程时，不要强行记忆。记住一点：在使用中学习。</span><br><span class="line"></span><br><span class="line">~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span><br><span class="line">             第一讲第四节：文本编辑之插入</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">     ** 在正常模式下，可以按下 i 键来插入文本。**</span><br><span class="line"></span><br><span class="line">  1. 请将光标移动到本节中下面标记有 ---&gt; 的第一行。</span><br><span class="line"></span><br><span class="line">  2. 为了使得第一行内容雷同于第二行，请将光标移至文本第一个准备插入字符</span><br><span class="line">     的位置。</span><br><span class="line"></span><br><span class="line">  3. 然后按下 i 键，接着输入必要的文本字符。</span><br><span class="line"></span><br><span class="line">  4. 每个错误修正完毕后，请按下 &lt;ESC&gt; 键返回正常模式。</span><br><span class="line">     重复步骤2至步骤4以便修正句子。</span><br><span class="line"></span><br><span class="line">---&gt; There is text misng this .</span><br><span class="line">---&gt; There is some text missing from this line.</span><br><span class="line"></span><br><span class="line">  5. 如果您对文本插入操作已经很满意，请接着阅读下面的第一讲第五节。</span><br><span class="line"></span><br><span class="line">~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span><br><span class="line">             第一讲第五节：文本编辑之添加</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            ** 按 A 键以添加文本。 **</span><br><span class="line"></span><br><span class="line">  1. 移动光标到下面第一个标记有 ---&gt; 的一行。</span><br><span class="line">     光标放在那一行的哪个字符上并不重要。</span><br><span class="line"></span><br><span class="line">  2. 按 A 键输入必要的添加内容。</span><br><span class="line"></span><br><span class="line">  3. 文本添加完毕后，按 &lt;ESC&gt; 键回到正常模式。</span><br><span class="line"></span><br><span class="line">  4. 移动光标到下面第二个标记有 ---&gt; 的一行。重复步骤2和步骤3以改正这个句子。</span><br><span class="line"></span><br><span class="line">---&gt; There is some text missing from th</span><br><span class="line">     There is some text missing from this line.</span><br><span class="line">---&gt; There is also some text miss</span><br><span class="line">     There is also some text missing here.</span><br><span class="line"></span><br><span class="line">  5. 当您对添加文本操作感到满意时，请继续学习第一讲第六节。</span><br><span class="line"></span><br><span class="line">~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span><br><span class="line">             第一讲第六节：编辑文件</span><br><span class="line"></span><br><span class="line">            ** 使用 :wq 以保存文件并退出。 **</span><br><span class="line"></span><br><span class="line">  特别提示：在执行以下步骤之前，请先读完整个小节！</span><br><span class="line"></span><br><span class="line">  1. 如您在第一讲第二节中所做的那样退出本教程： :q!</span><br><span class="line">     或者，如果您可以访问另一个终端，请在那里执行以下操作。</span><br><span class="line"></span><br><span class="line">  2. 在 shell 的提示符下输入命令： vim tutor &lt;回车&gt;</span><br><span class="line">     &#x27;vim&#x27;是启动 Vim 编辑器的命令，&#x27;tutor&#x27;是您希望编辑的文件的名字。</span><br><span class="line">     请使用一个可以改动的文件。</span><br><span class="line"></span><br><span class="line">  3. 使用您在前面的教程中学到的命令插入删除文本。</span><br><span class="line"></span><br><span class="line">  4. 保存改动过的文件并退出 Vim，按这些键： :wq  &lt;回车&gt;</span><br><span class="line"></span><br><span class="line">  5. 如果您在步骤1中已经退出 vimtutor，请重启 vimtutor 移动到下面的小结一节。</span><br><span class="line"></span><br><span class="line">  6. 阅读完以上步骤，弄懂它们的意义，然后在实践中进行练习。</span><br><span class="line"></span><br><span class="line">~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span><br><span class="line">                   第一讲小结</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  1. 光标在屏幕文本中的移动既可以用箭头键，也可以使用 hjkl 字母键。</span><br><span class="line">     h (左移)    j (下行)       k (上行)        l (右移)</span><br><span class="line"></span><br><span class="line">  2. 欲进入 Vim 编辑器(从命令行提示符)，请输入：vim 文件名 &lt;回车&gt;</span><br><span class="line"></span><br><span class="line">  3. 欲退出 Vim 编辑器，请输入 &lt;ESC&gt;   :q!   &lt;回车&gt; 放弃所有改动。</span><br><span class="line">                      或者输入 &lt;ESC&gt;   :wq   &lt;回车&gt; 保存改动。</span><br><span class="line"></span><br><span class="line">  4. 在正常模式下删除光标所在位置的字符，请按： x</span><br><span class="line"></span><br><span class="line">  5. 欲插入或添加文本，请输入：</span><br><span class="line"></span><br><span class="line">     i   输入欲插入文本   &lt;ESC&gt;        在光标前插入文本</span><br><span class="line">     A   输入欲添加文本   &lt;ESC&gt;             在一行后添加文本</span><br><span class="line"></span><br><span class="line">特别提示：按下 &lt;ESC&gt; 键会带您回到正常模式或者撤消一个不想输入或部分完整</span><br><span class="line">的命令。</span><br><span class="line"></span><br><span class="line">好了，第一讲到此结束。下面接下来继续第二讲的内容。</span><br><span class="line"></span><br><span class="line">~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span><br><span class="line">            第二讲第一节：删除类命令</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        ** 输入 dw 可以从光标处删除至一个单词的末尾。**</span><br><span class="line"></span><br><span class="line">  1. 请按下 &lt;ESC&gt; 键确保您处于正常模式。</span><br><span class="line"></span><br><span class="line">  2. 请将光标移动到本节中下面标记有 ---&gt; 的那一行。</span><br><span class="line"></span><br><span class="line">  3. 请将光标移至准备要删除的单词的起始处。</span><br><span class="line"></span><br><span class="line">  4. 接着输入 dw 删除掉该单词。</span><br><span class="line"></span><br><span class="line">  特别提示：当您输入时，字母 d 会同时出现在屏幕的最后一行。Vim 在等待您输入</span><br><span class="line">  字母 w。如果您看到的是除 d 外的其他字符，那表明您按错了；请按下 &lt;ESC&gt; 键，</span><br><span class="line">  然后重新再来。</span><br><span class="line"></span><br><span class="line">---&gt; There are a some words fun that don&#x27;t belong paper in this sentence.</span><br><span class="line"></span><br><span class="line">  5. 重复步骤3和步骤4，直至句子修正完毕。接着继续第二讲第二节内容。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span><br><span class="line">              第二讲第二节：更多删除类命令</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">           ** 输入 d$ 从当前光标删除到行末。**</span><br><span class="line"></span><br><span class="line">  1. 请按下 &lt;ESC&gt; 键确保您处于正常模式。</span><br><span class="line"></span><br><span class="line">  2. 请将光标移动到本节中下面标记有 ---&gt; 的那一行。</span><br><span class="line"></span><br><span class="line">  3. 请将光标移动到该行的尾部(也就是在第一个点号‘.’后面)。</span><br><span class="line"></span><br><span class="line">  4. 然后输入 d$ 从光标处删至当前行尾部。</span><br><span class="line"></span><br><span class="line">---&gt; Somebody typed the end of this line twice. end of this line twice.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  5. 请继续学习第二讲第三节就知道是怎么回事了。</span><br><span class="line"></span><br><span class="line">~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span><br><span class="line">             第二讲第三节：关于命令和对象</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  许多改变文本的命令都由一个操作符和一个动作构成。</span><br><span class="line">  使用删除操作符 d 的删除命令的格式如下：</span><br><span class="line"></span><br><span class="line">    d   motion</span><br><span class="line"></span><br><span class="line">  其中：</span><br><span class="line">    d      - 删除操作符。</span><br><span class="line">    motion - 操作符的操作对象(在下面列出)。</span><br><span class="line"></span><br><span class="line">  一个简短的动作列表：</span><br><span class="line">    w - 从当前光标当前位置直到下一个单词起始处，不包括它的第一个字符。</span><br><span class="line">    e - 从当前光标当前位置直到单词末尾，包括最后一个字符。</span><br><span class="line">    $ - 从当前光标当前位置直到当前行末。</span><br><span class="line"></span><br><span class="line">  因此输入 de 会从当前光标位置删除到单词末尾。</span><br><span class="line"></span><br><span class="line">特别提示：</span><br><span class="line">    对于勇于探索者，请在正常模式下面仅按代表相应动作的键而不使用操作符，您</span><br><span class="line">    将看到光标的移动正如上面的对象列表所代表的一样。</span><br><span class="line"></span><br><span class="line">~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span><br><span class="line">             第二讲第四节：使用计数指定动作</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">             ** 在动作前输入数字会使它重复那么多次。 **</span><br><span class="line"></span><br><span class="line">  1. 移动光标到下面标记有 ---&gt; 的一行的开始。</span><br><span class="line"></span><br><span class="line">  2. 输入 2w 使光标向前移动两个单词。</span><br><span class="line"></span><br><span class="line">  3. 输入 3e 使光标向前移动到第三个单词的末尾。</span><br><span class="line"></span><br><span class="line">  4. 输入 0 (数字零) 移动光标到行首。</span><br><span class="line"></span><br><span class="line">  5. 重复步骤2和步骤3，尝试不同的数字。</span><br><span class="line"></span><br><span class="line">---&gt; This is just a line with words you can move around in.</span><br><span class="line"></span><br><span class="line">  6. 请继续学习第二讲第五节。</span><br><span class="line"></span><br><span class="line">~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span><br><span class="line">               第二讲第五节：使用计数以删除更多</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">           ** 使用操作符时输入数字可以使它重复那么多次。 **</span><br><span class="line"></span><br><span class="line">  上面已经提到过删除操作符和动作的组合，您可以在组合中动作之前插入一个数字以</span><br><span class="line">  删除更多：</span><br><span class="line">     d   number(数字)   motion</span><br><span class="line"></span><br><span class="line">  1. 移动光标到下面标记有 ---&gt; 的一行中第一个大写字母单词上。</span><br><span class="line"></span><br><span class="line">  2. 输入 d2w 以删除两个大写字母单词。</span><br><span class="line"></span><br><span class="line">  3. 重复步骤1和步骤2，使用不同的数字使得用一个命令就能删除全部相邻的大写字母</span><br><span class="line">     单词</span><br><span class="line"></span><br><span class="line">---&gt;  this ABC DE line FGHI JK LMN OP of words is Q RS TUV cleaned up.</span><br><span class="line"></span><br><span class="line">~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span><br><span class="line">                第二讲第六节：操作整行</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">             ** 输入 dd 可以删除整一个当前行。 **</span><br><span class="line"></span><br><span class="line">  鉴于整行删除的高频度，Vi 的设计者决定要简化整行删除操作，您仅需要在同一行上</span><br><span class="line">  击打两次 d 就可以删除掉光标所在的整行了。</span><br><span class="line"></span><br><span class="line">  1. 请将光标移动到本节中下面的短句段落中的第二行。</span><br><span class="line">  2. 输入 dd 删除该行。</span><br><span class="line">  3. 然后移动到第四行。</span><br><span class="line">  4. 接着输入 2dd 删除两行。</span><br><span class="line"></span><br><span class="line">---&gt;  1)  Roses are red,</span><br><span class="line">---&gt;  2)  Mud is fun,</span><br><span class="line">---&gt;  3)  Violets are blue,</span><br><span class="line">---&gt;  4)  I have a car,</span><br><span class="line">---&gt;  5)  Clocks tell time,</span><br><span class="line">---&gt;  6)  Sugar is sweet</span><br><span class="line">---&gt;  7)  And so are you.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span><br><span class="line">               第二讲第七节：撤消类命令</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    ** 输入 u 来撤消最后执行的命令，输入 U 来撤消对整行的修改。 **</span><br><span class="line"></span><br><span class="line">  1. 请将光标移动到本节中下面标记有 ---&gt; 的那一行，并将其置于第一个错误</span><br><span class="line">     处。</span><br><span class="line">  2. 输入 x 删除第一个不想保留的字母。</span><br><span class="line">  3. 然后输入 u 撤消最后执行的(一次)命令。</span><br><span class="line">  4. 这次要使用 x 修正本行的所有错误。</span><br><span class="line">  5. 现在输入一个大写的 U ，恢复到该行的原始状态。</span><br><span class="line">  6. 接着多次输入 u 以撤消 U 以及更前的命令。</span><br><span class="line">  7. 然后多次输入 CTRL-R (先按下 CTRL 键不放开，接着按 R 键)，这样就</span><br><span class="line">     可以重做被撤消的命令，也就是撤消掉撤消命令。</span><br><span class="line"></span><br><span class="line">---&gt; Fiix the errors oon thhis line and reeplace them witth undo.</span><br><span class="line"></span><br><span class="line">  8. 这些都是非常有用的命令。下面是第二讲的小结了。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span><br><span class="line">                   第二讲小结</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  1. 欲从当前光标删除至下一个单词，请输入：dw</span><br><span class="line">  2. 欲从当前光标删除至当前行末尾，请输入：d$</span><br><span class="line">  3. 欲删除整行，请输入：dd</span><br><span class="line"></span><br><span class="line">  4. 欲重复一个动作，请在它前面加上一个数字：2w</span><br><span class="line">  5. 在正常模式下修改命令的格式是：</span><br><span class="line">               operator   [number]   motion</span><br><span class="line">     其中：</span><br><span class="line">       operator - 操作符，代表要做的事情，比如 d 代表删除</span><br><span class="line">       [number] - 可以附加的数字，代表动作重复的次数</span><br><span class="line">       motion   - 动作，代表在所操作的文本上的移动，例如 w 代表单词(word)，</span><br><span class="line">          $ 代表行末等等。</span><br><span class="line"></span><br><span class="line">  6. 欲移动光标到行首，请按数字0键：0</span><br><span class="line"></span><br><span class="line">  7. 欲撤消以前的操作，请输入：u (小写的u)</span><br><span class="line">     欲撤消在一行中所做的改动，请输入：U (大写的U)</span><br><span class="line">     欲撤消以前的撤消命令，恢复以前的操作结果，请输入：CTRL-R</span><br><span class="line"></span><br><span class="line">~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span><br><span class="line">               第三讲第一节：置入类命令</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        ** 输入 p 将最后一次删除的内容置入光标之后。 **</span><br><span class="line"></span><br><span class="line">  1. 请将光标移动到本节中下面第一个标记有 ---&gt; 的一行。</span><br><span class="line"></span><br><span class="line">  2. 输入 dd 将该行删除，这样会将该行保存到 Vim 的一个寄存器中。</span><br><span class="line"></span><br><span class="line">  3. 接着将光标移动到 c) 一行，即准备置入的位置的上方。记住：是上方哦。</span><br><span class="line"></span><br><span class="line">  4. 然后在正常模式下(&lt;ESC&gt;键进入)输入 p 将该行粘贴置入。</span><br><span class="line"></span><br><span class="line">  5. 重复步骤2至步骤4，将所有的行依序放置到正确的位置上。</span><br><span class="line"></span><br><span class="line">---&gt; d) Can you learn too?</span><br><span class="line">---&gt; b) Violets are blue,</span><br><span class="line">---&gt; c) Intelligence is learned,</span><br><span class="line">---&gt; a) Roses are red,</span><br><span class="line"></span><br><span class="line">~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span><br><span class="line">               第三讲第二节：替换类命令</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">      ** 输入 r 和一个字符替换光标所在位置的字符。**</span><br><span class="line"></span><br><span class="line">  1. 请将光标移动到本节中下面标记有 ---&gt; 的第一行。</span><br><span class="line"></span><br><span class="line">  2. 请移动光标到第一个出错的位置。</span><br><span class="line"></span><br><span class="line">  3. 接着输入 r 和要替换成的字符，这样就能将错误替换掉了。</span><br><span class="line"></span><br><span class="line">  4. 重复步骤2和步骤3，直到第一行已经修改完毕。</span><br><span class="line"></span><br><span class="line">---&gt;  Whan this lime was tuoed in, someone presswd some wrojg keys!</span><br><span class="line">---&gt;  When this line was typed in, someone pressed some wrong keys!</span><br><span class="line"></span><br><span class="line">  5. 然后我们继续学习第三讲第三节。</span><br><span class="line"></span><br><span class="line">特别提示：切记您要在使用中学习，而不是在记忆中学习。</span><br><span class="line"></span><br><span class="line">~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span><br><span class="line">            第三讲第三节：更改类命令</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">         ** 要改变文本直到一个单词的末尾，请输入 ce **</span><br><span class="line"></span><br><span class="line">  1. 请将光标移动到本节中下面标记有 ---&gt; 的第一行。</span><br><span class="line"></span><br><span class="line">  2. 接着把光标放在单词 lubw 的字母 u 的位置那里。</span><br><span class="line"></span><br><span class="line">  3. 然后输入 cw 以及正确的单词(在本例中是输入 ine )。</span><br><span class="line"></span><br><span class="line">  4. 最后按 &lt;ESC&gt; 键，然后光标定位到下一个错误第一个准备更改的字母处。</span><br><span class="line"></span><br><span class="line">  5. 重复步骤3和步骤4，直到第一个句子完全雷同第二个句子。</span><br><span class="line"></span><br><span class="line">---&gt; This lubw has a few wptfd that mrrf changing usf the change operator.</span><br><span class="line">---&gt; This line has a few words that need changing using the change operator.</span><br><span class="line"></span><br><span class="line">提示：请注意 ce 命令不仅仅是删除了一个单词，它也让您进入插入模式了。</span><br><span class="line"></span><br><span class="line">~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span><br><span class="line">               第三讲第四节：使用c更改更多</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">       ** 更改类操作符可以与删除中使用的同样的动作配合使用。 **</span><br><span class="line"></span><br><span class="line">  1. 更改类操作符的工作方式跟删除类是一致的。操作格式是：</span><br><span class="line"></span><br><span class="line">         c    [number]   motion</span><br><span class="line"></span><br><span class="line">  2. 动作参数(motion)也是一样的，比如 w 代表单词，$代表行末等等。</span><br><span class="line"></span><br><span class="line">  3. 请将光标移动到本节中下面标记有 ---&gt; 的第一行。</span><br><span class="line"></span><br><span class="line">  4. 接着将光标移动到第一个错误处。</span><br><span class="line"></span><br><span class="line">  5. 然后输入 c$ 使得该行剩下的部分更正得同第二行一样。最后按 &lt;ESC&gt; 键。</span><br><span class="line"></span><br><span class="line">---&gt; The end of this line needs some help to make it like the second.</span><br><span class="line">---&gt; The end of this line needs to be corrected using the  c$  command.</span><br><span class="line"></span><br><span class="line">~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span><br><span class="line">                  第三讲小结</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  1. 要重新置入已经删除的文本内容，请按小写字母 p 键。该操作可以将已删除</span><br><span class="line">     的文本内容置于光标之后。如果最后一次删除的是一个整行，那么该行将置</span><br><span class="line">     于当前光标所在行的下一行。</span><br><span class="line"></span><br><span class="line">  2. 要替换光标所在位置的字符，请输入小写的 r 和要替换掉原位置字符的新字</span><br><span class="line">     符即可。</span><br><span class="line"></span><br><span class="line">  3. 更改类命令允许您改变从当前光标所在位置直到动作指示的位置中间的文本。</span><br><span class="line">     比如输入 ce 可以替换当前光标到单词的末尾的内容；输入 c$ 可以替换当</span><br><span class="line">     前光标到行末的内容。</span><br><span class="line"></span><br><span class="line">  4. 更改类命令的格式是：</span><br><span class="line"></span><br><span class="line">     c   [number]   motion</span><br><span class="line"></span><br><span class="line">现在我们继续学习下一讲。</span><br><span class="line"></span><br><span class="line">~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span><br><span class="line">             第四讲第一节：定位及文件状态</span><br><span class="line"></span><br><span class="line">  ** 输入 CTRL-G 显示当前编辑文件中当前光标所在行位置以及文件状态信息。</span><br><span class="line">     输入大写 G 则直接跳转到文件中的某一指定行。**</span><br><span class="line"></span><br><span class="line">  提示：切记要先通读本节内容，之后才可以执行以下步骤!!!</span><br><span class="line"></span><br><span class="line">  1. 按下 CTRL 键不放开然后按 g 键。我们称这个键组合为 CTRL-G。</span><br><span class="line">     您会看到页面最底部出现一个状态信息行，显示的内容是当前编辑的文件名</span><br><span class="line">     和文件中光标位置。请记住行号，它会在步骤3中用到。</span><br><span class="line"></span><br><span class="line">提示：您也许会在屏幕的右下角看到光标位置，这会在 &#x27;ruler&#x27; 选项设置时发生</span><br><span class="line">      (参见 :help &#x27;ruler&#x27;)</span><br><span class="line"></span><br><span class="line">  2. 输入大写 G 可以使得当前光标直接跳转到文件最后一行。</span><br><span class="line">     输入 gg 可以使得当前光标直接跳转到文件第一行。</span><br><span class="line"></span><br><span class="line">  3. 输入您曾停留的行号，然后输入大写 G。这样就可以返回到您第一次按下</span><br><span class="line">     CTRL-G 时所在的行了。</span><br><span class="line"></span><br><span class="line">  4. 如果您觉得没问题的话，请执行步骤1至步骤3的操作进行练习。</span><br><span class="line"></span><br><span class="line">~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span><br><span class="line">            第四讲第二节：搜索类命令</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">     ** 输入 / 加上一个字符串可以用以在当前文件中查找该字符串。**</span><br><span class="line"></span><br><span class="line">  1. 在正常模式下输入 / 字符。您此时会注意到该字符和光标都会出现在屏幕底</span><br><span class="line">     部，这跟 : 命令是一样的。</span><br><span class="line"></span><br><span class="line">  2. 接着输入 errroor &lt;回车&gt;。那个errroor就是您要查找的字符串。</span><br><span class="line"></span><br><span class="line">  3. 要查找同上一次的字符串，只需要按 n 键。要向相反方向查找同上一次的字</span><br><span class="line">     符串，请输入大写 N 即可。</span><br><span class="line"></span><br><span class="line">  4. 如果您想逆向查找字符串，请使用 ? 代替 / 进行。</span><br><span class="line"></span><br><span class="line">  5. 要回到您之前的位置按 CTRL-O (按住 Ctrl 键不放同时按下字母 o)。重复按可以</span><br><span class="line">     回退更多步。CTRL-I 会跳转到较新的位置。</span><br><span class="line"></span><br><span class="line">---&gt;  &quot;errroor&quot; is not the way to spell error;  errroor is an error.</span><br><span class="line">提示：如果查找已经到达文件末尾，查找会自动从文件头部继续查找，除非</span><br><span class="line">      &#x27;wrapscan&#x27; 选项被复位。</span><br><span class="line"></span><br><span class="line">~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span><br><span class="line">           第四讲第三节：配对括号的查找</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">          ** 输入 % 可以查找配对的括号 )、]、&#125;。**</span><br><span class="line"></span><br><span class="line">  1. 把光标放在本节下面标记有 --&gt; 那一行中的任何一个 (、[ 或 &#123; 处。</span><br><span class="line"></span><br><span class="line">  2. 接着按 % 字符。</span><br><span class="line"></span><br><span class="line">  3. 此时光标的位置应当是在配对的括号处。</span><br><span class="line"></span><br><span class="line">  4. 再次按 % 就可以跳回配对的第一个括号处。</span><br><span class="line"></span><br><span class="line">  5. 移动光标到另一个 (、)、[、]、&#123; 或 &#125; 处，按 % 查看其所作所为。</span><br><span class="line"></span><br><span class="line">---&gt; This ( is a test line with (&#x27;s, [&#x27;s ] and &#123;&#x27;s &#125; in it. ))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">提示：在程序调试时，这个功能用来查找不配对的括号是很有用的。</span><br><span class="line"></span><br><span class="line">~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span><br><span class="line">              第四讲第四节：替换命令</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        ** 输入 :s/old/new/g 可以替换 old 为 new。**</span><br><span class="line"></span><br><span class="line">  1. 请将光标移动到本节中下面标记有 ---&gt; 的那一行。</span><br><span class="line"></span><br><span class="line">  2. 输入 :s/thee/the &lt;回车&gt; 。请注意该命令只改变光标所在行的第一个匹配</span><br><span class="line">     串。</span><br><span class="line"></span><br><span class="line">  3. 输入 :s/thee/the/g    则是替换全行的匹配串，该行中所有的 &quot;thee&quot; 都会被</span><br><span class="line">     改变。</span><br><span class="line"></span><br><span class="line">---&gt; thee best time to see thee flowers is in thee spring.</span><br><span class="line"></span><br><span class="line">  4. 要替换两行之间出现的每个匹配串，请</span><br><span class="line">     输入   :#,#s/old/new/g   其中 #,# 代表的是替换操作的若干行中</span><br><span class="line">                              首尾两行的行号。</span><br><span class="line">     输入   :%s/old/new/g     则是替换整个文件中的每个匹配串。</span><br><span class="line">     输入   :%s/old/new/gc    会找到整个文件中的每个匹配串，并且对每个匹配串</span><br><span class="line">                              提示是否进行替换。</span><br><span class="line"></span><br><span class="line">~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span><br><span class="line">                   第四讲小结</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  1. CTRL-G 用于显示当前光标所在位置和文件状态信息。</span><br><span class="line">     G 用于将光标跳转至文件最后一行。</span><br><span class="line">     先敲入一个行号然后输入大写 G 则是将光标移动至该行号代表的行。</span><br><span class="line">     gg 用于将光标跳转至文件第一行。</span><br><span class="line"></span><br><span class="line">  2. 输入 / 然后紧随一个字符串是在当前所编辑的文档中正向查找该字符串。</span><br><span class="line">     输入 ? 然后紧随一个字符串则是在当前所编辑的文档中反向查找该字符串。</span><br><span class="line">     完成一次查找之后按 n 键是重复上一次的命令，可在同一方向上查</span><br><span class="line">     找下一个匹配字符串所在；或者按大写 N 向相反方向查找下一匹配字符串所在。</span><br><span class="line">     CTRL-O 带您跳转回较旧的位置，CTRL-I 则带您到较新的位置。</span><br><span class="line"></span><br><span class="line">  3. 如果光标当前位置是括号(、)、[、]、&#123;、&#125;，按 % 会将光标移动到配对的括号上。</span><br><span class="line"></span><br><span class="line">  4. 在一行内替换头一个字符串 old 为新的字符串 new，请输入  :s/old/new</span><br><span class="line">     在一行内替换所有的字符串 old 为新的字符串 new，请输入  :s/old/new/g</span><br><span class="line">     在两行内替换所有的字符串 old 为新的字符串 new，请输入  :#,#s/old/new/g</span><br><span class="line">     在文件内替换所有的字符串 old 为新的字符串 new，请输入  :%s/old/new/g</span><br><span class="line">     进行全文替换时询问用户确认每个替换需添加 c 标志        :%s/old/new/gc</span><br><span class="line"></span><br><span class="line">~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span><br><span class="line">        第五讲第一节：在 VIM 内执行外部命令的方法</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">       ** 输入 :! 然后紧接着输入一个外部命令可以执行该外部命令。**</span><br><span class="line"></span><br><span class="line">  1. 按下我们所熟悉的 : 命令使光标移动到屏幕底部。这样您就可以输入一行命令了。</span><br><span class="line"></span><br><span class="line">  2. 接着输入感叹号 ! 这个字符，这样就允许您执行外部的 shell 命令了。</span><br><span class="line"></span><br><span class="line">  3. 我们以 ls 命令为例。输入 !ls &lt;回车&gt; 。该命令就会列举出您当前目录的</span><br><span class="line">     内容，就如同您在命令行提示符下输入 ls 命令的结果一样。如果 !ls 没起</span><br><span class="line">     作用，您可以试试 :!dir 看看。</span><br><span class="line"></span><br><span class="line">提示：所有的外部命令都可以以这种方式执行，包括带命令行参数的那些。</span><br><span class="line"></span><br><span class="line">提示：所有的 : 命令都必须以敲 &lt;回车&gt; 键结束。从今以后我们就不会总是提到这一点</span><br><span class="line">      了。</span><br><span class="line"></span><br><span class="line">~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span><br><span class="line">              第五讲第二节：关于保存文件的更多信息</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">         ** 要将对文件的改动保存到文件中，请输入 :w FILENAME 。**</span><br><span class="line"></span><br><span class="line">  1. 输入 :!dir 或者 :!ls 获知当前目录的内容。您应当已知道最后还得敲</span><br><span class="line">     &lt;回车&gt; 吧。</span><br><span class="line"></span><br><span class="line">  2. 选择一个未被用到的文件名，比如 TEST。</span><br><span class="line"></span><br><span class="line">  3. 接着输入 :w TEST  (此处 TEST 是您所选择的文件名。)</span><br><span class="line"></span><br><span class="line">  4. 该命令会以 TEST 为文件名保存整个文件 (Vim 教程)。为了验证这一点，</span><br><span class="line">     请再次输入 :!dir 或 :!ls 查看您的目录列表内容。</span><br><span class="line"></span><br><span class="line">请注意：如果您退出 Vim 然后在以命令 vim TEST 再次启动 Vim，那么该文件内</span><br><span class="line">     容应该同您保存时的文件内容是完全一样的。</span><br><span class="line"></span><br><span class="line">  5. 现在您可以删除 TEST 文件了。在 MS-DOS 下，请输入：   :!del TEST</span><br><span class="line">                                 在 Unix 下，请输入：     :!rm TEST</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span><br><span class="line">            第五讲第三节：一个具有选择性的保存命令</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        ** 要保存文件的部分内容，请输入 v motion :w FILENAME **</span><br><span class="line"></span><br><span class="line">  1. 移动光标到本行。</span><br><span class="line"></span><br><span class="line">  2. 接着按 v 键，将光标移动至下面第五个条目上。您会注意到之间的文本被高亮了。</span><br><span class="line"></span><br><span class="line">  3. 然后按 : 字符。您将看到屏幕底部会出现 :&#x27;&lt;,&#x27;&gt; 。</span><br><span class="line"></span><br><span class="line">  4. 现在请输入 w TEST，其中 TEST 是一个未被使用的文件名。确认您看到了</span><br><span class="line">     :&#x27;&lt;,&#x27;&gt;w TEST 之后按 &lt;回车&gt; 键。</span><br><span class="line"></span><br><span class="line">  5. 这时 Vim 会把选中的行写入到以 TEST 命名的文件中去。使用 :!dir 或 :!ls</span><br><span class="line">     确认文件被正确保存。这次先别删除它！我们在下一讲中会用到它。</span><br><span class="line"></span><br><span class="line">提示：按 v 键使 Vim 进入可视模式进行选取。您可以四处移动光标使选取区域变大或</span><br><span class="line">      变小。接着您可以使用一个操作符对选中文本进行操作。例如，按 d 键会删除</span><br><span class="line">      选中的文本内容。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span><br><span class="line">           第五讲第四节：提取和合并文件</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">       ** 要向当前文件中插入另外的文件的内容，请输入 :r FILENAME **</span><br><span class="line"></span><br><span class="line">  1. 请把光标移动到本行上面一行。</span><br><span class="line"></span><br><span class="line">特别提示：执行步骤2之后您将看到第五讲第三节的文字，请届时往下移动</span><br><span class="line">          以再次看到本讲内容。</span><br><span class="line"></span><br><span class="line">  2. 接着通过命令 :r TEST 将前面创建的名为 TEST 的文件提取进来。</span><br><span class="line">     您所提取进来的文件将从光标所在位置处开始置入。</span><br><span class="line"></span><br><span class="line">  3. 为了确认文件已经提取成功，移动光标回到原来的位置就可以注意有两份第</span><br><span class="line">     五讲第三节的内容，一份是原始内容，另外一份是来自文件的副本。</span><br><span class="line"></span><br><span class="line">提示：您还可以读取外部命令的输出。例如， :r !ls 可以读取 ls 命令的输出，并</span><br><span class="line">      把它放置在光标下面。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span><br><span class="line">                   第五讲小结</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  1. :!command 用于执行一个外部命令 command。</span><br><span class="line"></span><br><span class="line">     请看一些实际例子：</span><br><span class="line">     (MS-DOS)      (Unix)</span><br><span class="line">      :!dir           :!ls           -  用于显示当前目录的内容。</span><br><span class="line">      :!del FILENAME   :!rm FILENAME   -  用于删除名为 FILENAME 的文件。</span><br><span class="line"></span><br><span class="line">  2. :w FILENAME  可将当前 VIM 中正在编辑的文件保存到名为 FILENAME 的文</span><br><span class="line">     件中。</span><br><span class="line"></span><br><span class="line">  3. v motion :w FILENAME 可将当前编辑文件中可视模式下选中的内容保存到文件</span><br><span class="line">     FILENAME 中。</span><br><span class="line"></span><br><span class="line">  4. :r FILENAME 可提取磁盘文件 FILENAME 并将其插入到当前文件的光标位置</span><br><span class="line">     后面。</span><br><span class="line"></span><br><span class="line">  5. :r !dir 可以读取 dir 命令的输出并将其放置到当前文件的光标位置后面。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span><br><span class="line">             第六讲第一节：打开类命令</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">     ** 输入 o 将在光标的下方打开新的一行并进入插入模式。**</span><br><span class="line"></span><br><span class="line">  1. 请将光标移动到本节中下面标记有 ---&gt; 的那一行。</span><br><span class="line"></span><br><span class="line">  2. 接着输入小写的 o 在光标 *下方* 打开新的一行，这个命令会使您</span><br><span class="line">     进入插入模式。</span><br><span class="line"></span><br><span class="line">  3. 然后输入一些文字，之后按 &lt;ESC&gt; 键退出插入模式而进入正常模式。</span><br><span class="line"></span><br><span class="line">---&gt; After typing  o  the cursor is placed on the open line in Insert mode.</span><br><span class="line"></span><br><span class="line">  4. 为了在光标 *上方* 打开新的一行，只需要输入大写的 O 而不是小写的 o</span><br><span class="line">     就可以了。请在下行测试一下吧。</span><br><span class="line"></span><br><span class="line">---&gt; Open up a line above this by typing O while the cursor is on this line.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span><br><span class="line">            第六讲第二节：附加类命令</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">             ** 输入 a 将可在光标之后插入文本。 **</span><br><span class="line"></span><br><span class="line">  1. 请在正常模式下将光标移动到本节中下面标记有 ---&gt; 的第一行的行首。</span><br><span class="line"></span><br><span class="line">  2. 接着输入 e 直到光标位于 li 的末尾。</span><br><span class="line"></span><br><span class="line">  3. 输入小写的 a 则可在光标之后插入文本了。</span><br><span class="line"></span><br><span class="line">  4. 将单词补充完整，就像下一行中的那样。之后按 &lt;ESC&gt; 键退出插入模式回到</span><br><span class="line">     正常模式。</span><br><span class="line"></span><br><span class="line">  5. 使用 e 移动光标到下一步不完整的单词，重复步骤3和步骤4。</span><br><span class="line"></span><br><span class="line">---&gt; This li will allow you to pract appendi text to a line.</span><br><span class="line">---&gt; This line will allow you to practice appending text to a line.</span><br><span class="line"></span><br><span class="line">提示：a、i 和 A 都会带您进入插入模式，惟一的区别在于字符插入的位置。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span><br><span class="line">            第六讲第三节：另外一个置换类命令的版本</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">              ** 输入大写的 R 可连续替换多个字符。**</span><br><span class="line"></span><br><span class="line">  1. 请将光标移动到本节中下面标记有 ---&gt; 的第一行。移动光标到第一个 xxx 的</span><br><span class="line">     起始位置。</span><br><span class="line"></span><br><span class="line">  2. 然后输入大写的 R 开始把第一行中的不同于第二行的剩余字符逐一输入，就</span><br><span class="line">     可以全部替换掉原有的字符而使得第一行完全雷同第二行了。</span><br><span class="line"></span><br><span class="line">  3. 接着按 &lt;ESC&gt; 键退出替换模式回到正常模式。您可以注意到尚未替换的文本</span><br><span class="line">     仍然保持原状。</span><br><span class="line"></span><br><span class="line">  4. 重复以上步骤，将剩余的 xxx 也替换掉。</span><br><span class="line"></span><br><span class="line">---&gt; Adding 123 to xxx gives you xxx.</span><br><span class="line">---&gt; Adding 123 to 456 gives you 579.</span><br><span class="line"></span><br><span class="line">提示：替换模式与插入模式相似，不过每个输入的字符都会删除一个已有的字符。</span><br><span class="line"></span><br><span class="line">~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span><br><span class="line">              第六讲第四节：复制粘贴文本</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">         ** 使用操作符 y 复制文本，使用 p 粘贴文本 **</span><br><span class="line"></span><br><span class="line">  1. 定位到下面标记有 ---&gt; 的一行，将光标移动到 &quot;a)&quot; 之后。</span><br><span class="line"></span><br><span class="line">  2. 接着使用 v 进入可视模式，移动光标到 &quot;first&quot; 的前面。</span><br><span class="line"></span><br><span class="line">  3. 现在输入 y 以抽出(复制)高亮的文本。</span><br><span class="line"></span><br><span class="line">  4. 然后移动光标到下一行的末尾：j$</span><br><span class="line"></span><br><span class="line">  5. 接着输入 p 以放置(粘贴)复制了的文本。然后输入：a second &lt;ESC&gt;。</span><br><span class="line"></span><br><span class="line">  6. 使用可视模式选中 &quot; item.&quot;，用 y 复制，再用 j$ 将光标移动到下一行末尾，</span><br><span class="line">     用 p 将文本粘贴到那里。</span><br><span class="line"></span><br><span class="line">---&gt;  a) this is the first item.</span><br><span class="line">      b)</span><br><span class="line"></span><br><span class="line">  提示：您还可以把 y 当作操作符来使用；例如 yw 可以用来复制一个单词。</span><br><span class="line"></span><br><span class="line">~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span><br><span class="line">                第六讲第五节：设置类命令的选项</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">          ** 设置可使查找或者替换可忽略大小写的选项 **</span><br><span class="line"></span><br><span class="line">  1. 要查找单词 ignore 可在正常模式下输入 /ignore &lt;回车&gt;。</span><br><span class="line">     要重复查找该词，可以重复按 n 键。</span><br><span class="line"></span><br><span class="line">  2. 然后设置 ic 选项(Ignore Case，忽略大小写)，请输入： :set ic</span><br><span class="line"></span><br><span class="line">  3. 现在可以通过键入 n 键再次查找单词 ignore。注意到 Ignore 和 IGNORE 现在</span><br><span class="line">     也被找到了。</span><br><span class="line"></span><br><span class="line">  4. 然后设置 hlsearch 和 incsearch 这两个选项，请输入： :set hls is</span><br><span class="line"></span><br><span class="line">  5. 现在可以再次输入查找命令，看看会有什么效果： /ignore &lt;回车&gt;</span><br><span class="line"></span><br><span class="line">  6. 要禁用忽略大小写，请输入： :set noic</span><br><span class="line"></span><br><span class="line">提示：要移除匹配项的高亮显示，请输入：  :nohlsearch</span><br><span class="line">提示：如果您想要仅在一次查找时忽略字母大小写，您可以使用 \c：</span><br><span class="line">      /ignore\c &lt;回车&gt;</span><br><span class="line">~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span><br><span class="line">                   第六讲小结</span><br><span class="line"></span><br><span class="line">  1. 输入小写的 o 可以在光标下方打开新的一行并进入插入模式。</span><br><span class="line">     输入大写的 O 可以在光标上方打开新的一行。</span><br><span class="line"></span><br><span class="line">  2. 输入小写的 a 可以在光标所在位置之后插入文本。</span><br><span class="line">     输入大写的 A 可以在光标所在行的行末之后插入文本。</span><br><span class="line"></span><br><span class="line">  3. e 命令可以使光标移动到单词末尾。</span><br><span class="line"></span><br><span class="line">  4. 操作符 y 复制文本，p 粘贴先前复制的文本。</span><br><span class="line"></span><br><span class="line">  5. 输入大写的 R 将进入替换模式，直至按 &lt;ESC&gt; 键回到正常模式。</span><br><span class="line"></span><br><span class="line">  6. 输入 :set xxx 可以设置 xxx 选项。一些有用的选项如下：</span><br><span class="line">      &#x27;ic&#x27; &#x27;ignorecase&#x27;    查找时忽略字母大小写</span><br><span class="line">    &#x27;is&#x27; &#x27;incsearch&#x27;    查找短语时显示部分匹配</span><br><span class="line">    &#x27;hls&#x27; &#x27;hlsearch&#x27;    高亮显示所有的匹配短语</span><br><span class="line">     选项名可以用完整版本，也可以用缩略版本。</span><br><span class="line"></span><br><span class="line">  7. 在选项前加上 no 可以关闭选项：  :set noic</span><br><span class="line"></span><br><span class="line">~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span><br><span class="line">              第七讲第一节：获取帮助信息</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">              ** 使用在线帮助系统 **</span><br><span class="line"></span><br><span class="line">  Vim 拥有一个细致全面的在线帮助系统。要启动该帮助系统，请选择如下三种方</span><br><span class="line">  法之一：</span><br><span class="line">    - 按下 &lt;HELP&gt; 键 (如果键盘上有的话)</span><br><span class="line">    - 按下 &lt;F1&gt; 键 (如果键盘上有的话)</span><br><span class="line">    - 输入    :help &lt;回车&gt;</span><br><span class="line"></span><br><span class="line">  请阅读帮助窗口中的文字以了解帮助是如何工作的。</span><br><span class="line">  输入 CTRL-W CTRL-W   可以使您在窗口之间跳转。</span><br><span class="line">  输入 :q &lt;回车&gt; 可以关闭帮助窗口。</span><br><span class="line"></span><br><span class="line">  提供一个正确的参数给&quot;:help&quot;命令，您可以找到关于该主题的帮助。请试验以</span><br><span class="line">  下参数(可别忘了按回车键哦)：</span><br><span class="line"></span><br><span class="line">    :help w</span><br><span class="line">    :help c_CTRL-D</span><br><span class="line">    :help insert-index</span><br><span class="line">    :help user-manual</span><br><span class="line">~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span><br><span class="line">              第七讲第二节：创建启动脚本</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">              ** 启用 Vim 的特性 **</span><br><span class="line"></span><br><span class="line">  Vim 的功能特性要比 Vi 多得多，但其中大部分都没有缺省启用。为了使用更多的</span><br><span class="line">  特性，您得创建一个 vimrc 文件。</span><br><span class="line"></span><br><span class="line">  1. 开始编辑 vimrc 文件，具体命令取决于您所使用的操作系统：</span><br><span class="line">        :edit ~/.vimrc        这是 Unix 系统所使用的命令</span><br><span class="line">        :edit $VIM/_vimrc    这是 MS-Windows 系统所使用的命令</span><br><span class="line"></span><br><span class="line">  2. 接着读取 vimrc 示例文件的内容：</span><br><span class="line">        :r $VIMRUNTIME/vimrc_example.vim</span><br><span class="line"></span><br><span class="line">  3. 保存文件，命令为：</span><br><span class="line">        :write</span><br><span class="line"></span><br><span class="line">  下次您启动 Vim 时，编辑器就会有了语法高亮的功能。</span><br><span class="line">  您可以把您喜欢的各种设置添加到这个 vimrc 文件中。</span><br><span class="line">  要了解更多信息请输入 :help vimrc-intro</span><br><span class="line"></span><br><span class="line">~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span><br><span class="line">                第七讲第三节：补全功能</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">          ** 使用 CTRL-D 和 &lt;TAB&gt; 可以进行命令行补全 **</span><br><span class="line"></span><br><span class="line">  1. 请确保 Vim 不是在以兼容模式运行： :set nocp</span><br><span class="line"></span><br><span class="line">  2. 查看一下当前目录下已经存在哪些文件，输入： :!ls   或者  :!dir</span><br><span class="line"></span><br><span class="line">  3. 现在输入一个目录的起始部分，例如输入： :e</span><br><span class="line"></span><br><span class="line">  4. 接着按 CTRL-D 键，Vim 会显示以 e 开始的命令的列表。</span><br><span class="line"></span><br><span class="line">  5. 然后按 &lt;TAB&gt; 键，Vim 会补全命令为 :edit 。</span><br><span class="line"></span><br><span class="line">  6. 现在添加一个空格，以及一个已有文件的文件名的起始部分，例如： :edit FIL</span><br><span class="line"></span><br><span class="line">  7. 接着按 &lt;TAB&gt; 键，Vim 会补全文件名(如果它是惟一匹配的)。</span><br><span class="line"></span><br><span class="line">提示：补全对于许多命令都有效。您只需尝试按 CTRL-D 和 &lt;TAB&gt;。</span><br><span class="line">      它对于 :help 命令非常有用。</span><br><span class="line"></span><br><span class="line">~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span><br><span class="line">                  第七讲小结</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  1. 输入 :help 或者按 &lt;F1&gt; 键或 &lt;Help&gt; 键可以打开帮助窗口。</span><br><span class="line"></span><br><span class="line">  2. 输入 :help cmd 可以找到关于 cmd 命令的帮助。</span><br><span class="line"></span><br><span class="line">  3. 输入 CTRL-W CTRL-W  可以使您在窗口之间跳转。</span><br><span class="line"></span><br><span class="line">  4. 输入 :q 以关闭帮助窗口</span><br><span class="line"></span><br><span class="line">  5. 您可以创建一个 vimrc 启动脚本文件用来保存您偏好的设置。</span><br><span class="line"></span><br><span class="line">  6. 当输入 : 命令时，按 CTRL-D 可以查看可能的补全结果。</span><br><span class="line">     按 &lt;TAB&gt; 可以使用一个补全。</span><br><span class="line"></span><br><span class="line">~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>Vim操作大全</title>
    <url>/2021/04/20/Vim%E6%93%8D%E4%BD%9C%E5%A4%A7%E5%85%A8/</url>
    <content><![CDATA[<h6 id><a href="#" class="headerlink" title="#"></a>#</h6><h1 id="VIM-CHEATSHEET-中文速查表-by-skywind-created-on-2017-10-12"><a href="#VIM-CHEATSHEET-中文速查表-by-skywind-created-on-2017-10-12" class="headerlink" title="VIM CHEATSHEET (中文速查表)  -  by skywind (created on 2017/10/12)"></a>VIM CHEATSHEET (中文速查表)  -  by skywind (created on 2017/10/12)</h1><h1 id="Version-47-Last-Modified-2020-10-10-11-02"><a href="#Version-47-Last-Modified-2020-10-10-11-02" class="headerlink" title="Version: 47, Last Modified: 2020/10/10 11:02"></a>Version: 47, Last Modified: 2020/10/10 11:02</h1><h1 id="https-github-com-skywind3000-awesome-cheatsheets"><a href="#https-github-com-skywind3000-awesome-cheatsheets" class="headerlink" title="https://github.com/skywind3000/awesome-cheatsheets"></a><a href="https://github.com/skywind3000/awesome-cheatsheets">https://github.com/skywind3000/awesome-cheatsheets</a></h1><h6 id="-1"><a href="#-1" class="headerlink" title="#"></a>#</h6><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">##############################################################################</span><br><span class="line"># VIM CHEATSHEET (中文速查表)  -  by skywind (created on 2017/10/12)</span><br><span class="line"># Version: 47, Last Modified: 2020/10/10 11:02</span><br><span class="line"># https://github.com/skywind3000/awesome-cheatsheets</span><br><span class="line">##############################################################################</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">##############################################################################</span><br><span class="line"># 光标移动</span><br><span class="line">##############################################################################</span><br><span class="line"></span><br><span class="line">h                   光标左移，同 &lt;Left&gt; 键</span><br><span class="line">j                   光标下移，同 &lt;Down&gt; 键</span><br><span class="line">k                   光标上移，同 &lt;Up&gt; 键</span><br><span class="line">l                   光标右移，同 &lt;Right&gt; 键</span><br><span class="line">CTRL-F              下一页</span><br><span class="line">CTRL-B              上一页</span><br><span class="line">CTRL-U              上移半屏</span><br><span class="line">CTRL-D              下移半屏</span><br><span class="line">0                   跳到行首（是数字零，不是字母O），效用等同于 &lt;Home&gt; 键</span><br><span class="line">^                   跳到从行首开始第一个非空白字符</span><br><span class="line">$                   跳到行尾，效用等同于 &lt;End&gt; 键</span><br><span class="line">gg                  跳到第一行，效用等同于 CTRL+&lt;Home&gt;</span><br><span class="line">G                   跳到最后一行，效用等同于 CTRL+&lt;End&gt;</span><br><span class="line">nG                  跳到第n行，比如 10G 是移动到第十行</span><br><span class="line">:n                  跳到第n行，比如 :10&lt;回车&gt; 是移动到第十行</span><br><span class="line">10%                 移动到文件 10% 处</span><br><span class="line">15|                 移动到当前行的 15列</span><br><span class="line">w                   跳到下一个单词开头 (word: 标点或空格分隔的单词)</span><br><span class="line">W                   跳到下一个单词开头 (WORD: 空格分隔的单词)</span><br><span class="line">e                   跳到下一个单词尾部 (word: 标点或空格分隔的单词)</span><br><span class="line">E                   跳到下一个单词尾部 (WORD: 空格分隔的单词)</span><br><span class="line">b                   上一个单词头 (word: 标点或空格分隔的单词)</span><br><span class="line">B                   上一个单词头 (WORD: 空格分隔的单词)</span><br><span class="line">ge                  上一个单词尾</span><br><span class="line">)                   向前移动一个句子（句号分隔）</span><br><span class="line">(                   向后移动一个句子（句号分隔）</span><br><span class="line">&#125;                   向前移动一个段落（空行分隔）</span><br><span class="line">&#123;                   向后移动一个段落（空行分隔）</span><br><span class="line">&lt;enter&gt;             移动到下一行首个非空字符</span><br><span class="line">+                   移动到下一行首个非空字符（同回车键）</span><br><span class="line">-                   移动到上一行首个非空字符</span><br><span class="line">H                   移动到屏幕上部</span><br><span class="line">M                   移动到屏幕中部</span><br><span class="line">L                   移动到屏幕下部</span><br><span class="line">fx                  跳转到下一个为 x 的字符，2f/ 可以找到第二个斜杆</span><br><span class="line">Fx                  跳转到上一个为 x 的字符</span><br><span class="line">tx                  跳转到下一个为 x 的字符前</span><br><span class="line">Tx                  跳转到上一个为 x 的字符前</span><br><span class="line">;                   跳到下一个 f/t 搜索的结果</span><br><span class="line">,                   跳到上一个 f/t 搜索的结果</span><br><span class="line">&lt;S-Left&gt;            按住 SHIFT 按左键，向左移动一个单词</span><br><span class="line">&lt;S-Right&gt;           按住 SHIFT 按右键，向右移动一个单词</span><br><span class="line">&lt;S-Up&gt;              按住 SHIFT 按上键，向上翻页</span><br><span class="line">&lt;S-Down&gt;            按住 SHIFT 按下键，向下翻页</span><br><span class="line">gm                  移动到行中</span><br><span class="line">gj                  光标下移一行（忽略自动换行）</span><br><span class="line">gk                  光标上移一行（忽略自动换行）</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">##############################################################################</span><br><span class="line"># 插入模式：进入退出</span><br><span class="line">##############################################################################</span><br><span class="line"></span><br><span class="line">i                   在光标处进入插入模式</span><br><span class="line">I                   在行首进入插入模式</span><br><span class="line">a                   在光标后进入插入模式</span><br><span class="line">A                   在行尾进入插入模式</span><br><span class="line">o                   在下一行插入新行并进入插入模式</span><br><span class="line">O                   在上一行插入新行并进入插入模式</span><br><span class="line">gi                  进入到上一次插入模式的位置</span><br><span class="line">&lt;ESC&gt;               退出插入模式</span><br><span class="line">CTRL-[              退出插入模式（同 ESC 等价，但更顺手）</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">##############################################################################</span><br><span class="line"># INSERT MODE - 由 i, I, a, A, o, O 等命令进入插入模式后</span><br><span class="line">##############################################################################</span><br><span class="line"></span><br><span class="line">&lt;Up&gt;                光标向上移动</span><br><span class="line">&lt;Down&gt;              光标向下移动</span><br><span class="line">&lt;Left&gt;              光标向左移动</span><br><span class="line">&lt;Right&gt;             光标向右移动</span><br><span class="line">&lt;S-Left&gt;            按住 SHIFT 按左键，向左移动一个单词</span><br><span class="line">&lt;S-Right&gt;           按住 SHIFT 按右键，向右移动一个单词</span><br><span class="line">&lt;S-Up&gt;              按住 SHIFT 按上键，向上翻页</span><br><span class="line">&lt;S-Down&gt;            按住 SHIFT 按下键，向下翻页</span><br><span class="line">&lt;PageUp&gt;            上翻页</span><br><span class="line">&lt;PageDown&gt;          下翻页</span><br><span class="line">&lt;Delete&gt;            删除光标处字符</span><br><span class="line">&lt;BS&gt;                Backspace 向后删除字符</span><br><span class="line">&lt;Home&gt;              光标跳转行首</span><br><span class="line">&lt;End&gt;               光标跳转行尾</span><br><span class="line">CTRL-W              向后删除单词</span><br><span class="line">CTRL-O              临时退出插入模式，执行单条命令又返回插入模式</span><br><span class="line">CTRL-\ CTRL-O       临时退出插入模式（光标保持），执行单条命令又返回插入模式</span><br><span class="line">CTRL-R 0            插入寄存器（内部 0号剪贴板）内容，CTRL-R 后可跟寄存器名</span><br><span class="line">CTRL-R &quot;            插入匿名寄存器内容，相当于插入模式下 p粘贴</span><br><span class="line">CTRL-R =            插入表达式计算结果，等号后面跟表达式</span><br><span class="line">CTRL-R :            插入上一次命令行命令</span><br><span class="line">CTRL-R /            插入上一次搜索的关键字</span><br><span class="line">CTRL-F              自动缩进</span><br><span class="line">CTRL-U              删除当前行所有字符</span><br><span class="line">CTRL-V &#123;char&#125;       插入非数字的字面量</span><br><span class="line">CTRL-V &#123;number&#125;     插入三个数字代表的 ascii/unicode 字符</span><br><span class="line">CTRL-V 065          插入 10进制 ascii 字符（两数字） 065 即 A字符</span><br><span class="line">CTRL-V x41          插入 16进制 ascii 字符（三数字） x41 即 A字符</span><br><span class="line">CTRL-V o101         插入  8进制 ascii 字符（三数字） o101 即 A字符</span><br><span class="line">CTRL-V u1234        插入 16进制 unicode 字符（四数字）</span><br><span class="line">CTRL-V U12345678    插入 16进制 unicode 字符（八数字）</span><br><span class="line">CTRL-K &#123;ch1&#125; &#123;ch2&#125;  插入 digraph（见 :h digraph），快速输入日文或符号等</span><br><span class="line">CTRL-D              文字向前缩进</span><br><span class="line">CTRL-T              文字向后缩进</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">##############################################################################</span><br><span class="line"># 文本编辑</span><br><span class="line">##############################################################################</span><br><span class="line"></span><br><span class="line">r                   替换当前字符</span><br><span class="line">R                   进入替换模式，直至 ESC 离开</span><br><span class="line">s                   替换字符（删除光标处字符，并进入插入模式，前可接数量）</span><br><span class="line">S                   替换行（删除当前行，并进入插入模式，前可接数量）</span><br><span class="line">cc                  改写当前行（删除当前行并进入插入模式），同 S</span><br><span class="line">cw                  改写光标开始处的当前单词</span><br><span class="line">ciw                 改写光标所处的单词</span><br><span class="line">caw                 改写光标所处的单词，并且包括前后空格（如果有的话）</span><br><span class="line">c0                  改写到行首</span><br><span class="line">c^                  改写到行首（第一个非零字符）</span><br><span class="line">c$                  改写到行末</span><br><span class="line">C                   改写到行尾（同c$）</span><br><span class="line">ci&quot;                 改写双引号中的内容</span><br><span class="line">ci&#x27;                 改写单引号中的内容</span><br><span class="line">cib                 改写小括号中的内容</span><br><span class="line">cab                 改写小括号中的内容（包含小括号本身）</span><br><span class="line">ci)                 改写小括号中的内容</span><br><span class="line">ci]                 改写中括号中内容</span><br><span class="line">ciB                 改写大括号中内容</span><br><span class="line">caB                 改写大括号中的内容（包含大括号本身）</span><br><span class="line">ci&#125;                 改写大括号中内容</span><br><span class="line">cit                 改写 xml tag 中的内容</span><br><span class="line">cis                 改写当前句子</span><br><span class="line">c2w                 改写下两个单词</span><br><span class="line">ct(                 改写到小括号前</span><br><span class="line">c/apple             改写到光标后的第一个apple前</span><br><span class="line">x                   删除当前字符，前面可以接数字，3x代表删除三个字符</span><br><span class="line">X                   向前删除字符</span><br><span class="line">dd                  删除当前行</span><br><span class="line">d0                  删除到行首</span><br><span class="line">d^                  删除到行首（第一个非零字符）</span><br><span class="line">d$                  删除到行末</span><br><span class="line">D                   删除到行末（同 d$）</span><br><span class="line">dw                  删除当前单词</span><br><span class="line">diw                 删除光标所处的单词</span><br><span class="line">daw                 删除光标所处的单词，并包含前后空格（如果有的话）</span><br><span class="line">di&quot;                 删除双引号中的内容</span><br><span class="line">di&#x27;                 删除单引号中的内容</span><br><span class="line">dib                 删除小括号中的内容</span><br><span class="line">di)                 删除小括号中的内容</span><br><span class="line">dab                 删除小括号内的内容（包含小括号本身）</span><br><span class="line">di]                 删除中括号中内容</span><br><span class="line">diB                 删除大括号中内容</span><br><span class="line">di&#125;                 删除大括号中内容</span><br><span class="line">daB                 删除大括号内的内容（包含大括号本身）</span><br><span class="line">dit                 删除 xml tag 中的内容</span><br><span class="line">dis                 删除当前句子</span><br><span class="line">dip                 删除当前段落(前后有空白行的称为一个段落)</span><br><span class="line">dap                 删除当前段落(包括前后空白行)</span><br><span class="line">d2w                 删除下两个单词</span><br><span class="line">dt(                 删除到小括号前</span><br><span class="line">d/apple             删除到光标后的第一个apple前</span><br><span class="line">dgg                 删除到文件头部</span><br><span class="line">dG                  删除到文件尾部</span><br><span class="line">d&#125;                  删除下一段</span><br><span class="line">d&#123;                  删除上一段</span><br><span class="line">u                   撤销</span><br><span class="line">U                   撤销整行操作</span><br><span class="line">CTRL-R              撤销上一次 u 命令</span><br><span class="line">J                   链接多行为一行</span><br><span class="line">.                   重复上一次操作</span><br><span class="line">~                   替换大小写</span><br><span class="line">g~iw                替换当前单词的大小写</span><br><span class="line">gUiw                将单词转成大写</span><br><span class="line">guiw                将当前单词转成小写</span><br><span class="line">guu                 全行转为小写</span><br><span class="line">gUU                 全行转为大写</span><br><span class="line">&lt;&lt;                  减少缩进</span><br><span class="line">&gt;&gt;                  增加缩进</span><br><span class="line">==                  自动缩进</span><br><span class="line">CTRL-A              增加数字</span><br><span class="line">CTRL-X              减少数字</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">##############################################################################</span><br><span class="line"># 复制粘贴</span><br><span class="line">##############################################################################</span><br><span class="line"></span><br><span class="line">p                   粘贴到光标后</span><br><span class="line">P                   粘贴到光标前</span><br><span class="line">v                   开始标记</span><br><span class="line">y                   复制标记内容</span><br><span class="line">V                   开始按行标记</span><br><span class="line">CTRL-V              开始列标记</span><br><span class="line">y$                  复制当前位置到本行结束的内容</span><br><span class="line">yy                  复制当前行</span><br><span class="line">Y                   复制当前行，同 yy</span><br><span class="line">yiw                 复制当前单词</span><br><span class="line">3yy                 复制光标下三行内容</span><br><span class="line">v0                  选中当前位置到行首</span><br><span class="line">v$                  选中当前位置到行末</span><br><span class="line">viw                 选中当前单词</span><br><span class="line">vib                 选中小括号内的东西</span><br><span class="line">vi)                 选中小括号内的东西</span><br><span class="line">vi]                 选中中括号内的东西</span><br><span class="line">viB                 选中大括号内的东西</span><br><span class="line">vi&#125;                 选中大括号内的东西</span><br><span class="line">vis                 选中句子中的东西</span><br><span class="line">vip                 选中当前段落(前后有空白行的称为一个段落)</span><br><span class="line">vap                 选中当前段落(包括前后空白行)</span><br><span class="line">vab                 选中小括号内的东西（包含小括号本身）</span><br><span class="line">va)                 选中小括号内的东西（包含小括号本身）</span><br><span class="line">va]                 选中中括号内的东西（包含中括号本身）</span><br><span class="line">vaB                 选中大括号内的东西（包含大括号本身）</span><br><span class="line">va&#125;                 选中大括号内的东西（包含大括号本身）</span><br><span class="line">gv                  重新选择上一次选中的文字</span><br><span class="line">:set paste          允许粘贴模式（避免粘贴时自动缩进影响格式）</span><br><span class="line">:set nopaste        禁止粘贴模式</span><br><span class="line">&quot;?yy                复制当前行到寄存器 ? ，问号代表 0-9 的寄存器名称</span><br><span class="line">&quot;?d3j               删除光标下三行内容，并放到寄存器 ? ，问号代表 0-9 的寄存器名称</span><br><span class="line">&quot;?p                 将寄存器 ? 的内容粘贴到光标后</span><br><span class="line">&quot;?P                 将寄存器 ? 的内容粘贴到光标前</span><br><span class="line">:registers          显示所有寄存器内容</span><br><span class="line">:[range]y           复制范围，比如 :20,30y 是复制20到30行，:10y 是复制第十行</span><br><span class="line">:[range]d           删除范围，比如 :20,30d 是删除20到30行，:10d 是删除第十行</span><br><span class="line">ddp                 交换两行内容：先删除当前行复制到寄存器，并粘贴</span><br><span class="line">&quot;_[command]         使用[command]删除内容，并且不进行复制（不会污染寄存器）</span><br><span class="line">&quot;*[command]         使用[command]复制内容到系统剪贴板（需要vim版本有clipboard支持）</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">##############################################################################</span><br><span class="line"># 文本对象 - c,d,v,y 等命令后接文本对象，一般为：&lt;范围 i/a&gt;&lt;类型&gt;</span><br><span class="line">##############################################################################</span><br><span class="line"></span><br><span class="line">$                   到行末</span><br><span class="line">0                   到行首</span><br><span class="line">^                   到行首非空字符</span><br><span class="line">tx                  光标位置到字符 x 之前</span><br><span class="line">fx                  光标位置到字符 x 之处</span><br><span class="line">iw                  整个单词（不包括分隔符）</span><br><span class="line">aw                  整个单词（包括分隔符）</span><br><span class="line">iW                  整个 WORD（不包括分隔符）</span><br><span class="line">aW                  整个 WORD（包括分隔符）</span><br><span class="line">is                  整个句子（不包括分隔符）</span><br><span class="line">as                  整个句子（包括分隔符）</span><br><span class="line">ip                  整个段落（不包括前后空白行）</span><br><span class="line">ap                  整个段落（包括前后空白行）</span><br><span class="line">ib                  小括号内</span><br><span class="line">ab                  小括号内（包含小括号本身）</span><br><span class="line">iB                  大括号内</span><br><span class="line">aB                  大括号内（包含大括号本身）</span><br><span class="line">i)                  小括号内</span><br><span class="line">a)                  小括号内（包含小括号本身）</span><br><span class="line">i]                  中括号内</span><br><span class="line">a]                  中括号内（包含中括号本身）</span><br><span class="line">i&#125;                  大括号内</span><br><span class="line">a&#125;                  大括号内（包含大括号本身）</span><br><span class="line">i&#x27;                  单引号内</span><br><span class="line">a&#x27;                  单引号内（包含单引号本身）</span><br><span class="line">i&quot;                  双引号内</span><br><span class="line">a&quot;                  双引号内（包含双引号本身）</span><br><span class="line">2i)                 往外两层小括号内</span><br><span class="line">2a)                 往外两层小括号内（包含小括号本身）</span><br><span class="line">2f)                 到第二个小括号处</span><br><span class="line">2t)                 到第二个小括号前</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">##############################################################################</span><br><span class="line"># 查找替换</span><br><span class="line">##############################################################################</span><br><span class="line"></span><br><span class="line">/pattern            从光标处向文件尾搜索 pattern</span><br><span class="line">?pattern            从光标处向文件头搜索 pattern</span><br><span class="line">n                   向同一方向执行上一次搜索</span><br><span class="line">N                   向相反方向执行上一次搜索</span><br><span class="line">*                   向前搜索光标下的单词</span><br><span class="line">#                   向后搜索光标下的单词</span><br><span class="line">:s/p1/p2/g          将当前行中全替换p1为p2</span><br><span class="line">:%s/p1/p2/g         将当前文件中全替换p1为p2</span><br><span class="line">:%s/p1/p2/gc        将当前文件中全替换p1为p2，并且每处询问你是否替换</span><br><span class="line">:10,20s/p1/p2/g     将第10到20行中所有p1替换为p2</span><br><span class="line">:., ns/p1/p2/g      将当前行到n行中所有p1替换为p2</span><br><span class="line">:., +10s/p1/p2/g    将当前行到相对当前行加10行的区间中所有p1替换为p2</span><br><span class="line">:., $s/p1/p2/g      将当前行到最后一行中所有p1替换为p2</span><br><span class="line">:%s/1\\2\/3/123/g   将“1\2/3” 替换为 “123”（特殊字符使用反斜杠标注）</span><br><span class="line">:%s/\r//g           删除 DOS 换行符 ^M</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">##############################################################################</span><br><span class="line"># VISUAL MODE - 由 v, V, CTRL-V 进入的可视模式</span><br><span class="line">##############################################################################</span><br><span class="line"></span><br><span class="line">&gt;                   增加缩进</span><br><span class="line">&lt;                   减少缩进</span><br><span class="line">d                   删除高亮选中的文字</span><br><span class="line">x                   删除高亮选中的文字</span><br><span class="line">c                   改写文字，即删除高亮选中的文字并进入插入模式</span><br><span class="line">s                   改写文字，即删除高亮选中的文字并进入插入模式</span><br><span class="line">y                   拷贝文字</span><br><span class="line">~                   转换大小写</span><br><span class="line">o                   跳转到标记区的另外一端</span><br><span class="line">O                   跳转到标记块的另外一端</span><br><span class="line">u                   标记区转换为小写</span><br><span class="line">U                   标记区转换为大写</span><br><span class="line">g CTRL-G            显示所选择区域的统计信息</span><br><span class="line">&lt;Esc&gt;               退出可视模式</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">##############################################################################</span><br><span class="line"># 位置跳转</span><br><span class="line">##############################################################################</span><br><span class="line"></span><br><span class="line">CTRL-O              跳转到上一个位置</span><br><span class="line">CTRL-I              跳转到下一个位置</span><br><span class="line">CTRL-^              跳转到 alternate file (当前窗口的上一个文件）</span><br><span class="line">%                   跳转到 &#123;&#125; () [] 的匹配</span><br><span class="line">gd                  跳转到局部定义（光标下的单词的定义）</span><br><span class="line">gD                  跳转到全局定义（光标下的单词的定义）</span><br><span class="line">gf                  打开名称为光标下文件名的文件</span><br><span class="line">[[                  跳转到上一个顶层函数（比如C语言以大括号分隔）</span><br><span class="line">]]                  跳转到下一个顶层函数（比如C语言以大括号分隔）</span><br><span class="line">[m                  跳转到上一个成员函数</span><br><span class="line">]m                  跳转到下一个成员函数</span><br><span class="line">[&#123;                  跳转到上一处未匹配的 &#123;</span><br><span class="line">]&#125;                  跳转到下一处未匹配的 &#125;</span><br><span class="line">[(                  跳转到上一处未匹配的 (</span><br><span class="line">])                  跳转到下一处未匹配的 )</span><br><span class="line">[c                  上一个不同处（diff时）</span><br><span class="line">]c                  下一个不同处（diff时）</span><br><span class="line">[/                  跳转到 C注释开头</span><br><span class="line">]/                  跳转到 C注释结尾</span><br><span class="line">``                  回到上次跳转的位置</span><br><span class="line">&#x27;&#x27;                  回到上次跳转的位置</span><br><span class="line">`.                  回到上次编辑的位置</span><br><span class="line">&#x27;.                  回到上次编辑的位置</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">##############################################################################</span><br><span class="line"># 文件操作</span><br><span class="line">##############################################################################</span><br><span class="line"></span><br><span class="line">:w                  保存文件</span><br><span class="line">:w &lt;filename&gt;       按名称保存文件</span><br><span class="line">:e &lt;filename&gt;       打开文件并编辑</span><br><span class="line">:saveas &lt;filename&gt;  另存为文件</span><br><span class="line">:r &lt;filename&gt;       读取文件并将内容插入到光标后</span><br><span class="line">:r !dir             将 dir 命令的输出捕获并插入到光标后</span><br><span class="line">:close              关闭文件</span><br><span class="line">:q                  退出</span><br><span class="line">:q!                 强制退出</span><br><span class="line">:wa                 保存所有文件</span><br><span class="line">:cd &lt;path&gt;          切换 Vim 当前路径</span><br><span class="line">:pwd                显示 Vim 当前路径</span><br><span class="line">:new                打开一个新的窗口编辑新文件</span><br><span class="line">:enew               在当前窗口创建新文件</span><br><span class="line">:vnew               在左右切分的新窗口中编辑新文件</span><br><span class="line">:tabnew             在新的标签页中编辑新文件</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">##############################################################################</span><br><span class="line"># 已打开文件操作</span><br><span class="line">##############################################################################</span><br><span class="line"></span><br><span class="line">:ls                 查案缓存列表</span><br><span class="line">:bn                 切换到下一个缓存</span><br><span class="line">:bp                 切换到上一个缓存</span><br><span class="line">:bd                 删除缓存</span><br><span class="line">:b 1                切换到1号缓存</span><br><span class="line">:b abc              切换到文件名为 abc 开头的缓存</span><br><span class="line">:badd &lt;filename&gt;    将文件添加到缓存列表</span><br><span class="line">:set hidden         设置隐藏模式（未保存的缓存可以被切换走，或者关闭）</span><br><span class="line">:set nohidden       关闭隐藏模式（未保存的缓存不能被切换走，或者关闭）</span><br><span class="line">n CTRL-^            切换缓存，先输入数字的缓存编号，再按 CTRL + 6</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">##############################################################################</span><br><span class="line"># 窗口操作</span><br><span class="line">##############################################################################</span><br><span class="line"></span><br><span class="line">:sp &lt;filename&gt;      上下切分窗口并在新窗口打开文件 filename</span><br><span class="line">:vs &lt;filename&gt;      左右切分窗口并在新窗口打开文件 filename</span><br><span class="line">CTRL-W s            上下切分窗口</span><br><span class="line">CTRL-W v            左右切分窗口</span><br><span class="line">CTRL-W w            循环切换到下一个窗口</span><br><span class="line">CTRL-W W            循环切换到上一个窗口</span><br><span class="line">CTRL-W p            跳到上一个访问过的窗口</span><br><span class="line">CTRL-W c            关闭当前窗口</span><br><span class="line">CTRL-W o            关闭其他窗口</span><br><span class="line">CTRL-W h            跳到左边的窗口</span><br><span class="line">CTRL-W j            跳到下边的窗口</span><br><span class="line">CTRL-W k            跳到上边的窗口</span><br><span class="line">CTRL-W l            跳到右边的窗口</span><br><span class="line">CTRL-W +            增加当前窗口的行高，前面可以加数字</span><br><span class="line">CTRL-W -            减少当前窗口的行高，前面可以加数字</span><br><span class="line">CTRL-W &lt;            减少当前窗口的列宽，前面可以加数字</span><br><span class="line">CTRL-W &gt;            增加当前窗口的列宽，前面可以加数字</span><br><span class="line">CTRL-W =            让所有窗口宽高相同</span><br><span class="line">CTRL-W H            将当前窗口移动到最左边</span><br><span class="line">CTRL-W J            将当前窗口移动到最下边</span><br><span class="line">CTRL-W K            将当前窗口移动到最上边</span><br><span class="line">CTRL-W L            将当前窗口移动到最右边</span><br><span class="line">CTRL-W x            交换窗口</span><br><span class="line">CTRL-W f            在新窗口中打开名为光标下文件名的文件</span><br><span class="line">CTRL-W gf           在新标签页中打开名为光标下文件名的文件</span><br><span class="line">CTRL-W R            旋转窗口</span><br><span class="line">CTRL-W T            将当前窗口移到新的标签页中</span><br><span class="line">CTRL-W P            跳转到预览窗口</span><br><span class="line">CTRL-W z            关闭预览窗口</span><br><span class="line">CTRL-W _            纵向最大化当前窗口</span><br><span class="line">CTRL-W |            横向最大化当前窗口</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">##############################################################################</span><br><span class="line"># 标签页</span><br><span class="line">##############################################################################</span><br><span class="line"></span><br><span class="line">:tabs               显示所有标签页</span><br><span class="line">:tabe &lt;filename&gt;    在新标签页中打开文件 filename</span><br><span class="line">:tabn               下一个标签页</span><br><span class="line">:tabp               上一个标签页</span><br><span class="line">:tabc               关闭当前标签页</span><br><span class="line">:tabo               关闭其他标签页</span><br><span class="line">:tabn n             切换到第n个标签页，比如 :tabn 3 切换到第三个标签页</span><br><span class="line">:tabm n             标签移动</span><br><span class="line">:tabfirst           切换到第一个标签页</span><br><span class="line">:tablast            切换到最后一个标签页</span><br><span class="line">:tab help           在标签页打开帮助</span><br><span class="line">:tab drop &lt;file&gt;    如果文件已被其他标签页和窗口打开则跳过去，否则新标签打开</span><br><span class="line">:tab split          在新的标签页中打开当前窗口里的文件</span><br><span class="line">:tab ball           将缓存中所有文件用标签页打开</span><br><span class="line">:set showtabline=?  设置为 0 就不显示标签页标签，1会按需显示，2会永久显示</span><br><span class="line">ngt                 切换到第n个标签页，比如 2gt 将会切换到第二个标签页</span><br><span class="line">gt                  下一个标签页</span><br><span class="line">gT                  上一个标签页</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">##############################################################################</span><br><span class="line"># 书签</span><br><span class="line">##############################################################################</span><br><span class="line"></span><br><span class="line">:marks              显示所有书签</span><br><span class="line">ma                  保存当前位置到书签 a ，书签名小写字母为文件内，大写全局</span><br><span class="line">&#x27;a                  跳转到书签 a所在的行</span><br><span class="line">`a                  跳转到书签 a所在位置</span><br><span class="line">`.                  跳转到上一次编辑的行</span><br><span class="line">&#x27;A                  跳转到全文书签 A</span><br><span class="line">[&#x27;                  跳转到上一个书签</span><br><span class="line">]&#x27;                  跳转到下一个书签</span><br><span class="line">&#x27;&lt;                  跳到上次可视模式选择区域的开始</span><br><span class="line">&#x27;&gt;                  跳到上次可视模式选择区域的结束</span><br><span class="line">:delm a             删除缓冲区标签a</span><br><span class="line">:delm A             删除文件标签A</span><br><span class="line">:delm!              删除所有缓冲区标签(小写字母), 不能删除文件标签和数字标签</span><br><span class="line">:delm A-Z           删除所有文件标签(大写字母)</span><br><span class="line">:delm 0-9           删除所有数字标签(.viminfo)</span><br><span class="line">:delm A-Z0-9        删除所有文件标签和数字标签</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line">##############################################################################</span><br><span class="line"># 常用设置</span><br><span class="line">##############################################################################</span><br><span class="line"></span><br><span class="line">:set nocompatible   设置不兼容原始 vi 模式（必须设置在最开头）</span><br><span class="line">:set bs=?           设置BS键模式，现代编辑器为 :set bs=eol,start,indent</span><br><span class="line">:set sw=4           设置缩进宽度为 4</span><br><span class="line">:set ts=4           设置制表符宽度为 4</span><br><span class="line">:set noet           设置不展开 tab 成空格</span><br><span class="line">:set et             设置展开 tab 成空格</span><br><span class="line">:set winaltkeys=no  设置 GVim 下正常捕获 ALT 键</span><br><span class="line">:set nowrap         关闭自动换行</span><br><span class="line">:set ttimeout       允许终端按键检测超时（终端下功能键为一串ESC开头的扫描码）</span><br><span class="line">:set ttm=100        设置终端按键检测超时为100毫秒</span><br><span class="line">:set term=?         设置终端类型，比如常见的 xterm</span><br><span class="line">:set ignorecase     设置搜索忽略大小写(可缩写为 :set ic)</span><br><span class="line">:set noignorecase   设置搜索不忽略大小写(可缩写为 :set noic)</span><br><span class="line">:set smartcase      智能大小写，默认忽略大小写，除非搜索内容里包含大写字母</span><br><span class="line">:set list           设置显示制表符和换行符</span><br><span class="line">:set number         设置显示行号，禁止显示行号可以用 :set nonumber</span><br><span class="line">:set relativenumber 设置显示相对行号（其他行与当前行的距离）</span><br><span class="line">:set paste          进入粘贴模式（粘贴时禁用缩进等影响格式的东西）</span><br><span class="line">:set nopaste        结束粘贴模式</span><br><span class="line">:set spell          允许拼写检查</span><br><span class="line">:set hlsearch       设置高亮查找</span><br><span class="line">:set ruler          总是显示光标位置</span><br><span class="line">:set incsearch      查找输入时动态增量显示查找结果</span><br><span class="line">:set insertmode     Vim 始终处于插入模式下，使用 ctrl-o 临时执行命令</span><br><span class="line">:set all            列出所有选项设置情况</span><br><span class="line">:syntax on          允许语法高亮</span><br><span class="line">:syntax off         禁止语法高亮</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">##############################################################################</span><br><span class="line"># 帮助信息</span><br><span class="line">##############################################################################</span><br><span class="line"></span><br><span class="line">:h tutor            入门文档</span><br><span class="line">:h quickref         快速帮助</span><br><span class="line">:h index            查询 Vim 所有键盘命令定义</span><br><span class="line">:h summary          帮助你更好的使用内置帮助系统</span><br><span class="line">:h CTRL-H           查询普通模式下 CTRL-H 是干什么的</span><br><span class="line">:h i_CTRL-H         查询插入模式下 CTRL-H 是干什么的</span><br><span class="line">:h i_&lt;Up&gt;           查询插入模式下方向键上是干什么的</span><br><span class="line">:h pattern.txt      正则表达式帮助</span><br><span class="line">:h eval             脚本编写帮助</span><br><span class="line">:h function-list    查看 VimScript 的函数列表 </span><br><span class="line">:h windows.txt      窗口使用帮助</span><br><span class="line">:h tabpage.txt      标签页使用帮助</span><br><span class="line">:h +timers          显示对 +timers 特性的帮助</span><br><span class="line">:h :!               查看如何运行外部命令</span><br><span class="line">:h tips             查看 Vim 内置的常用技巧文档</span><br><span class="line">:h set-termcap      查看如何设置按键扫描码</span><br><span class="line">:viusage            NORMAL 模式帮助</span><br><span class="line">:exusage            EX 命令帮助</span><br><span class="line">:version            显示当前 Vim 的版本号和特性</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">##############################################################################</span><br><span class="line"># 外部命令</span><br><span class="line">##############################################################################</span><br><span class="line"></span><br><span class="line">:!ls                运行外部命令 ls，并等待返回</span><br><span class="line">:r !ls              将外部命令 ls 的输出捕获，并插入到光标后</span><br><span class="line">:w !sudo tee %      sudo以后保存当前文件</span><br><span class="line">:call system(&#x27;ls&#x27;)  调用 ls 命令，但是不显示返回内容</span><br><span class="line">:!start notepad     Windows 下启动 notepad，最前面可以加 silent</span><br><span class="line">:sil !start cmd     Windows 下当前目录打开 cmd</span><br><span class="line">:%!prog             运行文字过滤程序，如整理 json格式 :%!python -m json.tool</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">##############################################################################</span><br><span class="line"># Quickfix 窗口</span><br><span class="line">##############################################################################</span><br><span class="line"></span><br><span class="line">:copen              打开 quickfix 窗口（查看编译，grep等信息）</span><br><span class="line">:copen 10           打开 quickfix 窗口，并且设置高度为 10</span><br><span class="line">:cclose             关闭 quickfix 窗口</span><br><span class="line">:cfirst             跳到 quickfix 中第一个错误信息</span><br><span class="line">:clast              跳到 quickfix 中最后一条错误信息</span><br><span class="line">:cc [nr]            查看错误 [nr]</span><br><span class="line">:cnext              跳到 quickfix 中下一个错误信息</span><br><span class="line">:cprev              跳到 quickfix 中上一个错误信息</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">##############################################################################</span><br><span class="line"># 拼写检查</span><br><span class="line">##############################################################################</span><br><span class="line"></span><br><span class="line">:set spell          打开拼写检查</span><br><span class="line">:set nospell        关闭拼写检查</span><br><span class="line">]s                  下一处错误拼写的单词</span><br><span class="line">[s                  上一处错误拼写的单词</span><br><span class="line">zg                  加入单词到拼写词表中</span><br><span class="line">zug                 撤销上一次加入的单词</span><br><span class="line">z=                  拼写建议</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">##############################################################################</span><br><span class="line"># 代码折叠</span><br><span class="line">##############################################################################</span><br><span class="line"></span><br><span class="line">za                  切换折叠</span><br><span class="line">zA                  递归切换折叠</span><br><span class="line">zc                  折叠光标下代码</span><br><span class="line">zC                  折叠光标下所有代码</span><br><span class="line">zd                  删除光标下折叠</span><br><span class="line">zD                  递归删除所有折叠</span><br><span class="line">zE                  删除所有折叠</span><br><span class="line">zf                  创建代码折叠</span><br><span class="line">zF                  指定行数创建折叠</span><br><span class="line">zi                  切换折叠</span><br><span class="line">zm                  所有代码折叠一层</span><br><span class="line">zr                  所有代码打开一层</span><br><span class="line">zM                  折叠所有代码，设置 foldlevel=0，设置 foldenable</span><br><span class="line">zR                  打开所有代码，设置 foldlevel 为最大值</span><br><span class="line">zn                  折叠 none，重置 foldenable 并打开所有代码</span><br><span class="line">zN                  折叠 normal，重置 foldenable 并恢复所有折叠</span><br><span class="line">zo                  打开一层代码</span><br><span class="line">zO                  打开光标下所有代码折叠</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">##############################################################################</span><br><span class="line"># 宏录制</span><br><span class="line">##############################################################################</span><br><span class="line"></span><br><span class="line">qa                  开始录制名字为 a 的宏</span><br><span class="line">q                   结束录制宏</span><br><span class="line">@a                  播放名字为 a 的宏</span><br><span class="line">@@                  播放上一个宏</span><br><span class="line">@:                  重复上一个ex命令（即冒号命令）</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">##############################################################################</span><br><span class="line"># 其他命令</span><br><span class="line">##############################################################################</span><br><span class="line"></span><br><span class="line">CTRL-X CTRL-F       插入模式下文件路径补全</span><br><span class="line">CTRL-X CTRL-O       插入下 Omnifunc 补全</span><br><span class="line">CTRL-X CTRL-N       插入模式下关键字补全</span><br><span class="line">CTRL-X CTRL-E       插入模式下向上滚屏</span><br><span class="line">CTRL-X CTRL-Y       插入模式下向下滚屏</span><br><span class="line">CTRL-E              向上滚屏</span><br><span class="line">CTRL-Y              向下滚屏</span><br><span class="line">CTRL-G              显示正在编辑的文件名，以及大小和位置信息</span><br><span class="line">g CTRL-G            显示文件的：大小，字符数，单词数和行数，可视模式下也可用</span><br><span class="line">zz                  调整光标所在行到屏幕中央</span><br><span class="line">zt                  调整光标所在行到屏幕上部</span><br><span class="line">zb                  调整光标所在行到屏幕下部</span><br><span class="line">ga                  显示光标下字符的 ascii 码或者 unicode 编码</span><br><span class="line">g8                  显示光标下字符的 utf-8 编码字节序</span><br><span class="line">gi                  回到上次进入插入的地方，并切换到插入模式</span><br><span class="line">K                   查询光标下单词的帮助</span><br><span class="line">ZZ                  保存文件（如果有改动的话），并关闭窗口</span><br><span class="line">ZQ                  不保存文件关闭窗口</span><br><span class="line">CTRL-PgUp           上个标签页，GVim OK，部分终端软件需设置对应键盘码</span><br><span class="line">CTRL-PgDown         下个标签页，GVim OK，部分终端软件需设置对应键盘码</span><br><span class="line">CTRL-R CTRL-W       命令模式下插入光标下单词</span><br><span class="line">CTRL-INSERT         复制到系统剪贴板（GVIM）</span><br><span class="line">SHIFT-INSERT        粘贴系统剪贴板的内容（GVIM）</span><br><span class="line">:set ff=unix        设置换行为 unix</span><br><span class="line">:set ff=dos         设置换行为 dos</span><br><span class="line">:set ff?            查看换行设置</span><br><span class="line">:set nohl           清除搜索高亮</span><br><span class="line">:set termcap        查看会从终端接收什么以及会发送给终端什么命令</span><br><span class="line">:set guicursor=     解决 SecureCRT/PenguiNet 中 NeoVim 局部奇怪字符问题</span><br><span class="line">:set t_RS= t_SH=    解决 SecureCRT/PenguiNet 中 Vim8.0 终端功能奇怪字符</span><br><span class="line">:set fo+=a          开启文本段的实时自动格式化</span><br><span class="line">:earlier 15m        回退到15分钟前的文件内容</span><br><span class="line">:.!date             在当前窗口插入时间</span><br><span class="line">:%!xxd              开始二进制编辑</span><br><span class="line">:%!xxd -r           保存二进制编辑</span><br><span class="line">:r !curl -sL &#123;URL&#125;  读取 url 内容添加到光标后</span><br><span class="line">:g/^\s*$/d          删除空行</span><br><span class="line">:g/green/d          删除所有包含 green 的行</span><br><span class="line">:v/green/d          删除所有不包含 green 的行</span><br><span class="line">:g/gladiolli/#      搜索单词打印结果，并在结果前加上行号</span><br><span class="line">:g/ab.*cd.*efg/#    搜索包含 ab,cd 和 efg 的行，打印结果以及行号</span><br><span class="line">:v/./,/./-j         压缩空行</span><br><span class="line">:Man bash           在 Vim 中查看 man，先调用 :runtime! ftplugin/man.vim 激活</span><br><span class="line">/fred\|joe          搜索 fred 或者 joe</span><br><span class="line">/\&lt;\d\d\d\d\&gt;       精确搜索四个数字</span><br><span class="line">/^\n\&#123;3&#125;            搜索连续三个空行</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">##############################################################################</span><br><span class="line"># Plugin - https://github.com/tpope/vim-commentary</span><br><span class="line">##############################################################################</span><br><span class="line"></span><br><span class="line">gcc                 注释当前行</span><br><span class="line">gc&#123;motion&#125;          注释 &#123;motion&#125; 所标注的区域，比如 gcap 注释整段</span><br><span class="line">gci&#123;                注释大括号内的内容</span><br><span class="line">gc                  在 Visual Mode 下面按 gc 注释选中区域</span><br><span class="line">:7,17Commentary     注释 7 到 17 行</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">##############################################################################</span><br><span class="line"># Plugin - https://github.com/junegunn/vim-easy-align</span><br><span class="line">##############################################################################</span><br><span class="line"></span><br><span class="line">:EasyAlign =        以第一个匹配的=为中心对齐</span><br><span class="line">:EasyAlign *=       匹配并且对齐所有=</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">##############################################################################</span><br><span class="line"># Plugin - https://github.com/tpope/vim-unimpaired</span><br><span class="line">##############################################################################</span><br><span class="line"></span><br><span class="line">[space              向上插入空行</span><br><span class="line">]space              向下插入空行</span><br><span class="line">[e                  替换当前行和上一行</span><br><span class="line">]e                  替换当前行和下一行</span><br><span class="line">[x                  XML 编码</span><br><span class="line">]x                  XML 解码</span><br><span class="line">[u                  URL 编码</span><br><span class="line">]u                  URL 解码</span><br><span class="line">[y                  C 字符串编码</span><br><span class="line">]y                  C 字符串解码</span><br><span class="line">[q                  上一个 quickfix 错误</span><br><span class="line">]q                  下一个 quickfix 错误</span><br><span class="line">[Q                  第一个 quickfix 错误</span><br><span class="line">]Q                  最后一个 quickfix 错误</span><br><span class="line">[f                  切换同目录里上一个文件</span><br><span class="line">]f                  切换同目录里下一个文件</span><br><span class="line">[os                 设置 :set spell</span><br><span class="line">]os                 设置 :set nospell</span><br><span class="line">=os                 设置 :set invspell</span><br><span class="line">[on                 显示行号</span><br><span class="line">]on                 关闭行号</span><br><span class="line">[ol                 显示回车和制表符 :set list</span><br><span class="line">]ol                 不显示回车和制表符 :set nolist</span><br><span class="line">[b                  缓存切换到上一个文件，即 :bp</span><br><span class="line">]b                  缓存切换到下一个文件，即 :bn</span><br><span class="line">[B                  缓存切换到第一个文件，即 :bfirst</span><br><span class="line">]B                  缓存切换到最后一个文件，即 :blast</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">##############################################################################</span><br><span class="line"># Plugin - https://github.com/skywind3000/asyncrun.vim</span><br><span class="line">##############################################################################</span><br><span class="line"></span><br><span class="line">:AsyncRun ls        异步运行命令 ls 结果输出到 quickfix 使用 :copen 查看</span><br><span class="line">:AsyncRun -raw ls   异步运行命令 ls 结果不匹配 errorformat</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">##############################################################################</span><br><span class="line"># Plugin - https://github.com/gaving/vim-textobj-argument</span><br><span class="line">##############################################################################</span><br><span class="line"></span><br><span class="line">cia                 改写函数参数</span><br><span class="line">caa                 改写函数参数（包括逗号分隔）</span><br><span class="line">dia                 删除函数参数</span><br><span class="line">daa                 删除函数参数（包括逗号分隔）</span><br><span class="line">via                 选取函数参数</span><br><span class="line">vaa                 选取函数参数（包括逗号分隔）</span><br><span class="line">yia                 复制函数参数</span><br><span class="line">yaa                 复制函数参数（包括逗号分隔）</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">##############################################################################</span><br><span class="line"># 网络资源</span><br><span class="line">##############################################################################</span><br><span class="line"></span><br><span class="line">最新版本            https://github.com/vim/vim   </span><br><span class="line">Windows 最新版      https://github.com/vim/vim-win32-installer/releases</span><br><span class="line">插件浏览            http://vimawesome.com</span><br><span class="line">reddit              https://www.reddit.com/r/vim/</span><br><span class="line">正确设置 ALT/BS 键  http://www.skywind.me/blog/archives/2021</span><br><span class="line">视频教程            http://vimcasts.org/</span><br><span class="line">中文帮助            http://vimcdoc.sourceforge.net/doc/help.html</span><br><span class="line">中文版入门到精通    https://github.com/wsdjeg/vim-galore-zh_cn</span><br><span class="line">五分钟脚本入门      http://www.skywind.me/blog/archives/2193</span><br><span class="line">脚本精通            http://learnvimscriptthehardway.stevelosh.com/</span><br><span class="line">中文脚本帮助        vimcdoc.sourceforge.net/doc/eval.html</span><br><span class="line">十六年使用经验      http://zzapper.co.uk/vimtips.html</span><br><span class="line">配色方案            http://vimcolors.com/</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">##############################################################################</span><br><span class="line"># TIPS</span><br><span class="line">##############################################################################</span><br><span class="line"></span><br><span class="line">- 永远不要用 CTRL-C 代替 &lt;ESC&gt; 完全不同的含义，容易错误中断运行的后台脚本</span><br><span class="line">- 很多人使用 CTRL-[ 代替 &lt;ESC&gt;，左手小指 CTRL，右手小指 [ 熟练后很方便</span><br><span class="line">- 某些终端中使用 Vim 8 内嵌终端如看到奇怪字符，使用 :set t_RS= t_SH= 解决</span><br><span class="line">- 某些终端中使用 Vim 8.2+ 会看到一些奇怪字符，使用 :set t_TI= t_TE= 解决</span><br><span class="line">- 某些终端中使用 NeoVim 如看到奇怪字符，使用 :set guicursor= 解决</span><br><span class="line">- 使用 MS-Terminal 如果进入 Vim/NVim 会默认替换模式设置 :set t_u7= 解决</span><br><span class="line">- 多使用 ciw, ci[, ci&quot;, ci( 以及 diw, di[, di&quot;, di( 命令来快速改写/删除文本</span><br><span class="line">- 在行内左右移动光标时，多使用w b e或W B E，而不是h l或方向键，这样会快很多</span><br><span class="line">- SHIFT 相当于移动加速键， w b e 移动光标很慢，但是 W B E 走的很快</span><br><span class="line">- 自己要善于总结新技巧，比如移动到行首非空字符时用 0w 命令比 ^ 命令更容易输入</span><br><span class="line">- 在空白行使用 dip 命令可以删除所有临近的空白行，viw 可以选择连续空白</span><br><span class="line">- 缩进时使用 &gt;8j  &gt;&#125;  &lt;ap  &gt;ap  =i&#125;  == 会方便很多</span><br><span class="line">- 插入模式下，当你发现一个单词写错了，应该多用 CTRL-W 这比 &lt;BackSpace&gt; 快</span><br><span class="line">- y d c 命令可以很好结合 f t 和 /X 比如 dt) 和 y/end&lt;cr&gt;</span><br><span class="line">- c d x 命令会自动填充寄存器 &quot;1 到 &quot;9 , y 命令会自动填充 &quot;0 寄存器</span><br><span class="line">- 用 v 命令选择文本时，可以用 o 掉头选择，有时很有用</span><br><span class="line">- 写文章时，可以写一段代码块，然后选中后执行 :!python 代码块就会被替换成结果</span><br><span class="line">- 搜索后经常使用 :nohl 来消除高亮，使用很频繁，可以 map 到 &lt;BackSpace&gt; 上</span><br><span class="line">- 搜索时可以用 CTRL-R CTRL-W 插入光标下的单词，命令模式也能这么用</span><br><span class="line">- 映射按键时，应该默认使用 noremap ，只有特别需要的时候使用 map</span><br><span class="line">- 当你觉得做某事很低效时，你应该停下来，u u u u 然后思考正确的高效方式来完成</span><br><span class="line">- 用 y复制文本后，命令模式中 CTRL-R 然后按双引号 0 可以插入之前复制内容</span><br><span class="line">- 某些情况下 Vim 绘制高亮慢，滚屏刷新慢可以试试 set re=1 使用老的正则引擎</span><br><span class="line">- Windows 下的 GVim 可以设置 set rop=type:directx,renmode:5 增强显示</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">##############################################################################</span><br><span class="line"># References</span><br><span class="line">##############################################################################</span><br><span class="line"></span><br><span class="line">https://github.com/groenewege/vimrc/blob/master/vim_cheat_sheet.txt</span><br><span class="line">http://blog.g-design.net/post/4789778607/vim-cheat-sheet</span><br><span class="line">http://www.keyxl.com/aaa8263/290/VIM-keyboard-shortcuts.htm</span><br><span class="line">http://jmcpherson.org/editing.html</span><br><span class="line">http://www.fprintf.net/vimCheatSheet.html</span><br><span class="line">http://www.ouyaoxiazai.com/article/24/654.html</span><br><span class="line">http://bbs.it-home.org/thread-80794-1-1.html</span><br><span class="line">http://www.lpfrx.com/wp-content/uploads/2008/09/vi.jpg</span><br><span class="line">http://michael.peopleofhonoronly.com/vim/</span><br><span class="line">https://github.com/hobbestigrou/vimtips-fortune/blob/master/fortunes/vimtips</span><br><span class="line">https://github.com/glts/vim-cottidie/blob/master/autoload/cottidie/tips</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># vim: set ts=4 sw=4 tw=0 noet noautoindent fdm=manual :</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
</search>
